== OpenCog Benchmarker ==

This tool was developed by Joel Pitt, in order to assess the impact of
forthcoming changes to the AtomSpace API (running the AtomSpace as a separate
event loop that fulfils requests). We primarily want to be aware of any
effects in terms of latency, but the benchmarker also has some ability to track
the resident set size (memory) of the benchmark process.

A simple example:

 $ ./opencog/benchmark/atomspace_bm -m "addLink" -b -k -S 100 -n 1000 -f

This will run the addLink benchmark (-m "addLink"). It will also build a test
AtomSpace before running the benchmark (-b) which consists of random graph
whose size and link density can be changed on the command line.

It will run and measure the speed of the addLink method 1000 times (-n 1000),
but between each it will add 100 more random atoms to the AtomSpace (-S 100)
so that you can assess how performance scales with AtomSpace size. -k
calculates some statistics on the times (min, max, mean, etc) and -f dumps
the records to a file called addLink_benchmark.csv (the filename used is
always the method name appended by "_benchmark.csv").

You should see output like:

Building atomspace with 65536 atoms (20% links)
Adding 52428 nodes ...................................................
Adding 13108 links ...................................................
Built atomspace, execution time: 1.95s
------------------------------
Benchmarking AtomSpace's addLink method 1000 times ........................
Sum time for all requests: 0.056635 seconds
Memory (max RSS) change after benchmark: 464kb
Per operation stats, in CPU clock ticks: 
  N: 1000
  mean: 56
  min: 7
  max: 1798
  std: 222
------------------------------

Use the -l option to list methods that are implemented for testing, or -A to
run ALL methods sequentially. You can also specify multiple methods by using -m
repeatedly.

The option -? will get more detail.

== A note about memory measurement ==

We just measure changes in the max RSS (resident stack size). This means that
when memory is freed the benchmarker doesn't detect it. For this reason, if you
want (semi)meaningful memory measurements, you should:

- only benchmark a single method at a time
- not enable statistic calculation (-k), as this will be storing all the time
records (you can always output to file with -f and do stats calculations later)

== Graphs ==

There is a script scripts/make_benchmark_graphs.py which will create graphs
from the csv files. You must have matplotlib (Python graphing library)
installed for this to work. If you run the script from a directory with
files ending in "_benchmark.csv" in it, it will create a .png file for
each.

== TODO ==

The memory estimation should be made more informative and reliable. One way to
do this may be to use dmalloc or wrap the process in valgrind and create
a script to process memory calls. Python also has the subprocess module which
could spawn instances of the benchmarker for each method (since the current
memory measurement via max RSS isn't useful when running multiple methods).

A better way might be to record the number of mallocs:
http://www.gnu.org/s/libc/manual/html_node/Hooks-for-Malloc.html

