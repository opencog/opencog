
                            Benchmarking Diary
                            ------------------

This file contains assorted benchmark results.  This is meant to be a
historical log: as changes to the atomspace are made, we can see how
performance changes over time.


26 March 2013
-------------
Benchamrks performed on 2008 vintage CPU and system:
   AMD Athlon(tm) 64 X2 Dual Core Processor 6000+
   3GHz  1024 KB cache
   8GB ECC DDR2 RAM

First, some raw data, and then some commentary & analysis.

The below measures the performance of the raw AtomTable interface:
    ./atomspace_bm -A -b -X     # Note the -X is uppercase!

    Benchmarking AtomSpace's addNode method 100000 times ..........
    0.664715 seconds elapsed (150440.40 per second)
    Sum clock() time for all requests: 390000 (0.39 seconds, 256410 requests per second)
    ------------------------------
    Benchmarking AtomSpace's addLink method 100000 times ..........
    0.784073 seconds elapsed (127539.17 per second)
    Sum clock() time for all requests: 580000 (0.58 seconds, 172414 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getType method 100000 times ..........
    0.138640 seconds elapsed (721291.70 per second)
    Sum clock() time for all requests: 40000 (0.04 seconds, 2.5e+06 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getTV method 100000 times ..........
    0.142129 seconds elapsed (703586.45 per second)
    Sum clock() time for all requests: 50000 (0.05 seconds, 2e+06 requests per second)
    ------------------------------
    Benchmarking AtomSpace's setTV method 100000 times ..........
    0.166353 seconds elapsed (601131.38 per second)
    Sum clock() time for all requests: 60000 (0.06 seconds, 1.66667e+06 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getHandleSet method 100000 times ..........
    46.604309 seconds elapsed (2145.72 per second)
    Sum clock() time for all requests: 46240000 (46.24 seconds, 2162.63 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getNodeHandles method 100000 times ..........
    3.733971 seconds elapsed (26781.14 per second)
    Sum clock() time for all requests: 3410000 (3.41 seconds, 29325.5 requests per second)
    ------------------------------


The below measures the performance of the AtomSpaceImpl interface:
    ./atomspace_bm -A -b -x     # Note the -x is lowercase!

    Benchmarking AtomSpace's addNode method 100000 times ..........
    4.878072 seconds elapsed (20499.90 per second)
    Sum clock() time for all requests: 3150000 (3.15 seconds, 31746 requests per second)
    ------------------------------
    Benchmarking AtomSpace's addLink method 100000 times ..........
    7.789448 seconds elapsed (12837.88 per second)
    Sum clock() time for all requests: 3990000 (3.99 seconds, 25062.7 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getType method 100000 times ..........
    3.509915 seconds elapsed (28490.72 per second)
    Sum clock() time for all requests: 120000 (0.12 seconds, 833333 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getTV method 100000 times ..........
    3.953390 seconds elapsed (25294.75 per second)
    Sum clock() time for all requests: 160000 (0.16 seconds, 625000 requests per second)
    ------------------------------
    Benchmarking AtomSpace's setTV method 100000 times ..........
    3.941879 seconds elapsed (25368.61 per second)
    Sum clock() time for all requests: 160000 (0.16 seconds, 625000 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getHandleSet method 100000 times ..........
    79.308176 seconds elapsed (1260.90 per second)
    Sum clock() time for all requests: 66550000 (66.55 seconds, 1502.63 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getNodeHandles method 100000 times ..........
    7.634395 seconds elapsed (13098.61 per second)
    Sum clock() time for all requests: 3830000 (3.83 seconds, 26109.7 requests per second)
    ------------------------------


The below measures the performance of the AtomSpace interface:
    ./atomspace_bm -A -b

    Benchmarking AtomSpace's addNode method 100000 times ..........
    5.710748 seconds elapsed (17510.84 per second)
    Sum clock() time for all requests: 3330000 (3.33 seconds, 30030 requests per second)
    ------------------------------
    Benchmarking AtomSpace's addLink method 100000 times ..........
    7.793599 seconds elapsed (12831.04 per second)
    Sum clock() time for all requests: 4110000 (4.11 seconds, 24330.9 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getType method 100000 times ..........
    7.581572 seconds elapsed (13189.88 per second)
    Sum clock() time for all requests: 3830000 (3.83 seconds, 26109.7 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getTV method 100000 times ..........
    6.203707 seconds elapsed (16119.39 per second)
    Sum clock() time for all requests: 2260000 (2.26 seconds, 44247.8 requests per second)
    ------------------------------
    Benchmarking AtomSpace's setTV method 100000 times ..........
    4.281147 seconds elapsed (23358.23 per second)
    Sum clock() time for all requests: 1570000 (1.57 seconds, 63694.3 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getHandleSet method 100000 times ..........
    81.959277 seconds elapsed (1220.12 per second)
    Sum clock() time for all requests: 77910000 (77.91 seconds, 1283.53 requests per second)
    ------------------------------
    Benchmarking AtomSpace's getNodeHandles method 100000 times ..........
    9.935555 seconds elapsed (10064.86 per second)
    Sum clock() time for all requests: 7270000 (7.27 seconds, 13755.2 requests per second)
    ------------------------------



Commentary and Analysis
-----------------------
The "raw AtomTable" numbers are what one gets if one uses just the
AtomTable, and naked pointers.  The AtomTable offers some indexes for
quick lookup of atoms by name, type, etc.  Getting atom attributes is
fast, as the atom table provides a direct pointer to the atom. 

The AtomSpaceImpl interface is another "native" interface, but it hides
all raw pointers, and conducts all business by using handles.  Performance
is slower as a result.  We can estimate the overhead of this hiding by
comparing performance figures to the raw AtomTable results.

The AtomSpace interface attempts to provide thread safety by creating an
atomspace server, and communicating with it via request/response messages.
The server side itself uses the AtomSpaceImpl to do its work.  Again, 
performance is slower.  Comparing to the AtomSpaceImpl numbers gives
a hint of the overhead of the request/response architecture.

Rearrange in tabular form:

   test            AtomTable    AtomSpaceImpl    AtomSpace
   name            K-ops/sec      K-ops/sec      K-ops/sec
   ----            ---------    -------------    ---------
  addNode            256.4         31.7            30.0
  addLink (***)      172.4         25.06           24.3
  getType           2500          833              26.1
  getTV             2000          625              44.2
  setTV             1666          625              63.7
  getHandleSet         2.16         1.50            1.28
  getNodeHandles      29.3         26.1            13.76


(***) See comment below

Clearly, raw access is much faster.  Clearly, the particular
getHandleSet (one of dozens) was slowww.

All operations were performed 100K times. To see how overheads actually
work, its easier to consider the microseconds per call.  No calculator
is necessary: just take the elapsed time (seconds) and multiply by 10
to get microseconds:

   test            AtomTable    AtomSpaceImpl    AtomSpace
   name            usecs/call     usecs/call     usecs/call
   ----            ---------    -------------    ---------
  addNode             3.9           31.5           33.3
  addLink (***)       5.8           39.9           41.1
  getType             0.4            1.2           38.3
  getTV               0.5            1.6           22.6
  setTV               0.6            1.6           15.7
  getHandleSet      462.4          665.5          779
  getNodeHandles     34.1           38.3           72.7

(***) See comment below

So...
The AtomSpaceImpl adds an aprox 30 usec overhead to addNode/addLink and
the client/server interface adds another 2 usecs

Its not clear why AtomSpaceImpl does NOT add an overhead to getType and
get/setTV ... instead, there seems to be a very hefty communications 
overhead for this?  This doesn't make sense when compared to addNode/Link

It appears that both the AtomSpaceImpl, and the client/server design
add large overheads to the two handle-set gets. I assume this overhead
is because a lot of data is being moved around.

getNodeHandles is asking for nodes by name and type.  This is serviced
by the atom table, which maintains an index of these. (the NodeIndex)

getHandleSet asks for all atoms of a given type. Again, there is an
index in the AtomTable for this.  You'd think it should be faster;
I'm guessing that perhaps there is a lot of data copying going on here...

(***) The benchmark was broken, it was creating Links with an average
of 0.5 atoms in them, and a std deviation of 0.5. i.e. Most links had
one atoms in them; that's not typical. Thus, the reported numbers are
not realistic.


27 March 2013
-------------
Provided a new implementation for the getHandleSet that doubled
the performance. A major bottleneck was the generation of HandleEntry's
and then the copying of them into an std::vector.  Eliminating this
step saved a lot. See the template getHandleSet<OutputIterator> in 
AtomTable.h for the right way to do these.

In fact, *all* HandleEntry things should be eliminated and replaced
by iterators, and should use std::copy_if instead of filterSet.

   test            AtomTable    AtomSpaceImpl    AtomSpace
   name            K-ops/sec      K-ops/sec      K-ops/sec
   ----            ---------    -------------    ---------
  getHandleSet-old     2.16         1.50            1.28
  getHandleSet-new     4.33         4.33            0.658


Basically, much/most of the overhead that AtomSpaceImpl adds to
the AtomTable is due to the copying of HandleSets into vectors 
and such.  


29 March 2013
-------------
Replaced more uses of HandleEntry with C++11 unordered_set and etc.
This improved performance across the board, except for setTV which
got a lot slower (!?) and getNodeHandles which got a little slower.
New results:

   test            AtomTable    AtomSpaceImpl    AtomSpace
   name            K-ops/sec      K-ops/sec      K-ops/sec
   ----            ---------    -------------    ---------
  addNode            270.3         28.5            33.2
  addLink            178.6         24.8            25.8
  getType           3333          833              33.0
  getTV             2000          714              60.6
  setTV             5000          666              44.4
  getHandleSet         4.99         4.27            2.83
  getNodeHandles      37.3         19.1            13.7
  getOutgoingSet    3333         1000              44.6


1 April 2013
-------------
Replaced all uses of HandleEntry with C++11 unordered_set and etc.
HandleEntry is now extinct.  The nature of the benchmark changed
as well: instead of creating 65536 atoms initially, the new bnechmark
creates 256K atoms: four times more than before. This has an overall
negative impact on addLink, getHandleSet and getNodeHandles.  However,
the new size is more realistic, so the actual perf is more realistic
too.

Added a no-op test. The no-op makes no calls to the atom-anything at
all.  Thus it provides a measuremnt baseline. Based on the no-op test,
we see that getType, getTV, setTV and getOutgoingSewt are running at
near-noop speeds.

Added scheme tests. The scheme tests create strings, and pass them to
the scheme interpreter.  The scheme interpreter is currently built on
top of the AtomSpace.  The scheme API does not currently expose the 
getHandleSet and getNodeHandles API, so no measurements made for those.

New results:

   test           AtomTable    AtomSpaceImpl    AtomSpace    Scheme
   name           K-ops/sec      K-ops/sec      K-ops/sec   K-ops/sec
   ----           ---------    -------------    ---------   ---------
  noop             2500         2500            1250         2500
  addNode           333           97.1            30.1          5.10
  addLink            92.6         39.1            19.7          3.87
  getType          5000          769              76.3          3.90
  getTV            1429          769              45.5          4.53
  setTV            1429          667              47.2          2.30
  getHandleSet        1.12         1.11            0.742         -
  getNodeHandles     20.7         14.1            10.4           -
  getOutgoingSet   1666          909              50.5          2.78
  getIncomingSet   1111          476              75.8          1.82


Some comments about the scheme performance:
Measuring scheme performance the way it is currently measured is rather
misleading.  This is because the rate-limiting factor is the time it
takes to enter and exit the scheme interpreter.  Specifically, it seems
to take about 30 microseconds to call scm_with_guile() and about 110
microseconds to move from outside of scm_c_catch() to the inside, where
the C code for the atomspace is being called.  Thus, these two steps 
alone rate-limit the measurement to 7K-ops/sec max. 

In 'real life', both of these steps would be infrequent, as normally,
one would enter the interpeter, do a wholw lot of stuff, and then exit.
In other words, the correct way to measure performance for the scheme
bindings would be to create maybe 10 or 100 atoms, once one is 'inside'.

The above can be tested: grep for ONE_SHOT in the code.  It defines the
following loop, and then calls (crea 100)

(define (crea n) 
  (if (not (eq? n 0))
    (let ()
      (cog-new-node 'ParseNode
        (string-join (list "stuff " (number->string n))))
      (crea (- n 1))
    )
  )
)

Thus, 100 nodes are created for just one entry into the interpreter.
The resulting create performance works out to be 11.7K creates/sec
which although is not stellar, clearly spends nearly half its time in
the atomspace, as opposed to the scheme code ... which is BTW no longer
trivial.  To conclude: the AtomSpace is a bottleneck, even for scheme.



27 March 2013
-------------

Rerun benchmarks:
Intel(R) Xeon(R)  X5650  @ 2.67GHz  12MB cache per core, 12 cores.
76GB RAM

1 October 2013
--------------
Remeasure, just for grins. Not expecting any change from above; hardware
is the same, compiler is gcc-4.7.4 instead of gcc 4.6.2 as above.

Surprisingly, there is a 20%-40% drop for addNode, addLink !!
Performance of getHandleSet halved!          Is this a compiler thing???
Performance of getNodeHandles is 5x faster!  Is this a compiler thing???
I think this change is compiler-related.


   test           AtomTable    AtomSpaceImpl    AtomSpace    Scheme
   name           K-ops/sec      K-ops/sec      K-ops/sec   K-ops/sec
   ----           ---------    -------------    ---------   ---------
  noop             1428       
  addNode           222         
  addLink            75.7      
  getType          1429       
  getTV            1429      
  setTV            1666     
  getHandleSet        0.693  
  getNodeHandles    101     
  getOutgoingSet   1111    
  getIncomingSet   1000   


9 October 2013
--------------
Remeasure. This is after removing most of the TLB, and using
std::shared_ptr atoms and handles.  This is a rather large
change, and I expect performance to be slightly degraded
(other changes needed for perf improvement haven't yet done).
The slowdown compared to above is almost surely due to the
more complex form that Handle now has, resulting in more copying.
(results in first column).


   test           AtomTable     AtomTable II      III          IV
   name           K-ops/sec      K-ops/sec      K-ops/sec   K-ops/sec
   ----           ---------     ----------      ---------   ---------
  noop            10000          3333           10000          inf
  addNode           208           238             222         204
  addLink            60.6          58.8            57.8        56.8
  getType          5000           833             833         769
  getTV             909          1000             909         666
  setTV             769           588             625         625
  getHandleSet        0.195         0.438           0.487       0.428
  getNodeHandles     83.3          91.7            80.0        91.7
  getOutgoingSet    625           833             909         769
  getIncomingSet   5000          1429            2000         909


The second column replaces Handle by UUID in the TypeIndex. This improves
performance by avoiding excess copying during ahndle manipulations.

Column III shows the profile after two significant changes: rejiggering
the way the AtomPtr is held by the handle, and by adding locks to the
AtomTable.  Looks like no change (the numbers are rather noisy).

Column IV shows the profile after a massive conversion to use std::shared_ptr
for truth values.


Repeat measurements for current code (IV) for all interfaces:

                   -A -X         -A -x            -A          -A -g
   test           AtomTable    AtomSpaceImpl    AtomSpace    Scheme
   name           K-ops/sec      K-ops/sec      K-ops/sec   K-ops/sec
   ----           ---------    -------------    ---------   ---------
  noop              inf         1111            1250         1250
  addNode           204           79.4            29.7          4.87
  addLink            56.8         31.4            18.9          4.90
  getType           769          357              32.7          4.36
  getTV             666          400              52.6          5.87
  setTV             625          400              51.3          3.64
  getHandleSet        0.428        0.420           0.187         -
  getNodeHandles     91.7         49.3            23.8           -
  getOutgoingSet    769          370              55.6          3.73
  getIncomingSet    909          434              52.6          2.29


Hmm. A net overall slowdown in AtomSpaceImpl. This is presumably due to
excessive pointer mainuplation (resulting in excess shared_ptr count
increments & ctor/dtor ops).

AtomSpace perf is neutral: some things got faster, some got slower.
Some didn't change. Unclear what that's about -- a mixed effect, I guess.

Curiously, Scheme perf improved across the board ... even when atomspace
perf went down. Perhaps this has to do with truth-value pointer managment?

--------------
Remeasure, after bugfix: WOW! WHAT A HUGE DIFFERENCE!
Removed a single line of code:

--- a/opencog/atomspace/AtomTable.h
+++ b/opencog/atomspace/AtomTable.h
@@ -685,7 +685,6 @@ public:
      * Return true if the atom table holds this handle, else return false.
      */
     bool holds(Handle h) const {
-        h = getHandle(h);
         return (NULL != h) and h->getAtomTable() == this; 
     }

This was previously needed to resolve the shared_ptr for the handle.
It seems that the AtomSpace uses this holds() call a lot, and it was
paying a huge price for this (since resolution involved a search of
the indexes, among other things).  So remeasure:


                   -A -X         -A -x            -A          -A -g
   test           AtomTable    AtomSpaceImpl    AtomSpace    Scheme
   name           K-ops/sec      K-ops/sec      K-ops/sec   K-ops/sec
   ----           ---------    -------------    ---------   ---------
  noop              inf         2500            3333         3333
  addNode           204          156.0            69.4          6.46
  addLink            56.8         46.1            34.1          5.31
  getType           769          588             110            4.83
  getTV             666          556             238            8.13
  setTV             625          909             167            4.37
  getHandleSet        0.428        0.420           0.221         -
  getNodeHandles     91.7         82.6            50.7           -
  getOutgoingSet    769          476             204            4.87
  getIncomingSet    909          909             250            4.11


AtomSpaceImpl perf changes are neutral. Some things got faster, some got
slower. There's a huge amount of measurement noise; these numbers can be
hard to repeat (everything in this diary shows best of three).

AtomSpace perf: Wow! Huge improvement!  

XXX FIXME: There seems to be a benchmark bug somewhere ... the AtomSpace
figures are high ONLY if I'm doing something else CPU intensive at the 
same tie (e.g. compiling).  Even watching youtube helps a little, but not
as much. This alone reslts in a 2x or 3x performance improvement.
Theorize:
   -- Some cache or TLB effect, with other CPU causing different cache line
      alignments, thus avoiding contention.
   -- Some kerne context-swtiching effect: perhaps the clock() timer calls
      stall in the kernel, until some other context switch event forces
      a return?  Noop. Stub out the calls to clock(), but perf does not
      improve. (based on total elapsed time, which uses gettimeofday())
   -- Some kind of thread-dispatching effect: the AtomSpaceAsync threads
      stall. Perhaps a sched_yield() is needed in ASRequest?  No... tried
      that and it didn't work.
   -- It may still be a kernel-context wtiching thing; its just not
      cureable with sched_yield() or disabling clock()...

This is bad. It makes the measurements untrustworthy...

Plowing on anyway ... scheme and python, head-to-head.

                   -A -g          -A -c
   test            Scheme         Python
   name           K-ops/sec      K-ops/sec
   ----           ---------    -------------    ---------   ---------
  noop             3333          4000
  addNode             6.46          7.62
  addLink             5.31          6.32
  getType             4.83          7.30
  getTV               8.13          7.87
  setTV               4.37          7.40
  getHandleSet         -            0.057
  getNodeHandles       -            7.25
  getOutgoingSet      4.87          7.59
  getIncomingSet      4.11          7.68


Looks like the python bottleneck, whatever it is, dominates everything else.
Again: a 1.7x perf improvement when running a concurrent compile on other
CPU; without that, the python numbers would be in the 3.8-4.3 range.



27 October 2013
---------------
Remeasure after removing a variety of un-needed serialization code.
Summary: pretty much everything got faster for every layer.
 -- The AtomSpace API is now 2x to 5x faster than it was in March 2013
    when this diary was started.
 -- The AtomTable API is mostly 2x-3x slower than it was in March 2013
    Essentially all of this slow-down is attributable to the use of
    std::shared_ptr for memory management.  Perhaps someday, BoehmGC
    should be explored ... 

                   -A -X         -A -x            -A          -A -g
   test           AtomTable    AtomSpaceImpl    AtomSpace    Scheme
   name           K-ops/sec      K-ops/sec      K-ops/sec   K-ops/sec
   ----           ---------    -------------    ---------   ---------
  noop             5000         5000            2000          inf
  addNode           208          147              68.0          8.93
  addLink            55.2         46.7            37.4          7.52
  getType          1111          588             588            7.58
  getTV             769          476             250            6.37
  setTV             714          435             213            7.63
  getHandleSet        0.440        0.424           0.265         -
  getNodeHandles     82.6         84.0            56.8           -
  getOutgoingSet    666          500             435           11.90
  getIncomingSet   1111          833             232            5.32


5 November 2013
---------------
Remeasure after ripping out the AtomSpaceAsync wrapper.  Basically, expect
the atomspace and atomtable performance to be nearly identical (there are
still some differences and minor additinal layers, but they're mosty the
same).  There is no AtomSpaceImpl any more, so not measured.

Summary:
 -- AtomTable is not as good as the April 1 results; which are 2x to 2.5x 
    faster than below.  The primary reason for this is the use of
    std::shared_ptr for all pointers.  Clearly, moving to garbage collection
    seems to be the right thing to do, long-run.

 -- AtomSpace is 6x to 15x faster. This is due to the removal of the
    excessive data copying of AtomSpaceAsync, as well as less locking
    for most operations.

 -- We are not measuring threaded performance here, but that should also
    much much better, maybe in the 20x-50x range, since the AtomSpaceAsync
    had a single lock and thus huge lock contention.  Now, most lock 
    contention is gone, except for atomtable operations (all getHandleSet
    calls require index lookups and so will contend).
 
                   -A -X         -A          -A -g
   test           AtomTable    AtomSpace    Scheme
   name           K-ops/sec    K-ops/sec   K-ops/sec
   ----           ---------    ---------   ---------
  noop             5000        5000          
  addNode           196         185         
  addLink            87          78.1       
  getType          1000         769        
  getTV             769         833       
  setTV             555         455      
  getHandleSet        0.471       0.426        
  getNodeHandles     95.2        87.0
  getOutgoingSet    769         588          
  getIncomingSet    526         555         
  removeAtom        161         156


22 January 2014
---------------
Remeasure, because locks had to be added to the truth-value getters.
Upshot: adding locks to the getTV path did not affect performance.
The setTV path got faster, probably due to cleaner impl.

Significant improvements in getIncoming/OutgoingSet ... not sure why.

Big drop in getNodesHandles, don't know why.

 
                   -A -X         -A          -A -g
   test           AtomTable    AtomSpace    Scheme
   name           K-ops/sec    K-ops/sec   K-ops/sec
   ----           ---------    ---------   ---------
  noop            10000        3333      
  addNode           217          93.5         
  addLink            88          54.3       
  getType          1000         833        
  getTV             769         833    
  setTV             769         476      
  getHandleSet        0.423       0.401        
  getNodeHandles     41.8        30.5
  getOutgoingSet    833         666          
  getIncomingSet    714         555         
  removeAtom        145         130


7 March 2014
------------
Due to bug #524, scheme was always being interpreted, never compiled.
The previous commit fixes this, the setting #:compile? #t on
eval-string.  This should improve scheme performance.  Does it?
No, it des NOT. Gack. But of course. Compiling every time is just
going to slow everything down. Really, we should compile just once.
and measure the compiled code. Gack.  This needs a new benchmark...


                      -A -g
   test              compile
   name             K-ops/sec
   ----             ---------
  noop                 inf
  addNode              14.3
  addLink               6.67
  getType               0.826
  getTV                 2.04
  setTV                 1.92
  getOutgoingSet        1.92
  getIncomingSet        1.49
  removeAtom            0.392

                     27 Oct      -g       -G -r100   -G -r100
   test               2013      memo       interp     compile
   name            K-ops/sec   K-ops/sec  K-ops/sec  K-ops/sec
   ----            ---------   ---------  ---------  ---------
  noop                  inf      inf         inf
  addNode              8.93      9.61       10.7
  addLink              7.52      7.82        5.76
  getType              7.58      7.81       12.2
  getTV                6.37      8.62       13.4
  setTV                7.63      7.81        7.34
  getOutgoingSet      11.90     13.9        13.2
  getIncomingSet       5.32      8.62       12.6
  removeAtom            -       10.4        10.6

Comparing today's interpreted measurements to those from Oct 2013, looks
like everything improves.  The improvements are all due to the faster
underlying atomspace.

8 March 2014
------------
Again.  New flags:
 -g -M is now same as old -g : it runs scheme, and memoizes (memoization
       is run outside the timing loop).
 -g -C run scheme, and compile (compilation is done outside of timing loop)
 -g run scheme, raw, straight up interpretation: no memo, not compile.
 
                  previous     -g -M       -g         -g -C   
   test           memoize     memoize     interp     compile  
   name          K-ops/sec   K-ops/sec   K-ops/sec  K-ops/sec
   ----          ---------   ---------   ---------  ---------
  noop               inf         inf       inf         inf
  addNode           9.61         10.1       6.04        8.62
  addLink           7.82         6.80      2.36        7.24
  getType           7.81         6.85      4.69        7.41
  getTV             8.62         8.13      5.61        6.94
  setTV             7.81         7.25      3.73        7.46
  getOutgoingSet   13.9          6.41      4.75        7.12
  getIncomingSet    8.62         5.65      4.70        6.25
  removeAtom       10.4          7.87      4.19        5.15

In the above, "previous" is the column copied from above, yesterday.
It should be identical to -g -M but its not.  Today's runs seem a
lot slower.  There's a huge variability in reported times, run-to-run.
I don't like this failure to reproduce. I don't understand it.
By contrast, most of the other runs don't have this huge variability,
they are very consistent run to run.  Makes me think that there is
some cache/tlb invalidation that is dominating this.


                -g -M -r100    -g -r100   -g -C -r100    -A
   test           memoize       interp      compile    AtomSpace
   name          K-ops/sec     K-ops/sec   K-ops/sec   K-ops/sec
   ----          ---------     ---------   ---------   ---------
  noop               inf         inf          inf
  addNode           37.0        10.7         37.1         89.8
  addLink           57.3         3.32        67.0         58.1
  getType          102          11.8        128          833
  getTV            152          13.0        227          833
  setTV            128           7.25       203          625
  getOutgoingSet   217          12.6        288          697
  getIncomingSet   172          12.7        242          666
  removeAtom        69.4        10.7         74.6        130


Now, the above are a lot more interesting.   First, look at the -g -r100
column, and compare it to the -g column from the previous table.  The
difference between these two is the cost of entering and exiting guile.
The plain -g column enters guile, does one op, and exists.  The -r100
column enters guile, does 100 ops, and then exits.

The memoized and compiled columns should be compared to the interpreted
column: this shows exacly how painfully slow the interpreter is.  The
fact that the compiled and memoized columns are quite close to one
another illustrates that there's really not much going on in the
bindings.  For contrast, the last column shows the atomspace
performance. (repeated the measurement here, but it seems same as
before).

To summarize:
 1) the scheme bindings are pretty "thin"
 2) There's a huge overhead from using the scheme interpreter, as opposed
    to running compiled (or even memoized) code.
 3) There's a huge overhead to enter and exit the scheme subsystem.


So: there are two distinct costs: 
 A) Cost of calling a C++ smob function, from scheme.  The above numbers
    suggest that the ballpark cost of this is about 2uS or 3uS 
    (getOutgoingSet, getTV) but sometimes much worse: 7uS for getType,
    and 15 uS for addNode. This suggests severe cache-contention issues.

 B) Cost of calling scheme from C++. This seems to take about 175uS and
    sometimes much worse: 300uS to 400uS.  Perhaps this could be worked
    around by having another thread on standby:  linux thread-context
    switching should run abt about 5uS per switch ...

