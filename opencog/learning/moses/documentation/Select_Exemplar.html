<!DOCTYPE html>
<!-- saved from url=(0048)http://blog.opencog.org/2012/02/07/tuning-moses/ -->
<html dir="ltr" lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title>Tuning MOSES | OpenCog Brainwave</title>
<link rel="stylesheet" type="text/css" media="all" href="./Select_Exemplar_files/style.css">
				
<meta name="generator" content="WordPress 3.0.1">
<link rel="canonical" href="./Select_Exemplar_files/Select_Exemplar.html">
<link rel="stylesheet" type="text/css" href="./Select_Exemplar_files/defaults.css">
<link rel="stylesheet" type="text/css" href="./Select_Exemplar_files/theme.css">
</head>

<body class="single single-post postid-326 logged-in">
<div id="wrapper" class="hfeed">

	<div id="main">

		<div id="container">
			<div id="content" role="main">


				<div id="post-326" class="post-326 post type-post hentry category-design category-development category-documentation category-theory tag-learning tag-machinelearning tag-moses">
					<h1 class="entry-title">Tuning MOSES</h1>

					<div class="entry-meta">
						<span class="meta-prep meta-prep-author">Posted on</span> <a href="./Select_Exemplar_files/Select_Exemplar.html" title="8:53 pm" rel="bookmark"><span class="entry-date">February 7, 2012</span></a> <span class="meta-sep">by</span> <span class="author vcard"><a class="url fn n" href="http://blog.opencog.org/author/linasv/" title="View all posts by Linas Vepstas">Linas Vepstas</a></span>					</div><!-- .entry-meta -->

					<div class="entry-content">
<p>The following is a verbatim copy of a blog entry about MOSES. In fact, it is mostly about
tuning the operation of the <tt>select_exemplar()</tt> code so that MOSES finds solutions
more quickly.</p>
						<p>I’ve been studying <a href="http://wiki.opencog.org/w/MOSES">MOSES</a> recently, with an eye towards performance tuning it. Turns out optimization algorithms don’t always behave the way you think they do, and certainly not the way you want them to.</p>
<p>Given a table of values, MOSES will automatically learn a program that reproduces those values. That is, MOSES performs table regression: given N columns of “input” values, and one column of “output”, MOSES will create a program that outputs the output, given the inputs. &nbsp;MOSES can deal with both floating point and boolean inputs, and thus can learn, for example, expressions such as ((x&lt;2) AND b) OR (x*(y+1) &gt;3). &nbsp;MOSES programs are real “programs”: it can even learn branches and loops, although I won’t explore that here. For performance tuning, I studied the 4-parity problem: given 4 input bits, compute the parity bit. &nbsp;Written out in terms of just AND, OR and NOT, this is a fairly complex expression, and is rather non-trivial to learn.</p>
<p>MOSES performs learning by keeping a “metapopulation” of example programs, or “exemplars”. &nbsp;These are graded on how well the match the output, given the inputs. For the 4-parity problem, there are 2<sup>4</sup>=16 different possible inputs; a given program may get any number of these correct. &nbsp;For example, there are 16 ways to get one answer wrong; 16×15 ways to get two wrong, 16×15×14 ways to get three wrong, <em>etc.</em> This is the binomial distribution: (16 choose <em>k</em>) ways to get <em>k</em> answers wrong, in general. But this doesn’t mean that there are only 16 different programs that get one answer wrong: there are zillions: some simple, some very very complex.</p>
<p>As MOSES iterates, it accumulates a metapopulation of programs that best fit the data. As soon as it finds a program that gets more correct answers than the others, the old metapopulation is wiped out; but then, it starts growing again, as new programs with equal score are found. &nbsp;This is shown in the following graph:</p>
<div id="attachment_333" class="wp-caption alignnone" style="width: 650px"><a href="./Select_Exemplar_files/stats-r7.png"><img class="size-full wp-image-333" src="./Select_Exemplar_files/stats-r7.png" alt="Metapopulation size as function of generation." width="640" height="480"></a><p class="wp-caption-text">Metapopulation size as function of generation number.</p></div>
<p>The red line shows the metapopulation size (divided by 50), as a function of the generation number (that is, the iteration count). &nbsp;It can be seen to collapse every time the score improves; here, the “minus score”, in green is the number of wrong answers: a perfect score has zero wrong answers; the program stops when a perfect score is reached.</p>
<p>In blue, the complexity of the program — actually, the complexity of the least complex program that produces the given score. Computing the parity requires a fairly complex combination of AND’s OR’s and NOT’s; there is a minimum amount of complexity such a program can have. &nbsp;Here, for example, are two different programs that compute the parity perfectly, a short one:</p>
<pre>﻿﻿and(or(and(or(and(!$1 !$2) and(!$3 $4)) or(!$2 !$3))
   and($1 $2) and($3 !$4))
   or(and(!$1 $2) and($1 !$2) and(!$3 !$4) and($3 $4)))</pre>
<p>and a longer one:</p>
<pre>or(and(or(and(or(!$1 !$3) $4) and(or($1 !$2) !$3 !$4)
   and(or($3 $4) $2)) or(and(or(!$1 !$4) $2 !$3)
   and(or(!$2 $3) $1 $4) and(!$1 !$4) and(!$2 $3)))
   and($1 !$2 $3 !$4))</pre>
<p>More on complexity later.</p>
<p>But first: how long does it take for MOSES to find a solution to 4-parity? It turns out that this depends strongly on the random-number sequence. &nbsp;MOSES makes heavy use of a random number generator to explore the problem space. &nbsp;Each run can be started with a different seed value, to seed the random number generator. &nbsp;Some runs find the correct solution, some take a surprisingly long amount of time. Amazingly so: the distribution appears to follow a logarithmic distribution, as in the following graph:</p>
<div id="attachment_336" class="wp-caption alignnone" style="width: 650px"><a href="./Select_Exemplar_files/k4-bigger.png"><img class="size-full wp-image-336" src="./Select_Exemplar_files/k4-bigger.png" alt="" width="640" height="480"></a><p class="wp-caption-text">Runtime, showing temperature dependence</p></div>
<p>One the vertical axis, the amount of time, in seconds, to find a solution. One the horizontal axis, the order in which a solution was found, out of 20 random attempts.  The way to read this graph is as follows: there is a probability Pr=1/20 chance of finding a solution in about 10 seconds.  There is a Pr=2/20 chance of finding a solution in about 20 seconds, <em>etc.</em> Continuing: about a Pr=6/20 chance of finding a solution in less than about 100 seconds, and a Pr=17/20 chance of finding a solution in less than about 1000 seconds.</p>
<p>The shape of this graph indicates that there is a serious problem with the current algorithm. To see this, consider running two instances of the &nbsp;algorithm for 300 seconds each. Per the above graph, there is a 50-50 chance that each one will finish, or a 75% chance that at least one of them will finish. &nbsp;That is, we have a 75% chance of having an answer after 600 CPU-seconds. &nbsp;This is better than running a single instance, which requires about 900 seconds before it has a 75% chance of finding an answer! &nbsp;This is bad. &nbsp;It appears that, in many cases, the algorithm is getting stuck in a region far away from the best solution.</p>
<p>Can we do better? Yes. Write <em>p </em>= Pr(<em>t&lt;T</em>) for the probability that a single instance will find a solution in less time than T. &nbsp;Then, from the complexity point of view, it would be nice if we had an algorithm if two instances did NOT run faster than a single instance taking twice as long; that is, if</p>
<p style="padding-left: 30px">Pr(<em>t&lt;2T</em>) ≤ <em>p<sup>2</sup>+2p(1-p)</em></p>
<p>The first term, <em>p<sup>2</sup></em>, is the probability that both instances finished. &nbsp;The second term is the probability that one instance finished, and the other one did not (times two, as there are two ways this could happen). &nbsp; More generally, for <em>n</em> instances, &nbsp;we sum the probability that all <em>n</em> finished, with the probability that <em>n-1</em> finished, and one did not (<em>n</em> different ways), <em>etc.</em>:</p>
<p style="padding-left: 30px">Pr(<em>t&lt;nT</em>) ≤&nbsp;<em>p<sup>n</sup> + np<sup>n-1</sup>(1-p) + n(n-1)p<sup>n-2</sup>(1-p)<sup>2</sup> + … + np(1-p)<sup>n-1</sup></em></p>
<p>This inequality, this desired bound on performance, has a simple solution, given by the exponential decay of probability:</p>
<p style="padding-left: 30px">Pr(<em>t&lt;T</em>) = 1-exp(<em>-T/m</em>)</p>
<p>As before, &nbsp;Pr(<em>t&lt;T</em>) is the probability of finding a solution in less than time <em>T</em>, and<em> m</em> is the mean time to finding a solution (the expectation value). To better compare the measured performance to this desired bound, we need to graph the data differently:</p>
<p><a href="./Select_Exemplar_files/log-bound-unclamped.png"><img class="alignnone size-full wp-image-356" src="./Select_Exemplar_files/log-bound-unclamped.png" alt="Showing the exponential bound" width="640" height="480"></a></p>
<p>This graph shows the same data as before, but graphed differently: the probability of not yet having found a solution is shown on the horizontal axis. Note that this axis is logarithmic, so that the exponential decay bound becomes a straight line. &nbsp;Here, the straight purple line shows the bound for a 500 second decay constant; ideally, we’d like an algorithm that generates points below this line.</p>
<p>Before continuing, a short aside on the label “<em>temp</em>“, which we haven’t explained yet.&nbsp;During the search, MOSES typically picks one of the simplest possible programs out of the current metapopulation, and explores variations of it, it explores its local neighborhood. &nbsp;If it cannot find a better program, it picks another, simple, exemplar out of the metapopulation, and tries with that, and so on. &nbsp; It occurred to me that perhaps MOSES was being too conservative in always picking from among the least complex exemplars. &nbsp;Perhaps it should be more adventurous, and occasionally pick a complex exemplar, and explore variations on that. &nbsp; The results are shown in the green and blue lines in the graph above. &nbsp;The <tt>select_exemplar()</tt> function uses a Boltzmann distribution to pick the next exemplar to explore.  That is, the probability of picking an exemplar of complexity <em>C</em> as a starting point is</p>
<p style="padding-left: 30px">exp(-<em>C/temp</em>)</p>
<p>where <em>temp</em> is the “temperature” of the distribution. The original MOSES algorithm used <em>temp</em>=1, which appears to be a bit too cold; a temperature of 2 seems about right. &nbsp;With luck, this new, improved code will be checked into BZR by the time you read this.</p>
<p>There is another issue: the unbounded size of the metapopulation. When MOSES stalls, grinding away and having trouble finding a better solution, the size of the metapopulation tends to grow without bounds, linearly over time. It can get truly huge: sometimes up to a million, after a few thousand generations. &nbsp;Maintaining such a large metapopulation is costly: it takes up storage, and eats up CPU time to keep it sorted in order of complexity. &nbsp;Realistically, with a metapopulation that large, there is only a tiny chance (exponentially small!) that one of the high-complexity programs will be selected for the next round. The obvious fix is to clamp down on the population size, getting rid of the unlikely, high-complexity members. &nbsp; I like the results so far:</p>
<div id="attachment_357" class="wp-caption alignnone" style="width: 650px"><a href="./Select_Exemplar_files/log-bound-clamped.png"><img class="size-full wp-image-357" src="./Select_Exemplar_files/log-bound-clamped.png" alt="clamped data" width="640" height="480"></a><p class="wp-caption-text">Runtime, using a limited population size.</p></div>
<p>Clamping the population size clearly improves performance — by a factor of two or more, as compared to before. &nbsp;However, the troublesome behavior, with some solutions being hard to discover, remains.</p>
<p>Now, to attack the main issue: Lets hypothesize what might be happening, that causes the exceptionally long runtimes. &nbsp;Perhaps the algorithm is getting stuck at a local maximum? &nbsp;Due to the knob-insertion/tweaking nature of the algorithm, there are no “true” local maxima, but some may just have very narrow exits. &nbsp;The standard solution is to apply a simulated-annealing-type trick, to bounce the solver out of the local maximum. &nbsp;But we are already using a Boltzmann factor, as described above, so what’s wrong?</p>
<p>The answer seems to be that the algorithm was discarding the “dominated” &nbsp;exemplars, and was keeping only those with the best score, and varying levels of complexity. It only applied the Boltzmann factor to the complexity. &nbsp;What if, instead, we applied the Boltzmann factor to mixture of score and complexity? &nbsp;Specifically, lets try this:</p>
<p style="padding-left: 30px">exp(-(<em>C – S W) / temp</em>)</p>
<p>Here, <em>C</em> is the complexity, as before, while <em>S</em> is the score, and <em>W</em> a weight. &nbsp;That is, some of the time, the algorithm will select exemplars with a poor score, thus bouncing out of the local maximum. &nbsp;Setting <em>W</em> to zero regains the old behavior, where only the highest-scoring exemplars are explored. &nbsp;So .. does this work? Yes! Bingo! Look at this graph:</p>
<div id="attachment_363" class="wp-caption alignnone" style="width: 650px"><a href="./Select_Exemplar_files/log-bound-weighted1.png"><img class="size-full wp-image-363" src="./Select_Exemplar_files/log-bound-weighted1.png" alt="score-weighted annealing" width="640" height="480"></a><p class="wp-caption-text">Score-weighted Annealing</p></div>
<p>Two sets of data points, those for <em>W</em>=1/4 and 1/3, look very good. &nbsp;Its somewhat strange and confusing that other <em>W</em> values do so poorly. &nbsp; I’m somewhat worried that the <em>W</em>=1/4 value is “magical”: take a look again at the very first graph in this post. &nbsp;Notice that every time a better solution is found, the complexity jumps by about 4. &nbsp;Is this ﻿the <em>W</em>=1/4 value special to the 4-parity problem? Will other problems behave similarly, or not?</p>
<p>I’m continuing to experiment. Collecting data takes a long time.  More later…  The above was obtained with the  code in bzr revision 6573, with constant values for “temp” and “weight” hand-edited as per graphs.  Later revisions have refinements that fundamentally alter some loops, including that in <tt>select_exemplar()</tt>, thus altering the range of reasonable values, and the meaning/effect of some of these parameters. Sorry <img src="./Select_Exemplar_files/icon_smile.gif" alt=":-)" class="wp-smiley"> </p>
<p>I do hope that this post does offer some insight into how MOSES actually works. &nbsp;A general overview of MOSES can be found on the <a href="http://wiki.opencog.org/w/MOSES">MOSES wiki</a>, as well as a detailed description of the <a href="http://wiki.opencog.org/w/MOSES_algorithm">MOSES algorithm</a>. But even so, the actual behavior, above, wasn’t obvious, at least to me, until I did the experiments.</p>
<h2>Appendix: Parallelizability</h2>
<p>A short footnote about the generic and fundamental nature of the exponential decay of time-to-solution in search problems. Earlier in this post, there is a derivation of exponential decay as the result of running <em>N</em> instances in parallel. &nbsp; How should this be understood, intuitively?</p>
<p>Search algorithms are, by nature, highly parallelizable: there are many paths (<em>aka exemplars</em>) to explore; some lead to a solution, some do not. &nbsp;(An exemplar is like a point on a path: from it, there are many other paths leading away). &nbsp;A serial search algorithm must implement a chooser: which exemplar to explore next?&nbsp;If this chooser &nbsp;is unlucky/unwise, it will waste effort exploring exemplars that don’t lead to a solution, before it finally gets around to the ones that do. &nbsp;By contrast, if one runs <em>N</em> instances in parallel (<em>N</em> large), then the chooser doesn’t matter, as the <em>N-1</em> ‘bad’ exemplars don’t matter: the one good one that leads to a solution will end the show.</p>
<p>Thus, we conclude: if a serial search algorithm follows the exponential decay curve, then it has an optimal chooser for the next exemplar to explore. &nbsp;If it is “worse” than exponential, then the chooser is poorly designed or incapable. &nbsp;If it is “better” than exponential, then that means that there is a fixed startup cost associated with each parallel instance: cycles that each instance must &nbsp;pay, to solve the problem, but do not directly advance towards a solution. &nbsp;Ideal algorithms avoid/minimize such startup costs. &nbsp;Thus, the perfect, optimal algorithm, when run in serial mode, will exhibit exponential solution-time decay.</p>
<p>The current MOSES algorithm very nearly achieves this for 4-parity, as shown in this last figure, which compares the original chooser to the current one (bzr revno 6579)</p>
<p><a href="./Select_Exemplar_files/final.png"><img class="alignnone size-full wp-image-366" src="./Select_Exemplar_files/final.png" alt="runtime, tuned chooser" width="640" height="480"></a></p>
<p><em> </em></p>

</div><!-- .entry-content -->


</body></html>
