
    Diary of MOSES performance and Enhancement Work
    -----------------------------------------------
       Linas Vepstas -- started June 2014


Diary of notes, thoughts, measurements, pertaining to work on MOSES.

Boosting in MOSES
-----------------
There are several very different ways of envisioning boosting in MOSES,
each being more or less compatible with the formal definition of
boosting. See http://en.wikipedia.org/wiki/Boosting_(meta-algorithm)

(Caution: the formulas in http://en.wikipedia.org/wiki/AdaBoost
are incorrect and misleading, and suggests an inapprorpriate algorithm
and implementation. Beware!)

The formal definition of boosting invokes the idea of a weighted
ensemble of 'weak learners'.  Each member of the ensemble is given a
weight, with the weights being issued so as to minimize the total error
of the prediction being made by the ensemble. The accuracy of the
ensemble is then measured, and used to identify those samples in the
training set that are being classified incorrectly.  New weak learners
are then trained, with an emphasis on correctly classifying those
samples that were gotten incorrect by the ensemble.

For MOSES, it appropriate definition of 'ensemble' is ambiguous.
The ensemble can be: 

  A) The set of instances in the current deme
  B) The evolutionary history of a given exemplar
  C) The collection of combo trees (exemplars) in the metapopulation.
  D) Some combination of the above.

Let's recall the definition of an exemplar, a deme, and a
metapopulation.

 *) An 'exemplar' is a single, fixed combo tree.  It has some
    evolutionary heritage, having evolved from earlier trees.

 *) The metapopulation is the current collection of the fittest
    (most accurate) exemplars.

 *) A deme is an exemplar that has been decorated with knobs, together
    with a large set of possible knob settings (instances).  Each
    instance is conceptually a single combo tree, that differs
    structurally from its parent exemplar in some 'minor' way.

Lets look at how the boosting algo would work for each scenario. But
first, lets sketch the current 'hill-climbing' algo, as currently
performed by MOSES.

--------------
Hill-climbing) The current canonical algo.

 H.1) Select an exemplar from the metapopulation, create deme.
 H.2) Generate N instances by turning N knobs.
 H.3) Score all N instances, using a uniform weight on all samples.
 H.4) Select the highest-scoring instance. (this is the hill-climb).
 H.5) Go to step H.2

 H.6) Terminate above loop in some way. (e.g. top of hill reached)
 H.7) Put M of the top-scoring instances into the metapopulation.
 H.8) Go to step H.1

 H.9) Terminate above loop in some way.
 H.10) Use the top K exemplars in the metapopulation to create a
       traditional ensemble, for off-line prediction/classification.


--------------
Scenario A) The set of instances is an ensemble.  In this scenario, the
boosting algorithm becomes a variant the current 'hill-climbing' algo.
The steps are numbered below so that they correspond to the
hill-climbing steps. To summarize: step H.3 is replaced by the Boost
algo, so that H.3 is a bunch of steps.

 A.1) Select an exemplar from the metapopulation. (same as H.1)
 A.2) Generate N instances by turning N knobs. (same as H.2)
 A.3.1) Score all N instances, using a uniform weight on all samples.
 A.3.2) Pick the best instance, per boost, add it to the ensemble.
 A.3.3) Adjust per-sample weight based on ensemble mis-prediction
 A.3.4) Re-score remaining N-1 instances with new weighted scorer.
 A.3.5) Go to step A.3.2

 A.3.7) Terminate above loop using some criteria,
 A.4) Select the highest-scoring instance, based on the weighted
      scorer available at termination time.
 A.5) Go to step A.2 (Same as H.5)

 A.6.1) Terminate above loop in some way. (e.g. top of hill reached)
 A.6.2) Discard all row-weights.
 A.7) Put M of the top-scoring instances into metapop.  (Same as H.7)
 A.8) Go to step A.1 (Same as H.8)

 A.9) Same as H.9
 A.10) Same as H.10

The above seems to make sense, and superfically seems to be compatible
with the formal definition of boosting.  However, step A.4 seems somewhat
strange. Lets ponder this.

In step A.4, we pick the instance with the highest weighted score,
rather than the instance with the highest overall accuracy. What does
this mean?  What effect does this have?  Well, it selects for an
instance that gets right what previous instances got wrong.  By the time
that we get past step A.7, what we've done is to create M exemplars that
got things right that the initial exemplar did not.  Do any of these M
exemplars have a better absolute value accuracy score than the initial
exemplar? Unclear, possibly not.  So we have research question 1:

 RQ.1) Compare the 'raw' score of the M exemplars from step A.7 to the
       score from ordinary hill-climbing H.7.  Is it possible that the
       weighted hill-climb of step A.4 somehow avoided a plateau that
       H.4 may have gotten stuck in?  viz. Can scenario A) boosting
       actually outperform ordinary hill-climbing? (At each step?)

 RQ.2) Is the final metapopulation of A.10 stronger/fitter/better than
       that of H.10, given same amount of CPU time?  (Hill-climbing is
       less CPU intensive, so can perform more iterations).

--------------
Secnario B) The ensemble is the evolutionary history of an exemplar.

There's actually two Scenario B's. So:

----
Scenario Ba) Just like Scenario A, except that at step A.6.2, we save
the final weights, associate them with this particular exemplar.  These
weights are then used in step A.3.1, instead of a uniform distribution.

This begs the question:
  RQ.3) Is scenario Ba better than plain-A?  It seems to offer some kind
        of continuinty with what came before, but does that matter?  The
        only reason the top-of-hill A.6.1 is evaded is because the knobs
        are all different ...

----
Scenario Bh) Just like plain-old hill-climbing H, except that steps H.3
and H.7 are modified. Step H.7 is replaced by the boost algo, with step
H.3 using the appropriate boosting scoring function.

 Bh.1) Select an exemplar from the metapopulation. (Same as H.1)
 Bh.2) Generate N instances by turning N knobs. (Same as H.2)
 Bh.3x) Score all N instances, using a weight distribution that was
        previously saved with the exemplar.
 Bh.4) Select the highest-scoring instance. (Same as H.4)
 Bh.5) Go to step Bh.2 (Same as H.5)

 Bh.6) Terminate above loop in some way. (e.g. top of hill reached)
 Bh.7.1) Select M top-scoring instances. 
 Bh.7.2) Add that instance to an ensemble that contains its parent.
 Bh.7.3) Update row weights for this ensemble.
 Bh.7.4) Place the ensemble into the metapopulation.
 Bh.8) Go to step Bh.1 (Same as step H.8)

Here, the ensemble consists only of an exemplar, and all of its parents
from which it is descended.  These seems to be rather thin.  The problem
is that step 6-8 don't happen that often -- and so the ensembles never
get very big.  Naively, boosting would seem to work best when the
ensembles have many members. This begs the question:

 RQ.4) How big to the ensembles get, for the Bh approach?

and 
 RQ.5) Same as RQ.2 -- net-net, is this better?

--------------
Secnario C) Maintain an ensemble drawn from the metapopulation.

Again, this is a variant of the canonical hill-climbing algo, with
modifications show below.  To summarize, step H.7 is replaced by the
boost algo.  Unlike Bh.7, though, the ensemble is drawn from the
entire metapop.

 C.1) Select an exemplar from the metapopulation. (Same as H.1)
 C.2) Generate N instances by turning N knobs. (Same as H.2)
 C.3x) Score all N instances, using weighted scorer.
 C.4) Select the highest-scoring instance. (Same as H.4)
 C.5) Go to step C.2 (Same as H.5)

 C.6) Terminate above loop in some way. (Same as H.6)
 C.7.1) From 1 till M, do:
 C.7.2) Pick the best instance, per boost, add it to the ensemble.
 C.7.3) Adjust per-sample weight based on instance mis-prediction.
 C.7.5) Re-score remaining M-1 instances with new weighted scorer.
 C.7.6) Place all instances into the metapop.
 C.7.7) Go to step C.7.2

 C.8) Go to step C.1 (Same as H.8)

 C.9) Terminate above loop in some way. (Same as H.9)
 C.10) Use the ensemble built in step C.7.2 as the final output of
       the process. i.e. The ensemble is what is finally used for
       off-line prediction/classification.

Scenario C appears to be the one that is closest in spirit to the 
traditional boosting algos, and thus will be implemented first.

Scenario C does have interesting issues with the metapopulation
management.  With each iteration, the scoring criteria for managing the
metapopulation will change, and the metapop will contain a mixture of
trees from earlier rounds, and the current round.  This poses a
question: 

 RQ.6) Should the metapop be rescored with the new re-weighted scorer,
       or is it OK to leave stale score in there?

Design issues
-------------
How to implement Scenario C:
- Add single weight to scored combo tree. 

- Maintain a weight column .. where? In a new boosing scorer.
  Needs to be like the behave_cscore, but different.

Scoring Requirements:
1) hill-climber needs a *single* scalar score, to determine which way is "up".
2) This scalar score needs to be a sum of:
   a) a complexity penalty, obtained from the combo tree,
   b) a diversity penalty, based on
      i) the metapopulation
      ii) possibly the behavioral score
      iii) possibly the combo tree
   c) a function of the behavioral score, such as a direct sum,
      or possibly a weighted bscore (e.g. via boosting).

3) Multiple tables will be explicitly NOT supported. Why? Because its
   not clear how to combine these.  One way would be to combine them
   into one big table.  Another possibility is to use a different
   bscorer on each; but I don't know of a use-case for that.  I don't
   know how to weight these relative to one-another.  So, for now, 
   bonly one table.

Solution:
-- remove multibscore (done)
-- provide a utility for #2 that assembles a composite score from the 
   parts, and can be queried for the scalar score. Done: its behave_cscore.

Notes/TODO:
-- complexity is not being factored in ...  Done.
-- scorer is currently not using the user-specified weight column !?
   Done.  I thinnk there's nothing to do ... except maybe test ... 
-- create unit test for boosting
-- document eval-table
   maybe rename eval-combo, or eval-combo-table

Problems:
1) best score must now be the ensemble score.
2) the "update best cands" is no longer trying to beat the best in the
   metapop, because the criteria for "best" haas changed.  This seems to
   mean that the entire metapop needs to be rescored.  Maybe leave this
   as open question.




What is a deme_t?

typedef instance_set<composite_score> deme_t;
struct instance_set : public vector<scored_instance<ScoreT> >
All instances in instance set have same field description.

why is there ever more than one instance set??

==============

24 June 2014
------------
Compare the boosted & non-boosted version of the parity problem.

Time to perfect score:
time moses -Hpa -k3 --boost=1  : 1/2 second
time moses -Hpa -k4 --boost=1  : 1.8 seconds
time moses -Hpa -k5 --boost=1  : 9.2 seconds
time moses -Hpa -k6 --boost=1  : 6m13 seconds

Wow!  Recall that without boosting, -k4 would take minutes, (depending
very strongly on the initial random seed) and that -k5 was mostly 
unsolvable (would take an hour or two for a few lucky random seeds)

Wow, even -k6 finds a solution, although it is long, and takes much
more time to find.   ... and needs to have --reveist=50

So: -k4 speedup == 100x roughly
    -k5 speedup == more than 500x

Hmm  different rand seeds:

           -k4     -k5      -k6      -k6
 random    time    time     time     size
  seed    (secs)  (secs)   (secs)  (trees)
 ------   ------  ------   ------  -------
   r0      1.8      11.2     568
   r1      1.8       9.2     392
   r2      2.5      10.4     190
   r3      2.6      11.8     260
   r4      2.3       9.2     260     633
   r5      2.2      18.0    2385    2244
   r6      1.8      12.7     152     413
   r7      2.6      13.5    2360    2266
   r8      2.0      12.3     501     963
   r9      2.0       9.9    1207    1644

size(trees) above it the number of trees in the ensemble.

crazy ideas:

one rather crazy experiment would be to find three with almost equal
scores, combine them as indicated, run reduct, and then repeat.  How
badly would this damage things?  Would it be an effective way of
obtaining a single tree that is almost right, and then can be finished
off in the usual fashion?

or perhaps the weights should be forced to be valued as small fractions,
so that they can be exactly combined ... 

July 2014
---------
Discrete AdaBoost reference:

 * Yoav Freund, Robert E. Schapire, "A Short Introduction to Boosting"
   (1999) Journal of Japanese Society for Artificial Intelligence, vol. 14
   issue 5 pp 771-780

Boosting as gradient-descent, for contin-valued problems:
 * Robert E. Schapire, YoramSinger, "Improved boosting algorithms using
   confidence-rated predictions." (1998) Proc 11th Ann Conf Computational
   Learning Theory pp 80-91

Outlier mitigation:
 * GentleAdaBoost
 * BrownBoost


Selection scoring with boosting
-------------------------------

Performance of the selectionUTest on select.csv with differrent evals:

     num_evals  error   dedupe   nodupe
     ---------  -----   ------   ------
          1K     58      29        32
          2K     58      29        32 
          5K     56      29        28
         10K     56      28        22
         20K     56      20        11
         40K     56      15         1
         80K     56      15         1

This is a very degenerate dataset: select.csv contains a total of 329 rows.
Of these only 60 are unique, so max score can't really do better than that.

Evaluation halts after after 49K evals ... 

The dedupe column shows results for the select-less-dupe.csv file  This one
has 329 rows, of which  286 are unique.

The nodupe column shows results for the select-no-dupe.csv file  This one
has 329 rows, all of which are unique.

Above results are off-by-one, final results are 55, 14 and 0 not 56, 15 and 1.

Arghh. Also, inappropriatte weight tallys.  Redo the above.
select.csv has 329 rows. The 0.8 and 0.9 bounds are at
   select_bscore: lower_bound = 1329.39 upper bound = 1650.22

XXXXX TODO


Localized Boosting; Mixture of Experts
--------------------------------------
To be used with the pre scorer. 

The pre scorer is used to mine for "experts": combo trees that give
correct answers only on that subset of positive results that they are
certain about.

To boost the pre scorer, the boosting discriminator needs to:
-- get rewarded for TP answers
-- get punished for FP answers
-- no-op for FN, TN answers

Thus, the weights only get adjusted for rows that are part of the
positive set. 


search terms:
-- localized boosting  LocBoost
-- boosting by resampling.


Localized Boosting
------------------
Definition: "Localized Boosting" and the LocBoost algo is used to
train and apply experts only in a "local" part of the dataset. That
is, the experts are expert only for those inputs that are near (e.g
Hamming-distance-near) to thier domain of expertize.  This idea makes
sense if the inut data is real-valued but low-dimensional: the local
experts are expert only in some submanifold of R^n.  For us, the input
is high-dimensional and is boolean.


Irrelevant variables, Ignored variables, Local experts.
-------------------------------------------------------
The precision-scorer does this: a given combo tree pays attention to
only certain inputs, but ignores all others.  If those inputs are
present/absent, then the row is selected.  However, it could happen
that the training set simply did not include (enough) values of the
ignored variables that could be important.  Thus, an expert should be
considered to be "reliable" if, for the subset that it selects, the
ignored variables are uniformly distributed.   We do not currently
make this kind of a check for truly irrelevant variables

The idea of localized boosting is to say "this expert is an expert
only when the pattern is selected *and also* the input training set
had a uniform distribution on these specific ignored variables".
If the training set did not have a uniform distribution of *some*
of the ignored variables, then the expert is no longer expert for
those cases (e.g. the confidence is ranked lower).


Precision, selection base results
---------------------------------
Create 3 files with top scorers marked s boolean 1 else 0.
select.csv: 70% mark should be at 0.7*329 = 230.2 so should
have 230 lines marked with 0 and 99 lines marked with 1.

   954.393 < div < 955.542

So preselect.csv is select.csv, with all scores above 955 are marked.
Ditto, preselect-less-dupe.csv and preselect-no-dupe.csv.

The result are tables w/ 329 data points, of which 230 lines are
false, 99 are true. (i.e. 230/329 = 70% of lines...)

Compare the -Hit and -Hselect scorer directly on preselect-no-dupe.csv

The 4 columns below are:

moses -Hit -Yval -ipreselect-no-dupe.csv 
moses -Hit -Yval -ipreselect-no-dupe.csv --boost=1
moses -Hselect  -iselect-no-dupe.csv -q0.7
moses -Hselect  -iselect-no-dupe.csv -q0.7 --boost=1


     num_evals  Hit         Hit            Hselect     Hselect
                  (time)   boost (time)      (time)  boost (time)
     --------- ---------   ------------   ----------  ----------
        10K    46 (0m17s)    30 (0m15s)   48 (0m19s)  36 (0m14s)
        20K    41 (0m42s)    22 (0m47s)   36 (0m39s)  21 (0m41s)
        40K    29 (2m34s)     8 (2m8s)    27 (2m24s)   6 (1m46s)
        80K    24 (9m21s)     0 (4m35s)   26 (8m40s)   0 (2m56s)
       160K    22 (30m18s)    -           22 (24m10s)  -

Conclude: The differences between Hit and Hselect are small and would
surely wash out if the results were averaged over different initial
random seeds. This is not a surprise; both scorers are doing essentially
the same thing for this dataset.  So the above is really just a
"sanity check" to make sure that the code is good, and, indeed, it
seems to be good.

--------------------------------------------

Try again on the dataset with duplicates. As before, there are 329 rows
total, with 230 false, and 99 true.  However, due to duplicates, the
best possible score is -50, and the effective length is 329-2*50=229.

moses -Hit -Yval -ipreselect.csv 
moses -Hit -Yval -ipreselect.csv --boost=1
moses -Hselect  -iselect.csv -q0.7
moses -Hselect  -iselect.csv -q0.7 --boost=1


     num_evals  Hit         Hit            Hselect     Hselect
                  (time)   boost (time)      (time)  boost (time)
     --------- ---------   ------------   ----------  ----------
         5K                   3 (0m6s)                 4 (0m7s)
        10K    29 (0m16s)     0 (0m11s)   29 (0m16s)   0 (0m12s)
        20K     4 (0m35s)     -            4 (0m36s)   -
        40K     4 (1m24s)     -            4 (1m24s)   -
        80K     4 (4m7s)      -            4 (4m2s)    -
       160K     0 (4m45s)     -            0 (11m38s)  -


Notes:
-- preselect.csv has best score of 50, due to duplicate rows.
   All reported scores are realtive to this score.

-- The Hselect and the Hit scorers behave nearly identically, as
   expected for this case.


===============
The bug below got fixed. The fix is to always score relative to the
best-possible score.

Since there are 329 rows, this means 329-51 = 278 rows, half of which
are 278/2 = 139 which is what the ensemble scorer needs to beat.

wtf:  -51 + 87 = 36 / 329

OK, so this looks like a bug to me; number of rows is being handled
incorrectly for the degenerate boosted case.
behave_len is wrong for ensemble err.

We need: 
 non-degenerate, non weighted:
      best score = 0 so  err = score / num rows; 

 non-degenerate, weighted:
      best score = 0 so  err = score / weighted num rows; 
      since the score is weighted.

      e.g. two rows with weights: 
           0.1
           2.3
      so if 0.1 wrong, then err = 0.1/2.4
      and if 2.3 is wrong, err = 2.3/2.4  

 degenerate, non-weighted:

      best score > 0   err = (score - best_score) / eff_num_rows;

      where eff_num_rows = sum_row fabs(up-count - down-count)
      (its also the "worst possible score")

      e.g. five uncompressed rows:
           up:1  input-a  
           dn:2  input-a
           up:2  input-b
      best score is 1 (i.e. is 4-5 where 4 = 2+2).
      so if first is wrong, then err = (1-1)/5 = 0/3
      so if second is wrong, then err = (2-1)/5 = 1/3
      so if third & first is wrong, then err = (3-1)/3 = 2/3
      so if third & second is wrong, then err = (4-1)/3 = 3/3
      
However, above is wrong because -very-best needs to be rescaled too..

===========================

Precision scoring
-----------------
Results for the Hpre scorer, using the same datasets as above.

Results depend sharply on the complexity ratio and the temperature,
a quick sniff test shows: -z150 for the ratio and -v25 for the
temperature seem to work very well for preselect-no-dupe.csv and

-z150 -v25 for preselect-no-dupe.csv
-z350 -v45 for preselect.csv

   moses -Hpre -q0.05 -Yval -ipreselect.csv

Max attainable precision, with perfect recall, on preselect.csv should
be: 
   (329-50)/329 = 0.848024
That's because 50 rows are dupes, and selecting those will give fp's.
However, if we allow a smaller recall, and never recall the dupe rows,
then a best-possible precision of 1.0 is acheivable, if the allowed
recall is small enough.
  

     num_evals  preselect       no-dupe       
                      (time)       (time)  
     --------- --------------   ---------
        10K    0.78476 (0m14s)  1.0 (0m2s)
        40K    0.78947 (3m31s)

Above: the no-dupe column uses -v25 -z150
the plain  preselect column uses -v45 -z350

Results might improve with additional tuning; but this requires
averaging over multiple random seeds.

-------------------------
General strategy:

current scorer:
  if row picked, sum the outputs (= 0.5 * (pos - neg))
  active += pos+neg (weighted correctly)

  row score = sum of outputs / total weight of selected.

best-possible-score:
   tree picks only rows that are true positives, never picks
   rows that are true negatives.

   Ergo, this should be 1/total active weight


ensemble addition:
   If a selection has perfect score, then add it to the ensebmle
   with ??? weight.

   +1 for every correct entry
   less for partially correct...

   If its perfect, and activation is high enough, then we are done
   (the single solution is sufficient)

   If its perfect, but activation too low, then add with ??? weight.

   If its not perfect, means some selections are wrong.
   two alternatives:
    a) just don't add it.
    b) add it, but ... ?

ensemble scoring: 
   Members in ensemble select rows.  Alternatives:
    a) if ensemble selects, then done, because selection is never
       wrong, because we never added wrong selectors.
       (ensemble members are T to select or F for don't-care) 

    b) ensemble is sometimes wrong, selecting bad rows.  How
       can this be corrected?  How do I un-select? 
       b1) alter weights so that bad rows are almost never
           selected again by new ensemble members.
       b2) add a dual un-select ensemble, which un-selects
           bad rows.

yet another alternative;
    c) if a row is correctly selected, decrease its weight.
       if a row is incorrectly selected, don't admit to ensemble.
       if a row is not selected, but should have been, increase
       its desirablilty.

row weights: these need to be normalized to sum to 1.0

row weight adjust:
  w[i] *= rcpalpha if selected & right
       *= expalpha if selected & wrong
       *= 1.0 if not selected.

Strategy A:
-----------
-- Accept a tree into ensemble only if it has perfect score. 
   The weight it is given does not matter.
-- Decrease the weight of the rows it selects.  Increase the
   weights of all the other rows.  The idea is to have it focus on
   those rows it hasn't learned.  These weights will affect the
   optimization (hill-climbing) stage.
-- The returned get_weighted_tree() also needs modification.

Strategy B:
-----------
-- Maintain two ensembles: one that selects, and one that
   anti-selects.  However, these require two distinct metapops,
   one metapop to train each.
-- This allows imperfect selectors to be admitted into the ensemble,
   as long as a counter-manding selector is found for the badly-choosen
   rows.
-- The above seems too complex, for now.

Strategy C:
-----------
-- Allow imperfect selectors into the ensemble.  The ensemble then
   selects only when a row gets more than a minimum vote.  The actual
   minimum is maintained as a running bias in the ensemble: it is
   a hair larger than the strongest vote for an incorrect row.  That
   is, if bad rows are being selected with a strength s; then the bias
   is the largest such s, and the ensemble should reject all rows with
   a vote of less than s. 


Stragetgy A Boosted pre-scorer results
--------------------------------------
Strategy A has been implemented. Since the ensemble always has a
precision of 1.0, the actual score is always 1.0 - activation_penalty.

Columns are:
 no-dupe: -i preselect-no-dupe.csv -Hpre -Yval -q0.05 -z150 -v25 --boost=1
 presel: -i preselect.csv -Hpre -Yval -q0.05 -z350 -v45 --boost=1
 
     num_evals  no-dupe         presel
                       (time)           (time)  
     --------- --------------   --------------
        10K    0.50226 (0m21s)  0.27912 (0m16s)
        20K    0.50226 (1m25s)  0.80556 (0m59s)
        40K    0.68458 (4m36s)  0.80488 (3m48s)
        80K    1.0     (6m8s)   0.80000 (14m15s) 

XXX except above numbers include a bug in the code: the scores sent in
for training were mis-computed; basically, weights were being
double-applied.  Anyway, they kind of suck. So this bug gets fixed
mid-Sept 2014 and gives the results below...

     num_evals  no-dupe                 presel
                       (time) recall            (time)  recall
     --------- ----------------------   ----------------------
        10K    1.0     (0m5s) 0.19192   0.72449 (0m12s) 0.71717
        20K                             0.67133 (0m33s) 0.96970 
        40K                             0.67133 (1m28s) 0.96970
        80K                             0.67133 (3m35s)


Wow! These results are excellent!

The scores go down!  Why? Oh, I see .. the recall actually goes up.
The recall is obtained by running eval-candidate 9see below). Strange
side effect, that the recall goes up ... but precision goes down.
Is this a bug ??  Yes it is ... it needs fixing. See below.

---------
But first -- Are the reported trees correct?  Verify:

   eval-candidate -i preselect.csv -uoutput -C pre.combo -Hprerec

This gives 0.72449 . Yep. so that works.  Checked the 20K tree too.

The no-dupe result gives 1.0 for the pre scorer with -q0.05  whoa ...
So that all checks out.
---------

XXX there is still another bug lurking. For compressed tables,
with both pos and neg compressed together, with pos dominating
over the neg, these compressed rows were allowed into the ensemble
(because they were mostly pos).  This bug incorrectly allowed
imprecise scorers (as they pick the mixed rows) into the ensemble.
This is why the score goes down, even as the recall stoots way up.

Some debug and verification:
Code currently claims:
Precision scorer, best score = 0.941176
activation at best score = 0.051672
precision of 'true' exemplar is 0.300912 = 99 / 329
(all 329 rows selected, of which 99 are true.)
This leaves 329-99 = 230 false rows.

sum = -65.5 = 0.5 (tp - fp) of the selected rows, all rows selected.
= 0.5 *(99 - 230) = -131 / 2 = sum_outputs(orow)

Note that sum/active = -65.5 / 329 = - 0.19908 = 0.300912 - 0.5 = prec - 0.5
just as expected....

-----------
OK, as mentioned above, there is another bug in the boosted precision scorer.
Need to fix this.

The plan: for training, need to use the weighted bscores. Due to
normalization, the weighted bscore is NOT a simple dot-product of 
weights times bscore.  For acceptance into the ensemble, a flat bscore
needs to be used.  There is no easy way of getting the flat score from
the weighted score, and v.v. (Well, one alternative is to extend the
bscore ... that would require upcasting to use it...) So, for now,
just recalculate the bscore as needed.

plan:
1) add_expert() needs to obtain/compute flat bscore.
   add only those that are perfect, and not anticipated.
2) get_error() needs to take combo trees, no bscores.
3) in precision_bscore, can just call operator() for trees.

Now fixed, I think. circa 21 Sept 2014 or so.
-------
So, remeasure:

Columns are:
no-dupe: -i preselect-no-dupe.csv -Hpre -Yval -q0.05 -z150 -v25 --boost=1
presel: -i preselect.csv -Hpre -Yval -q0.05 -z350 -v45 --boost=1


 num      no-dupe                           presel
 evals     (time)  recall  activation           (time)  recall  activation
 ----- ----------------------------   ----------------------------------
 10K   1.0 (0m11s) 0.19192 0.051672   -0.41403 (0m26s) 0.04040  0.012158
 20K                                  -0.19089 (1m20s) 0.05051  0.015198
 40K                                   0.50226 (5m23s) 0.10101  0.030395


Wow. That sucks. Review of log syas..WOW... remove the activation penalty
during training!!! ... as we expect that most perfect scorers will have a
huge penalty  ... the varying weights should prevent on type taking over,
and should help w/ diversity.
-------

OK, so above now implemented. Try again.

 num      no-dupe                           presel
 evals     (time)  recall activa  cpxy          (time)  recall  activ  cpxy
 ----- -------------------------------  -----------------------------------
 10K   1.0 (0m17s) 0.1717 0.05167 103   -0.1909 (0m31s) 0.0505 0.0152   77
 20K                                    -0.0086 (1m55s) 0.0606 0.0182  176
 40K                                     0.1456 (7m24s) 0.0707 0.0213  416

Still not very impressive...

two more wtfs:

1) are the expalpha reversed ???
2) push down weights forever, not just per-shot.

Explore 1) -- reverse things.
All three columns are 'presel' as above.

reverse means: weights[i] = (0.0 < bs[i]) ? expalpha : rcpalpha;
       in metapopulation/ensemble.cc -- the weighting is reversed, which should
       be very wrong.

promo means flag:  --boost-promote=9999 tacked on, to promote all
       perfect scorers found.

 num      reverse        reverse-promo     promo
 evals         (time)           (time)          (time) recall  activ  cpxy
 ----- ---------------- --------------   ---------------------------------
 10K   -0.4140 (0m6s)   -0.1909 (0m8s)   0.2791 (0m34s) 0.0808 0.0243  240
 20K   -0.1909 (0m16s)  -0.1909 (0m22s)  0.2791 (1m56s) 0.0808 0.0243  400
 40K   -0.1909 (0m42s)  -0.1909 (1m8s)   0.2791 (6m55s) 0.0808 0.0243  900

Reverse certainly blows through the scoring function evaluations much much
quicker.  The results stink, as they should.

promo starts with a bang then fades. Probably due to issue number 2) above.
So lets fix that issue.
---------
OK, so patch ensemble to keep scores low on previously selected rows.
Now we get:

 num     promo
 evals        (time) recall  activ  cpxy
 ----- ---------------------------------
 10K   0.5976 (0m24s) 0.1111 0.0334 1306
 20K   0.6846 (1m22s) 0.1212 0.0365 1401
 40K   0.6846 (5m1s)  0.1212 0.0365 1817

Much better. But it stil stalls out.  Lets try a permanent-knockout variation.

Same as above, but flatter weight accumulation.

 num     promo
 evals        (time) recall  activ  cpxy
 ----- ---------------------------------
 10K   0.5976 (0m24s)
 20K   0.6846 (1m45s)
 40K   0.6846 (5m1s)

No discernable change ... I suppose thats good, right?

Below is how the above varies, for different random seeds.
The variation is pretty dramatic ...

 seed  promo
 -r   score
 ----------   
 0   0.5023 
 1   0.5976
 2   0.1456
 3   0.1456
 4   0.3969
 5  -0.1909
 6   0.2791
 7  -0.0086
 8   0.5023
 9  -0.1909


================================
An activation of 0.05 seems far too easy.  Try again with -q0.25:

Immediatly below are the late-August 2014 results, with the buggy code.

  nevals  no-dupe                           presel
                  (time)  recall  activa            (time) recall
  ------ -------- ------- ------- -------   -------------- ------
    10K   0.27912 (0m12s) 0.40404 0.12158   0.73034 (0m5s) 0.65657
    20K   0.50226 (0m42s) 0.50505 0.15198   0.73034 (0m5s)
    40K   0.63329 (2m57s) 0.57576 0.17325   0.73034 (0m5s)
    80K   0.85292 (9m2s)  0.71717 0.21580 

When validating: for the 10K result, the prerec scorer claims:
precision = 1.000000  recall = 0.404040  recall penalty = 0
But the pre scorer claims:
precision = 1.000000  activation=0.121581  activation penalty=-7.208840e-01

The matrix of results is:
   tp = 40  fp = 0    tp+fp = 40 
   fn = 59  tn = 230  fn+tn = 289 
   tp+fn = 99  fp+tn = 230  total = 329

Activation = tp/total =  0.121580547112462

Why is it terminating?  Because its getting perfect score on ensemble
... !? even though min activation not yet hit ??
... that's due to another bug, now fixed (mid-Sept 2014)

wtf in the logfile:  Expert: candidate score=-1.90976 error=0
 Best score: [score=153.4647216796875

so the normalization is bad ... 

-------

Comparable non-boosted results, for -q0.05:

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- --------------    --------------
         1K    0.61643 (0m1s)    0.72727 (0m1s)
         2K    0.80952 (0m2s)    0.72727 (0m2s)
         5K    1.0 (0m2s)        0.76471 (0m5s)
        10K     -                0.78476 (0m14s)
        40K     -                0.78947 (3m35s)
        80K     -                0.82143 (14m0s)

Comparable non-boosted results, for -q0.25:

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- --------------    --------------
         5K    0.68421 (0m6s)    
        10K    0.68421 (0m21s)   0.65549 (0m16s)
        20K    0.72043 (1m11s)   0.70588 (1m4s)
        40K    0.72043 (4m9s)    0.70588 (4m32s)
        80K    0.72414 (15m6s)   0.72043 (18m19s)

OK, the non-boosted results are better or sometimes a tie, this is
because the hill-climbing stage was not making use of the penalties.
This was a mis-design in the score-mangling stages.  Yuck, scoring is
just complicated.  So redo:  below are boosted versions, with the
fixed ranking during hill-climbing:

(This and the rest are late-August 2014 results)

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- ---------------   --------------
        10K    0.59757 (0m17s)   0.76190 (0m21s)
        20K    0.83873 (1m2s)    0.77272 (1m21s)
        40K    1.0     (2m53s)   0.78788 (4m44s)
        80K     -                0.80000 (17m1s)

And again with -q0.25:

     num_evals  no-dupe            presel
                        (time)            (time)  
     --------- ----------------   --------------
        10K    -2.02347 (0m19s)   0.63925 (0m21s)
        20K    -1.80032 (1m2s)    0.65588 (1m15s)
        40K    -1.46385 (3m48)    0.68839 (4m34s)
        80K    -0.92485 (13m36s)  0.71084 (16m5s)

Take a third shot, with a different weighting scheme.  The above
used an expalpha of 1.2, while the below uses an expalpha of 2.0.
The larger exalpha should encourage the discovery of more diverse
experts.

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- ---------------   --------------
        10K    0.14558 (0m15s)   0.69587 (0m19s)
        20K    0.90772 (0m44s)   0.69587 (1m9s)
        40K    1.0     (1m3s)    0.78378 (4m15s)
        80K     -                0.79070 (16m26s)

And again, with -q0.25:

     num_evals  no-dupe          presel
                        (time)            (time)  
     --------- ----------------   --------------
        10K    -1.33032 (0m18s)   0.50092 (0m28s)
        20K    -1.21254 (1m0s)    0.52000 (1m30s)
        40K    -1.01187 (3m33s)   0.66387 (5m1s)
        80K    -0.84481 (13m15s)  0.66667 (19m42s)

A third possibility: try adding multiple trees per run ... below results
are boosted, and add 3 trees per knob-expansion cycle.

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- ---------------   --------------
        10K    0.27912 (0m21s)   0.80000 (0m17s)
        20K    0.83873 (1m5s)    0.80000 (1m2s)
        40K    1.0     (2m48s)   0.80435 (3m50s)
        80K                      0.68539 (15m0s)

Wow. The score went down for the last one ... strange.

Try adding the top 7:  (--boost-promote=7)

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- ---------------   --------------
        10K    0.39690 (0m16s)   0.78049 (0m17s)
        20K    0.83873 (1m4s)    0.78049 (1m4s)
        40K    1.0     (3m11s)   0.77049 (3m54s)
        80K                      0.68421 (15m19s)

Hmm .. score dropping in the left column. How odd ... 

The precision scorer just strikes me as being awfully underwhelming ... 


Stragetgy C Boosted pre-scorer results
--------------------------------------
The strategy A results are underwhelming.  Try again with strategy C.

no-dupe: moses -ipreselect-no-dupe.csv -Hpre -Yval -q0.05 -z150 -v25 
         --boost=1 --boost-exact=0

presel: moses -ipreselect.csv -Hpre -Yval -q0.05 -z350 -v45 
         --boost=1 --boost-exact=0

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- ---------------   --------------
        10    -1.10718 (0m17s)   0.00273 (0m18s)
        20K   -0.70171 (1m10s)  -1.80032 (1m7s)
        40K   -0.70171 (4m26s)  -inf     (4m6s)
        80K   -0.70171 (15m32s)

What a disaster.  The harder we look, the worse the results.
Presumably, what is happening is that, as the bias increases more and
more to prevent bad rows from entering the selection, that the number
of good rows is decreasing as well, thus raising the penalty and
lowering the score.

Experiment: scale the bias.  Same as above, but with --boost-bias=0.5

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- ---------------   --------------
        10K    0.68000 (0m17s)   0.62025 (0m18s)
        20K    0.60976 (1m10s)   0.65101 (1m6s)
        40K    0.56757 (4m27s)   0.62821 (4m10s)
        80K    0.71111 (15m40s)  0.63514 (14m42s)

Much better. Not great, but better.  Presumably, the scaled bias now
allows some imprecise selection to happen .. but overall, its an
improvement.

Future directions
-----------------
* Explore the "local expert" idea above.
* Try to understand the underlying claim about getting good results
  from the precision scorer.

=================================================================
Sept 2014
---------
There were multiple bugs/design issues affecting the precision scorer.
Re-run above experiments.

no-mo: moses -ipreselect-no-dupe.csv -Hpre -Yval -q0.05 -z150 -v25 
         --boost=1 --boost-promote=9999

promo: moses -ipreselect.csv -Hpre -Yval -q0.05 -z350 -v45 
         --boost=1 --boost-promote=9999

 num     promo
 evals        (time) recall  activ  cpxy
 ----- ---------------------------------
 10K   0.5976 (0m24s) 0.1111 0.0334 1306
 20K   0.6846 (1m22s) 0.1212 0.0365 1401
 40K   0.6846 (5m1s)  0.1212 0.0365 1817

Above are validated with
   eval-candidate -i preselect.csv -uoutput -C pre.combo -Hpre -q0.05

Again, with -q0.25

 num     promo -q0.25
 evals         (time)  recall  activ  cpxy
 ----- -----------------------------------
 10K  -1.2125  (0m24s) 0.0909 0.0274   345
 20K  -1.1072  (1m27s) 0.1010 0.0304   680
 40K  -1.1072  (5m29s) 0.1010 0.0304  1739
 80K  -0.9249 (23m42s) 0.1212 0.0365 16016

OK, that's pretty grim...

 num     no-mo
 evals        (time) recall  activ  cpxy
 ----- ---------------------------------
  10K   1.0  (0m16s) 0.1717 0.0517  673

OK, so that works well, as alwasy. Of course its far less challenging.

 num     no-mo -q0.25  (no-dupe, with boosting and full promotion)
 evals         (time)  recall  activ  cpxy
 ----- -----------------------------------
  10K  -0.4653 (0m23s) 0.1919 0.0578  774
  20K   0.0242 (1m23s) 0.3131 0.0942 1163
  40K   0.1456 (5m19s) 0.3535 0.1064 2305
  80K   0.6156 (20m0s) 0.5657 0.1702 5069
 160K   0.7949 (57m7s) 0.6768 0.2036 7569

-------
Lets compare these to the non-boosted results, from up-top.
Sadly, the non-boosted results prety much always win,
speed-wise and score-wise.

Comparable non-boosted results, for -q0.05:

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- --------------    --------------
         1K    0.61643 (0m1s)    0.72727 (0m1s)
         2K    0.80952 (0m2s)    0.72727 (0m2s)
         5K    1.0 (0m2s)        0.76471 (0m5s)
        10K     -                0.78476 (0m14s)
        40K     -                0.78947 (3m35s)
        80K     -                0.82143 (14m0s)

Comparable non-boosted results, for -q0.25:

     num_evals  no-dupe          presel
                       (time)            (time)  
     --------- --------------    --------------
         5K    0.68421 (0m6s)    
        10K    0.68421 (0m21s)   0.65549 (0m16s)
        20K    0.72043 (1m11s)   0.70588 (1m4s)
        40K    0.72043 (4m9s)    0.70588 (4m32s)
        80K    0.72414 (15m6s)   0.72043 (18m19s)

How much of this is due to a fateful random number choice?
All evals are for 10K and -q0.05. All promo times are about 20s to 30s.

   seed  promo   presel
    -r   score   score    time  precision
    ----------   ------------------------
    0   0.5023   0.76000  26s
    1   0.5976   0.78476  16s
    2   0.1456   0.08769  29s
    3   0.1456   0.78261  18s
    4   0.3969   0.73684  34s
    5  -0.1909   0.88888  29s 0.88880
    6   0.2791   0.76190  16s
    7  -0.0086   0.65000  24s
    8   0.5023   0.78947  19s
    9  -0.1909   0.73684  27s

OK, so boosting is beaten every time, it seems.  But here's one notable
thing: the non-boosted ones don't have perfect precision, but they do
mostly seem to always meet thier activation goals.  By contrast, the 
boosted ones always have perfect precision, but fail to meet thier
activation goals.  So this is kind-ocf comparing apples to oranges.

The penalty for missing activation is severe -- but there is no penalty
for imperfect precision. How would the non-boosted scorer fare, if there
was a penalty for poor precision?

