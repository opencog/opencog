
Diary of performance measurements and tuning for MOSES
------------------------------------------------------
    Linas Vepstas -- January 2012 - August 2013

Maintained by Linas Vepstas.  Started January 2012.  A summary
of lessons learned is immediately below.   The rest is a diary,
and is not meant for general reading;  it won't be explained for
the general public, either you'll get it or you won't.  This is 
for my personal use, to take notes.  If you are a moses hacker,
maybe this data will be interesting.  I'm only checking this into
bzr so that I can go back & double-check things & have a record of
progress.

Timing measurements are very specific for my current setup.   Your
results will be different.  Results will change as algorithms are
modified; the below is being used to help me get baseline performance
numbers, to understand how modifications affect performance, and to 
help make sure there are no regressions.

Most of these were done on an AMD Athlon 64 X2 dual core (usually
running on a single core only), clocked at 3GHz. This has a 1MB cache,
and 8GBytes unbuffered ECC DDR2-266(???) RAM.

=======================================================================
Lessons learned

There are many lessons learned from the results below.  Most of these
have been incorporated into the code base.  Here's a summary of some of
these.

++ True hillcmbing vs. expanded neighborhood search. The original
   "hillclimbing" code, circa bzr revision 6500, actually implemented a
   kind of neighborhood search algorithm. That is, it searched a portion
   of the nearest neighbors of an instance. If it found an improvement,
   it returned to the metapop outer loop.  If it did not find
   improvement, it expanded the search to 2nd-nearest neighbors, and so
   on. The search was rarely exhaustive; the neighborhoods were 
   sub-sampled, as the size of the neighborhoods grows combinatorially.

   By bzr rev 6522, the algo was refactored to implement "true
   hillclimbing".  A portion of the nearest neighbors are searched. If
   improvement is found, the best instance is defined as the new
   center, and it's nearest neighbors are searched. This process is
   repeated until there is no improvement, and only then does control
   return to the metapop outer loop.  The "true hillclimbing" algo
   provides a 2.3x performance improvement over the old algo, as
   documented below.  The behaviour of the old algo can be regained by
   using the -L1 -T1 -I0 flags.

   The so-called "simulated annealing" code still implements the old,
   local neighborhood search algo, except that it uses a temerature to
   occasionally widen the search to farther distances.  Thus, it should
   be renamed to "star-shaped search", as the searched neighborhood is
   star-shaped; the fatness of the star depending on a temperature.


++ The "high score" trap. The runtime performance of the code circa
   bzr rev 6522-6560 had a very strong dependence on the initial random
   seed. For many seeds, runtime was worse-than-exponentially slower as
   compared to the fastest cases.  This appears to be due to the fact
   that sometimes the top-scored exemplar in the metapopulation does not
   make a very good tree on which to build knobs.  In essence, the outer
   loop, the metapopulation loop, was a strict hill-climber; it worked
   only with the top scorers.  The solution to this was to convert the
   metapop select_exemplar() routine to use an annealing-style
   temperature.  Thus, some fraction of the time, a lower-scoring
   exemplar would be used as the starting place for knob-building.
   Additional performance is gained by weighting the complexity into
   the selection: that is, a lower-scored, but also lower complexity
   exemplar may be a better starting place than a high-score,
   high-complexity exemplar.  The discovery & solution of this behaviour
   is documented on the opencog blog; the blog page captured in the
   file documentation/Select_Exemplar.html in this directory. Misc data
   and graphing tools can be found in diary/complexity-temperature.
   In the diary below, this starts at the section titled:
   ++++++ The "high score" trap. AKA "select exemplar annealing" +++++
   The final, fixed form of this code can be found in bzr rev 6582;
   the -v and the -z flags are used to set the complexity weight and
   temperature. The resulting performance improvement is about 200x 
   for 4-parity (runtime, 20 different random seeds, before: about
   55K seconds = 15 hours, after: about 300 seconds)  This is a huge
   win. 

   BTW, its not just the temperature that does this, but also a better
   control over the metapopulation size.  See below.

   Occam's Razor:  The old code did have a so-called "Occam's Razor"
   feature, which was effectively quite similar to the above, except
   that 1) it was undocumented so I didn't get it, and 2) it had a 
   hard-coded temperature that was inapropriate and 3) it had a
   profusion of minus signs that confused me.  Eventually realizing
   that the complexity temperature and penalty were the same concept,
   the two were merged, and code cleaned up by bzr rev 6971, with
   final cleanups by bzr rev 6990.

   The original Occam temperature was not recognized as a temperature,
   but instead appealled to a concept of Solomonoff prior distribution
   probbility.  The distribution was 2^-complexity, with complexity
   meaured in bits.  By contrast, the current code uses a distribution
   of exp(-beta * complexity).  Solomonoff would be regained if
   beta==log2==0.69 but instead, current experimental evidence suggests
   that a beta==2.4 works quite well.   Note tha the equivalent
   temperature is 3 or 4 times lower than what Solomonoff suggests.
   But perhaps we could/should raise the temperature a tad, and the
   complexity penalty a tad... still experimentally unclear.  The
   current beta is beta=100/(12*3.5)=2.4  with 100==hard-coded,
   12==used with the "magic" dataset, with -v12 flag and 3.5==the -z
   option default.   Note also, the man page gives probablistic 
   argument for the correct prior value of complexity coeff.  It is
   also possible that the moses code is significantly under-estimating
   the true complexity; if so, the Solomonoff prior would be closer
   to the experimentally optimal temperature.  But it is hard to say,
   at this point.
   

++ Better population control; skip behavioral score computation.
   Older code kept an excessively bloated metapopulation, members of
   which were astronomically unikely to be choosen as future
   exemplars.  This gets fixed. In addition, old code was computing
   behavioral scores, even though domination-merging is no longer
   used. (Domination merging is a pig). Skipping this provides a huge
   performance saving for complex exemplars, such as the bank.csv
   dataset, but only minor effects (about 7%) for 4-parity.  This
   is all checked in by bzr rev 6617.  Note that the combined effects
   of the last several changes are about 3x to 4x, changing times for
   4-parity from about 300 sconds in rev 6582 to 72 secons in rev 6617.

   The metapopulation size is managed by refering to the score of
   exemplars; this obviates the need for the "dominated merge" logic.
   The dominated merge logic is very complicated and quite slow;
   skipping this accounts for a large performance boost.  In addition,
   domination had a way of knocking out exemplars with decent scores,
   needed for the annealing choice.

   That is, by using a temperature during selection, we mean that the
   exemplars are choosen with a Boltzmann distribution, with exemplar x
   being choosen with probability p(x) ~ exp(-E(x)/kT) where E(x) is the
   score of exemplar x. Because of the rapid fall-off of the
   exponential,  exemplars with scores that are worse than 20 ro 30
   times kT are very very unlikely to be chosen (since exp(-20) or
   exp(-30) is so very tiny).   Thus, it is quite reasonable to cull
   exemplars with scores worse than this from the metapopulation.  This
   will dramatically decrease the size of the metapopulation to a more
   manageable level.  In particular, it will discard many non-dominated
   exemplars, while also keeping many dominated exmplars.  The net
   result is that the domination computations, slow and cpu-intensive
   to begin with, are proven to be ineffective, and are rendered obsolete.


++ Exhaustive neighborhood search. The moses algo needs to apportion
   cpu cycles between the outer, metapop loop, and the inner, deme
   exploration loop.  The old algo, circa 6582, did this via a fraction
   of the remaining cycles. If the max allowed cycles (-m flag) was too
   low, the nearest neighbor exploration would be subsampled (instead of
   an exhaustive search). Altering the nearest neighbor search (when the
   hamming distance really is just one) to be an exhaustive search seems
   to improve things.  Note this affects just the bank.csv dataset, as
   this was not an issue for 4-parity. Part of the problem with the bank
   dataset is that it jumps from relatively simple exemplars in the
   first round, to very complex (squared) exemplars in later
   knob-building rounds; these later exemplars are very expensive to
   evaluate, so it makes sense to exhaust the simple cases first.
   Fixed by bzr rev 6590.  See "budge" column in "regression" section
   below.

   However, an exhaustive neighborhood search is not always needed;
   in fact, for some problems, a non-exhaustive search can provide a big
   win.  This occurs for problems with a large feature set, such as 30 or
   more.  As an example, the "magic-16-sect" dataset, described below,
   has 150 features.  In the first generation, the field set contained
   about 2K bits.  That is, the initial exemplar had enough knobs that
   the size of the nearest neighborhood of a given exemplar was about
   2K.  Such a nieghborhood can be productively searched exhaustively.
   However, the second generation exemplar had a neighborhood size of
   15K, the third generation 61K and the fourth generation had 141K.
   Exploring these exhaustively seems pointless: even if only 0.1% of
   the neighborhood had scores better than the starting point, an
   exhaustive search would produce 15, 61 or 141 improvements.  For
   hill-climbing, only 1 better score is needed; thus, exhaustive
   search was oversupplying candidates, at a rather expensive cost.
   Furthermore, by employing cross-over (see below), even more viable
   instances are generated.   Thus, performance can be significantly
   improved by sub-sampling the neighborhood.  Dramatic improvements
   can be seen by searching as little as 1% of the neighborhood.  Use
   the --hc-fraction-of-nn and --hc-max-nn-evals options from bzr rev
   7099.  Its probably better to use --hc-max-nn-evals alone, so that
   small neighborhoods are exhaustively searched, while the search of
   large neighborhoods is clamps to a fixed size.


++ Correlated knob-turning ("Cross-over").  Explores the hypothesis
   that high-scoring instances are highly correlated.  This does indeed
   seem to be the case for the bank.csv dataset, and so the best new 
   instances are mergers of the previous high scorers.  This seems
   to offer a huge win, by offering much of the benefit of EDA/BOA
   with none of the cost. Research cmpleted, was checked in sometime
   after bzr 6617.

   Result: A genetic cross-over algorithm was implmenented.  That is,
   by taking two high-scoring instances, and crossing them to have a
   new instance sharing both their traits, one often gets a new instance
   that is fitter than either of the parents.  Cross-over is done by
   comparing each instance to its common ancestor, to see what knobs
   have changed, and then, for the child, taking the union of the 
   knob changes.   Since the hill-climber nearest-neighbor search always
   tweaks just one knob, each parent will have just a one-knob
   difference, and the child will thus be at Hamming distance two from
   the common ancestor.  Likewise, one can take the union of knobs from
   the highest-scoring 3 or 4 instances, to create children at distance
   3 or 4.  The current hillclimber will first attempt an exhaustive 
   search, and then create 40 cross-overs at distance 2, 40 more at
   distance 3 and 40 more at distance 4.  Very often, one or more of
   these 120 instance will have an improved score.

   In a certain sense, the above mimics the best aspects of an EDA/BOA
   algorithm, without the overheads.  That is, it assumes a correlation
   of knob settings, and assumes a perfect correlation for the top-40.
   It took no effort to make this assumption; yet it appears to usually
   be true, and the rewards of assuming it are quickly reaped.


++ Contin learning vs. discretization.  Currently, learning from
   continuous-valued datasets works.  However, it doesn't work very
   well; it is significantly out-performed by discretizing the dataset
   into many binary-valued features.  Using 4 or 6 booleans for each
   oringial contin feature seems to work best.  Improving this situation
   is TBD.
   
   For the above problem discretizing contin variables, it seems that
   feature selection is useless or even worse-than-useless.  Compare,
   e.g. 16-sect-nn-5K to 16-sect-C48-j12, below. Research is ongoing.
   



=======================================================================
Major Studies

Diary contains these major sections, below.

-- Study of parity problem perforrmance, Jan-Feb 2012
   (discrete logic problem, w 3/4/5 features, and 8/16/32 rows)

-- Study of bank dataset  Feb-March 2012
   (learning of continuous-valued function)
   (38 features, 90 rows)

-- Study of Iris dataset  May 2012
   (clustering/categorization problem, 4 continuous features, 150 rows.
   Want one of 3 output enums.)

-- Study of MAGIC (atmospheric gamma ray) dataset  May-June 2012
   (event identification/rejection problem, 19K rows, 11 contin-valued 
   features.  When discretized, 150 binary fetures.)

-- Study of Yeast dataset June-July 2012
   Yeast classification, 1.5K records, 8 features.
   Other systems get 55% accuracy. (per ascii description)
   Moses best: 78% accuracy


=======================================================================
Diary started in January 2012

+++++ True hillcmbing vs. expanded neighborhood search. +++++

Opencog revision 6386:
----------------------
-Hpa -k3 -m50000    (3-parity)
-r0: 0m15.051s
-r1: 0m1.406s
-r2: 0m15.265s
-r3: 0m3.281s
-r4: 0m17.628s
-r5: 0m3.876s
-r6: 0m5.590s
-r7: 0m22.319s
-r8: 0m19.817s
-r9: 0m3.958s


-Hpa -k4 -m50000
(no r flag):  0m52.547s (best is -1)
-r1: 0m51.907s  (also -1, as are other results unless otherwise indicated)
-r2: 1m26.161s
-r3: 1m1.008s  (best is -2)
-r4: 1m26.425s
-r5: 1m15.517s
-r6: 1m24.877s
-r7: 0m53.119s (best is -2)
-r8: 1m4.888s
-r9: 1m20.561s (best is -2)

To get the above in later versions, need to use -L1 -T1 flags.

circa revision 6541:
--------------------
time opencog/learning/moses/main/moses -Hpa -k3 -m50000
(no -r flag): 0m0.892s
-r0: 0m0.535s
-r1: 0m0.796s
-r2: 0m7.317s
-r3: 0m1.334s
-r4: 0m0.895s
-r5: 0m8.400s
-r6: 0m2.420s
-r7: 0m10.862s
-r8: 0m2.026s
-r9: 0m7.471s

avg: 4.19 (approx)

Again, Now with -L1 -T1:
-r0: 0m9.078s
-r1: 0m1.556s
-r2: 0m23.966s
-r3: 0m3.546s
-r4: 0m5.201s
-r5: 0m4.385s
-r6: 0m9.747s
-r7: 0m9.991s
-r8: 0m19.364s
-r9: 0m9.968s

avg: 9.68 (approx)

So the 'true hillclimbing' is 2.3x faster than old stuff, for this.

Now try 4-parity...
-Hpa -k4 -m50000
-r0: 158 secs, -1 score
-r1: 2m23.313s  (a -1 solution)
-r2: 2m30.541s  (also -1, as are rest, unless indicated).
-r3: 1m40.566s  (bravo perfect score!)
-r4: 1m39.910s (best is -2)
-r5: 2m4.012s
-r6: 2m39.442s
-r7: 0m42.020s (perfect score)
-r8: 1m42.506s (-2)
-r9: 2m9.784s (-2)

so recap: -Hpa -k4 -m50000
perfect score: 2
-1 score: 5
-2 score: 3

So -m50K is not enough.

try again -m150K
-Hpa -k4 -m150000
-r0: 278.223783 secs perfect score
-r1: -1
-r2: 180.539163 secs perfect score
-r3: 97.197070 secs perfect score
-r4: -2
-r5: 142.639202
-r6: -1
-r7: 42.500166
-r8: -1
-r9: -2

Note: perfect runtimes almost unchanged, and fewer loosers.

so recap: -Hpa -k4 -m150000
perfect score: 5
-1 score: 3
-2 score: 2

try again -m450K
-r0: 279.876706
-r1: -1
-r2: 185.149519
-r3: 97.226776
-r4: -1
-r5: 145.637475
-r6: -1
-r7: 43.974498
-r8: 709.072294
-r9: -1

so recap: -Hpa -k4 -m450000
perfect score: 6
-1 score: 4
-2 score: 0

try again -m1.6M
-r0: 286.623444  secs
-r1: -1
-r2: 196.127073
-r3: 100.873139
-r4: 1690.134350
-r5: 148.591779
-r6: 9752.876748
-r7: 51.243247
-r8: 731.033613
-r9: 2367.365400

Comments on above:
I've started trying to understand how hill-climbing actually works. Recall,
we now have two styles of hill-climbing, and each behaves differently.  The
"new style" does this:

-- construct representation by adding knobs
-- twiddle one knob at a time, till one knob causing greatest improvement is found.
-- repeat above step until no more improvement.
-- go back to step 1.

I ran above on 4-parity, with ten different random seeds (-r0 through -r9)
The time to solution (perfect score) depends *very strongly* on the random
seed. In one case, a solution is found in less than a minute; a few more
cases find a solution in 2-3-4 minutes anothr in 12 minutes, another in 25
minutes, another in 45 minutes, another in 3 hours. In one case, no perfect
score found after more than 3.5 hours search.


I've graphed these (see attachment) in order of discovery.  I am trying to
understand why this is happening, and what to do about it.  No clear ideas,
yet. 

I'm also trying to measure 4-parity using the "old-style" algo. Before I
describe this, one salient remark:  the above timings are independent of the
"max iterations" flag. That is, for "moses -Hpa -k4 -r2 -mNNN"  (4-parity,
rand-seed=2, max-iter=NNN)  then, if NNN is large enough, a perfect score is
obtained after 196 seconds, always, independent of NNN.  And if NNN is too
small, no solution is found.  I mention this, because it is very different
than the "old-style" behaviour.

=======================================

The "old-style" algorithm is this: (-L1 -T1)
-- step 1. construct representation by adding knobs
-- twiddle one knob at a time, till one knob causing greatest
   improvement is found.
-- if there's improvement, go to step 1.
-- if no improvement, twiddle two knobs simultaneously, and either go
   to step 1 or try 3 knobs.
-- try up to five knobs, then go to step 1.

Now, all over again but with -L1 -T1
-Hpa -k4 -L1 -T1 -m50000
-r0: -1
-r1: -3
-r2: 70.442446
-r3: -1
-r4: -2
-r5: -4
-r6: -1
-r7: -2
-r8: -2
-r9: -3

-Hpa -k4 -L1 -T1 -m150000
-r0: 80.953417
-r1: -1
-r2: -1
-r3: -2
-r4: -2
-r5: -2
-r6: -2
-r7: -1
-r8: -2
-r9: 138.244841

-Hpa -k4 -L1 -T1 -m450000
-r0: 419.329227
-r1: 476.257538
-r2: -1
-r3: -1
-r4: -1
-r5: -2
-r6: 585.746513
-r7: 963.175023
-r8: -2
-r9: 413.799492

-Hpa -k4 -L1 -T1 -m1600000
-r0: 413.346996
-r1: 490.218301
-r2: 2844.111736
-r3: 1144.400860
-r4: 1110.296915
-r5: 1973.547115
-r6: 1012.321808
-r7: fail OOM 5.5 gigs virt, 3.7g RSS

Here, the behaviour is very different.  If NNN is small, then only one of
ten random seeds result in a solution, in about a minute. The other 9 are
wayy off.  If NNN is larger, two solutions are found, in about 2 minutes
each, the other 8 are way off, but not as bad as before.  There is no
solution in 1 minute!. The -r seed value that had previously found a soln
now finds no soln.

This pattern continues: for larger NNN, five solutions found, out of ten.
The fastest one found takes 8 minutes; the slowest took 15 minutes.  That
is, raising the max-iteration bound destroys the ability to find solutions
quickly for some random seeds, while improving  number of seeds that do lead
to solutions.  Very strange.

=======================================
++++++ The "high score" trap. AKA "select exemplar annealing" +++++

Hypothesis: to answer question "why do some random seeds take so long w/
hill-climbing"?  Highest ranked exemplars, no matter how they are festooned
with knobs, have no knob settngs that improve the score. However, they have
many, many settings that provide an equivalent score, and none of these
dominate: thus the metapopulation keeps growing w/o bound. Thus, more and
more exemplars get explored, going down the list, until one gets deep into
the not-very-fit section of the metapopulation. Then, at last, an exemplar
is found that, when properly decorated, does have knob settings that result
in a solution.

Propose: instead of exploring metapopulation from highest score to lowest,
instead explore from lowest complexity to highest.  Problem: the correct
parity solution is going to have a high complexity...

Maybe weighted sum of complexity, score ...

plot:
deme generation, metapop size, score, complexity, evaluations.

cat moses.log | grep Stats | cut -d" " -f5

OK, so graphing the above states clearly indicates that the metapop grows
without bound.  up until point when an improved score is found, and then the
metapop is cut down to almost nothing at all.  Meanwhile, both score and
complexity are held constant for very long periods of time.  (err. best
score, and complexity of highest-ranked exemplar).

  -M [ --max-candidates ] arg (=-1)     Maximum number of considered candidates
                                        to be added to the metapopulation after
                                        optimizing deme.
                                        

graph: time to next discovery.
graph: eval time to num evals...
experiment: limit max metapop size.

Here's the old, uncapped, unlimited data:
with revised timings, due to code restructuring (stubbing out logger calls)
-r0: 410 gens 79883 evals   286 secs (262 secs revised)
-r1: -1
-r2: 196.127073 (revised 172 secs)
-r3: 100.873139 (revised 93 secs)
-r4: 1325 gens 486915 evals   1690 secs (revised 1560)  0.78 gens/sec  288 evals/sec
-r5: 148.591779 (revised 134)
-r6: 9752.876748
-r7: 130 gens 20432 evals  51 secs (revised 39 secs)  2.5 gens/sec   400 evals/sec
-r8: 510 gens 234820 evals 731.033613 (revised at 665 secs)
-r9: 2367.365400 (revised 2150 secs)


experiment: keep non-dominated exemplars only if they have a large hamming
distance between one-another.

Sucess! (kind of).... when limiting pool size to 1000 the -Hpa -k4 -r0 was solved in 98 secs
instead of 260 secs.  However, smaller limits did not work...
 Does sorting time make a big diff ?

-r0-capped: 217 gens, 38338 evals 98 secs
-r4-capped: 872 gens, 262530 evals 782 secs
-r7 capped: 507 gens, 97009 evals 310 secs   XXXX so this gets worse!!
-r8 capped: 788 gens, 265462 evals 698 secs   XXX this got worse ...

do r8, then r9 then r6

without the cap, get a population of over 6K towards the end.
-- propose: remove anything that's been tried already.
   tried that quickly, didn't seem to go well.  I don't like this idea...

idea:
-- limit the pool size to function of complexity:
   e.g. 20 bit complexity requires 2^20 = 10^6 for exhaustive enumeration.
   limit pool to 2^(complex/2) or 2^(2*complex/3) ... 

------
idea: select_exemplar() has a probability of selection that is SA-like.
try hotter temperatures. Origianlly have T=1

p = (p > 0 ? 0 : pow2((p - highest_comp)/T));

-1 means timoeut at 450K

        T=1     T=2    T=3
-r0:     262       8      4
-r1:  >19261      14    514   
-r2:     172    5371    444
-r3:      93    1168   5520
-r4:    1560    1131     58
-r5:     134     220   2106
-r6:    9752.x   502     53
-r7:      39      43   2566 
-r8:     665     324   6867
-r9:    2150     628    143        
-r10:     30     522    158
-r11:     15      67     77
-r12:    360    1656    700
-r13:    292     568    140
-r14:    961      20    931
-r15:   1972      83    306
-r16:     34     916   1547
-r17:   7138     187     90
-r18: >31836    2991     22  (-m1.1M)
-r19:    172     549    808

450K == 1/2 hour

sorting first ten by runtime:

# first column: original data
# second column: a 10% performance speedbump due to logging.
# third column: broaden the complexity chooser (by 2)
#
1  51.243247   39 8
2  100.873139  93 14
3  148.591779  134   43
4  196.127073  172   220
5  286.623444  262   324
6  731.033613  665   502
7  1690.134350 1560  628
8  2367.365400 2150  1134
9  9752.876748 9752  1168

sorting the first 20 by rutime:

#
# moses performance data, runtime in seconds
# using default hillclimbing.
# -Hpa -k4 in order of runtime
#
# first column: original data
# second column: a 10% performance speedbump due to logging.
# third column: broaden the complexity chooser (by 2)
# fourth column: broaden the complexity chooser (by 3)
#
# in select_exemplar():
# p = (p > 0 ? 0 : pow2((p - highest_comp)/T));
#
#   orig       T=1    T=2    T=3
1   15          15      8      4
2   30          30     14     22
3   34          34     20     53
4   51.24       39     43     58
5   100.87      93     67     77
6   148.59     134     83     90
7   172        172    187    140
8   196.12     172    220    143
9   286.62     262    324    158
10  292        292    502    306
11  360        360    522    444
12  731.03     665    549    514
13  961        961    568    700
14  1690.13   1560    628    808
15  1972      1972    916    931
16  2367.36   2150   1131   1547
17  7138      7138   1168   2106
18  9752.87   9752   1656   2566
# 19 >19261 at -m1 for T=1
19  9999     19261   2991   5520
# 20  >31836 at -m1.1M for T=1
20  9999      31836  5371   6867


Email 6 Feb 2012:
As mentioned before, the run-time performance of MOSES in finding a
perfect solution to certain problems can sometimes take an exponentially
long time, dependeing on the initial random seed.  I sent a graph
before, and attached is a new one. As before, I've been focusing on the 
4-parity problem.

For those runs that take huge amounts of time to solve, what appears to
be happening is that the algo quickly finds a fairly good solution,
matching all but one of the posible outputs, and then gets hung up. It
examines huge numbers of exemplars, attaching knobs to each, twiddling
these, finding plenty of new, non-dominated solutions. But it just can't
find a fitter solution.  So, it occurred to me, perhaps it is failing to
look at a diverse-enough set of initial exemplars.  I assumed that the
algo was selecting the fittest possible exemplar, with the lowest
possible complexity, and trying to mutate that. I figured that,
perhaps, it would work faster if it occasionally attempted mutating
some high-complexity exemplar.  (This was directly inspired by the other
chain of emails).

This is done in select_exemplar(). Turns out, the code was already doing
the above. That is, even the so-called "hill-climbing" code doesn't
hill-climb when picking an exemplar; instead it implements a 
simulated-annealing-style algo. It didn't always pick the
lowest-complexity exemplar to mutate, it sometimes did pick something
more complex.  The problem is that the temperature was set too low.
It didn't try the high-complexity exemplars often enough.  So I
twiddled, and got the graph below.

The label "pow2(-complexity/T)" refers to the line of code in
select_exemplar().  I tried T=1,2,3. (T is the tmerpature; this is the
Bolzmann distribution).  The original code had a hard-coded T=1 in it.

-- Runtime improves by a factor of 2x or better for T=2, usually.
   (The green line) Sort-of. Almost 10x better for the high-runtime
   cases, but worse on the medium runtime cases.
-- The exponential behavriour does not go away.  The overall slope
   is unchanged.  The slope is bad: The fastest case takes 4 seconds.
   Half the time, a perfect solution is found in under 300 seconds.
   Half the time, a perfect solution requires much much much more than
   that: sometimes many many hours.

I'd like to try other temperatures, and a larger dataset, but this
all requires a lot of cpu time.  Working on it ... 

I've checked in code for this, but its not in final form yet.

---------------------------------------------------

As above, but for T=2 in select_exemplar() temperature, comparing
capped and uncapped metapop size.

       uncapped   capped
-r0:        8        8
-r1:       14       14
-r2:     5371    >5957
-r3:     1168      358
-r4:     1131      680
-r5:      220     1082
-r6:      502      371
-r7:       43       41
-r8:      324       59
-r9:      628       67
-r10:     522       35
-r11:      67      161
-r12:    1656      548
-r13:     568    >5553
-r14:      20      104
-r15:      83     1299
-r16:     916       94
-r17:     187      459
-r18:    2991     1211
-r19:     549       43

Wow! a clear winner!
Temp=2 clamped, avg time== 913 secs

And again for temp=1

                clamped
        T=1       T=1
-r0:     262      494
-r1:  >19261      226
-r2:     172      834
-r3:      93      108
-r4:    1560       72
-r5:     134      398
-r6:    9752.x   4559
-r7:      39      135
-r8:     665      212
-r9:    2150      927
-r10:     30      770
-r11:     15     2225
-r12:    360      646
-r13:    292     3522
-r14:    961      871
-r15:   1972       68
-r16:     34       70
-r17:   7138    >5948
-r18: >31836      235
-r19:    172     2028

Temp=1 clamped, avg time== 1221 secs
Wow .. a clear looser!

         T=3    clamped
-r0:        4      4
-r1:      514   1047
-r2:      444    118
-r3:     5520    855
-r4:       58     60
-r5:     2106    192
-r6:       53     46
-r7:     2566   2074
-r8:     6867    235
-r9:      143    138 
-r10:     158    936
-r11:      77    114
-r12:     700   1268
-r13:     140     35
-r14:     931    128
-r15:     306   1260
-r16:    1547     34
-r17:      90    722
-r18:      22     39
-r19:     808    860


avg clamped == 508 -- wow this is the best yet ... 
There's no long-running outliers to pull the thing way out ... 
graph shows that T=2 is still the best, so use that in the code.

Current consolidated dataset:

#
# moses performance data, runtime in seconds
# using default hillclimbing.
# -Hpa -k4 in order of runtime
#
# first column: original data
# second column: a 10% performance speedbump due to logging.
# third column: broaden the complexity chooser (by 2)
# fourth column: broaden the complexity chooser (by 3)
# fifth col: T=2 and clamped metapop size.
#
# in select_exemplar():
# p = (p > 0 ? 0 : pow2((p - highest_comp)/T));
#
#                                 clamp  clamp  clamp 
#   orig       T=1    T=2    T=3   T=2    T=1    T=3
1   15          15      8      4     8     68      4
2   30          30     14     22    14     70     34
3   34          34     20     53    35     72     35
4   51.24       39     43     58    41    108     39
5   100.87      93     67     77    43    135     46
6   148.59     134     83     90    59    212     60
7   172        172    187    140    67    226    114
8   196.12     172    220    143    94    235    118
9   286.62     262    324    158   104    398    128
10  292        292    502    306   161    494    138
11  360        360    522    444   359    646    192
12  731.03     665    549    514   371    770    235
13  961        961    568    700   358    834    722
14  1690.13   1560    628    808   459    871    835
15  1972      1972    916    931   548    927    855
16  2367.36   2150   1131   1547   680   2028    860
17  7138      7138   1168   2106  1082   2225   1047
18  9752.87   9752   1656   2566  1211   3522   1260
# 19 >19261 at -m1.1M for T=1,  >6666 at -m1.6M
19  9999     19261   2991   5520  6666   4559   1268
# 20  >31836 at -m1.1M for T=1  the 6666 7777 values are bogus
20  9999      31836  5371   6867  7777   6666   2074


See http://blog.opencog.org/2012/02/07/tuning-moses/ for written
explanation.
==================================================================

Try again with -I1 .. what will this do??
Also explore -P pos_size_ratio

As above, for T=2 in select_exemplar() temperature, comparing
capped and uncapped metapop size.  (capped is done via the auto-capping
mechanism that is the current default.)

Third column is with -I1 spec'ed, so that metapop includes dominated
exemplars.

Fourth column: -P40 -- pop-size-ratio = 40

       uncapped   capped   domin   P40    P10
-r0:        8        8       8      9      8
-r1:       14       14      15     15     15
-r2:     5371    >5957   >1453  >1574  >1623
-r3:     1168      358     344    356    358
-r4:     1131      680     662    698    718
-r5:      220     1082    1022   1090   1136
-r6:      502      371     374    374    407
-r7:       43       41      44     43     44
-r8:      324       59      62     61     63
-r9:      628       67      69     68     69
-r10:     522       35      36     35     36
-r11:      67      161     160    159    163
-r12:    1656      548     543           567
-r13:     568    >5553   >1355         >1496
-r14:      20      104     108           109
-r15:      83     1299    1290          1351
-r16:     916       94      92            93
-r17:     187      459     502           477
-r18:    2991     1211   >1147         >1185
-r19:     549       43      45            45


Looks like a tie to me... 
Upshot: using the -I1 flag to keep dominated exemplars in the population
does not seem to affect runtime. I'm guessing that this is because the
dominated exemplars are never selected.

==================================================================

Modified sort order:
bool composite_score::operator>(const composite_score &r)
{
   return (3*first + second) > (3*r.first + r.second);
}

This results in the metapop being kept in a different order, for
example, the first 12 entries might be this:

metap 0 score=-5 complex=-8
metap 1 score=-5 complex=-8
metap 2 score=-8 complex=0
metap 3 score=-6 complex=-6
metap 4 score=-6 complex=-6
metap 5 score=-6 complex=-6
metap 6 score=-6 complex=-6
metap 7 score=-5 complex=-9
metap 8 score=-5 complex=-9
metap 9 score=-5 complex=-9
metap 10 score=-8 complex=-1
metap 11 score=-8 complex=-1

OK, so 3 (plus auto-clamping) fails, no solutions found at all.
Disable autoclamping.  Gahh .. seems to run forever ... 
Redesign clamping ... 

Below, for different mixing weights W

         capped  W=4    W=5    W=6    W=8    W=3    W=2
-r0:        8     40      2     11     71    144   >516
-r1:       14    121     79    122     50  >1004   >536
-r2:    >5957     77     84    202  >1448    103   >543
-r3:      358    471     16     99    521     69   >556
-r4:      680     76     35     74  >1247    150   >503 
-r5:     1082    280     44     46     88    404   >524
-r6:      371     95    785    100     83    382   >541
-r7:       41    125    425     53    153     53   >510
-r8:       59     86  >1236  >1148    898     38   >525
-r9:       67     48     31  >1365  >1401     95   >551
-r10:      35     34     94     40     50    192   >528
-r11:     161    319    761   1060   1230    197   >552
-r12:     548     42    786    176  >1348     44   >545
-r13:   >5553    154     49    251  >1323     47   >508
-r14:     104     69    451    211     29     21   >545
-r15:    1299    287   1483    105    169    140   >551
-r16:      94  >1907     82     51    103    266   >509
-r17:     459    141    196    165    900    127   >508
-r18:    1211     87  >1095  >1123    402     93   >558
-r19:      43     73     72    630    392    188   >557


W=4 avg == 227

==================================================================

Arghhhh Move to floats, revamp, all changes

set weight to very large to regain old behaviour (i.e. rank first by
score, next by complexity).

As above -Hpa -k4 ...
-z12 -v1 -m50000  (90 seconds, when no solution)

negative numbers indicate no solution, and how many wrong the best
guess had.

z:    -z12  -z12   -z12   -z12   -z12   -z12    -z4    -z4   -z4    -z4
temp: -v100 -v50   -v20   -v10    -v5    -v2    -v5    -v2   -v1    -v3
-r0:   -2     -1     -1     -1     -1     -1     34     43    -1     30
-r1:   -1     80     -1     -2     16     30     72     -2    -1     -2
-r2:   -1     -1     65     36     33    111     34    155   150     -1
-r3:   14     -2     -2     -1     -2     -1     81     -1    -1     -3
-r4:   67     67     -1     -1     27     -2     69     -1    36     83
-r5:   -2     -2     -3     -3     -1     92     -1     -1    -2     -2
-r6:  102     -1     -1     10     29     -1     67     -2    -1     62
-r7:   -1     -1     -1     -2     -1     -2     21     77    45     -1
-r8:   -3    133     34     -1     -2     -2     39     52    62     84
-r9:   -2     -2     -1     -2     -1     -1     -1     18    22     16
-r10:  -2     -1     51     23     -2     -1     -1     -2    63     -2
-r11:  -1     -1     35    103     25     48     -2     -1    58    116
-r12:  -1     -1     29     -1     28     38     91     36    60     -1
-r13: 109     74     -1    105     -1     11     -1     -1   122     -1
-r14:  -2    103     -2     -1     45     53     21    172    -2    133
-r15:  -1     -1     -1    123     -1     86     -2     -2    -1     -1
-r16:  -1     -1     66     17     -2     -1     -1     16    -1     69
-r17:  -2     83     11     -1     89     -1    208     -2    -2     -1
-r18:  -1     -1     26     -1     -2     -1     75    147    -1     91
-r19:  41     91     -1     -2     -2     -1    113     -1    -1     -1

tot:    5      7      8      7      8      8     13      9     9

The tot bove is how many had perfect score, out of the 20.

Again, but this time with longer run-times, for the graphs:
-m45000

z:        -z4     -z4
temp:     -v5     -v2
-r0:       34      43
-r1:       72     705
-r2:       34     770
-r3:       81     213
-r4:       69     426
-r5:      276     136
-r6:       67      -1
-r7:       21
-r8:       39
-r9:      279
-r10:     661
-r11:     243
-r12:      91
-r13:     152
-r14:      21
-r15:     282
-r16:     284
-r17:     208
-r18:      75
-r19:     113 

avg:      161

==================================================================
Above may be non-reproducible, due to changes in the max-pop-size logic.
So restart data collection.

As above -Hpa -k4 ...

z:      -z3   -z3   -z3   -z4   -z4   -z4   -z2   -z2 -z3.5 -z3.5 -z3.5
temp:   -v3   -v5   -v7   -v5   -v3   -v7   -v3   -v5   -v5   -v4   -v6
-r0:    308    19    24   711    42   631    -4    -4    24    59    33
-r1:    156   239    49   760   100   106    -4    -3    61   117   104
-r2:     72   230    76   103   802   136    -4         227    30    95
-r3:     84   349    83    73   296    48               207    41   101
-r4:    254    35   203   232    41    82                47   375    54
-r5:     85    94   168    19   109   337                22   110    23
-r6:    396    98   114    62   504   113               129    42   124
-r7:    182   715    17   586    43    88               222    66    28
-r8:     34    59   140   137    72   161               234   617    75
-r9:    819    73    71    25    14   132               125    24    61
-r10:   173   170   120   260   579  1166                34    19    27
-r11:    52    96    40    90   118   650               135    92    38
-r12:   188    16   716    70   701   670                21   241    23
-r13:   379    76   141   378   416   220               140    83    31
-r14:   136   518   502   230   242   560                88    52    51
-r15:   361    44    82    71   975    22                85   135   172
-r16:   250   258    74   261  1123   111                55    54     6
-r17:    34   387  1047   187   762   710               132   142    87
-r18:    11   168   516   183   183    85               114    33    15
-r19:    13     9    49   806    20   411                 5    10    52

avg:    199   183  211    262   357   322               105   117    60
120avg:       179                                       115
240avg:                                                             109

setting:  avg:   avg-over-what:
z3.5-v9   132    120 iter
z3.5-v7   110    120 iter
z3.5-v6   109    240 iter
z3.5-v5   115    120
z3.5-v4   131    120

Holy cow, that last one is smokin!
But a run over 240 different random seeds averaged out to only 109
seconds, not 60 seconds, so the smaller run just was not realistic.

==================================================================

 Attempts at 5-parity

parity -k5 results   -Hpa -k5 

All are -m150K unless otherwise noted, right?
All are for bzr rev 6582, unless otherwise noted.


                                                 rev 6622
                                   -m450K -m1.5M  -m1.5M
     -z4v6 -z5v6 -z6v6 -z7v6 -z6v4  -z6v6 -z6v6   -z6v6
-r0    -8    -6    -6    -8    -6    -4     -3      -1
-r1    -6    -8    -9    -6    -6    -4     -2      -2
-r2    -8    -9    -5    -8    -5    -5     -3      -4
-r3    -9    -6    -7    -8    -6    -3     -1     907
-r4    -6    -6    -9    -9    -9    -6     -1      -3
-r5   -11    -8    -6    -7    -5    -6     -4    1796
-r6    -6    -6    -6    -8    -6    -2   6881      -3
-r7    -6    -9    -6   -10   -10    -2   3078      -2
-r8    -4    -5    -7    -6    -8    -5             -4
-r9    -8    -7    -7    -6    -6    -4             -3
-r10   -7    -8    -6    -7    -7    -6             -4
-r11   -6    -6    -6    -6   -10    -4            714
-r12   -9    -6    -5    -5    -7  1117           4032
-r13   -9    -7    -4    -7    -6    -4             -3
-r14   -7    -7    -6    -6    -7    -4             -5
-r15   -7    -8    -7    -7    -7    -5             -4
-r16  -10    -5    -6    -5    -9    -5             -1
-r17  -10    -5    -8   -10    -7    -4             -4
-r18   -5   -10   -10    -6    -5    -4             -4
-r19   -8    -6    -8    -5    -8    -3           5906

best: -4x1       -4x1   
      -5x1 -5x3  -5x2   -5x3  -5x3
      -6x5 -6x7  -6x8   -6x6  -6x6


Use the -s0 flag to turn off caching, so as not to run out of RAM.
 ... WOW! HOLY COW! That makes a HUGE difference in RAM usage!!

-s1: 2419 at 04:45:26:632 from 21 at 03:25:09:777 == 1:20:17 == 4817
-s0: 2419 at 06:12:58:836 from 21 at 04:45:49:574 == 1:27:09 == 5229

Hmmm ... turning it off results in an 8% slowdown.

/sbin/sysctl vm.swappiness

===========================================================
Is over-fitting a danger?  Well, we are over-fitting if the compexlity
of the solution formula approaches the complexity of the raw table of
data.  So lets compare them:

3-arity: 3 boolean-valued inputs one boolean-valued output,
total: 2^3=8 rows 4 bits per row: thus, complexity of table
is 4x8=32 bits.
typical moses solution: 10 or 11 bits.

4-arity: 4 boolean-valued inputs one boolean-valued output,
total: 2^4=16 rows 5 bits per row: thus, complexity of table
is 5x16=80 bits.
typical moses solution: 20 bits

Clearly, for the parity problem, the solution does not over-fit.

===========================================================
===========================================================
===========================================================
===========================================================
===========================================================
Banking questionaire dataset, February/March 2012
Data collected on the bank.csv test case (in the example-data directory).

Original hill-climbing algorithm:
moses -Hit -uQ3 -i bank.csv -W1

        score   time
-m10K:  -361     128
-m20K:  -278     487
-m40K:  -215    1808
-m80K:

Re-do -- the above had a bug in reduct, where the argmument of impulse()
was not being reduced.  Fixed in bzr commit 6586

          z-default             -z6 -vdefault      -z10, -v1
         score   time  plex  score  time  plex  score time   plex
-m10K:   -361     176   39   -346   194    34    -367  162    25
-m20K:   -258     477   47   -275   540    41    -274  656    51
-m40K:   -210    2496   71
-m80K:   -174   15445 
-m160K:  -159   50430

complexity of bank.csv:
91 lines, 38 bits input per line, one contin out, valued: 1-16: 4 bits
total: 91x42 = 3822 bits
Conclude: it'll be hard to over-fit the data, at the rate we are going.

Note really poor run-time performance of above.  This seems to be due to
this:  First, we build a fairly small exemplar, and hill-climb it... but
not until exhaustion, because the hill-climbing budget is badly
calculated.  Then, for the second round, we build a massively complex
exemplar, which is very expensive to evaluate... and then we don't even
beother to exhaust the nearest-neighbor search before throwing it away.
Solution: at least do an exhaustive search for closest
nearest-neightbors, before giving up.  This seems to help:

              z-default           exhaust             copy avoid
         score   time  plex   score   time  plex   score   time   plex
-m10K:   -340    45     14    -340     45    14    -340     41    14
-m20K:   -265   167     30    -265    171    32    -265    162    32
-m40K:   -228   658     52    -248   3103    40    -248   2670    40
-m80K:                        -219  11117    50    -219  10435    50
-m160K:                       -196  29415    60    -196  26655    60
-m320K:                        4 gigs ram...

Gahh ... -m40K goes to 3103 seconds, -248 score, -40 complexity when
the first search is exhausted, and the second exemplar gets built.
The second exemplar is huge/complex, and very slow to evaluate...

OK, "Select candidates to merge" is taking too long:
8754 in 2:16 or 136 secs = 15 millisecs each -- now 2:10
11904 in 1:35:23 or 5723 secs = 480 millisecs each -- now 1:29:09
29846 in 4:19:24 = 15563 secs = 520 millisecs each -- now 

The "copy avoid" columns avoid a few pointless copies of the combo
tree to the stack.  The --now times are the copy-avoid.

Performance of 
        auto select_candidates =
in metapopulation.h is still a disaster area.
But this is entirely due to 
combo_tree tr = this->_rep->get_candidate(inst, true);
which in turn has 80% of time spent in get_clean_combo_tree()
in representation/representation.cc 
and this is not easily fixable.

======================================================================

Redo, with -n log -n exp -n sin  and again with -ndiv

             baseline            nosin exp log       nosinexplog nodiv
         score   time   plex   score   time  plex   score   time  plex
-m10K:   -340     41    14     -274    128    47     -258    256   32
-m20K:   -265    162    32     -222    519    55     -232    720   42
-m40K:   -248   2670    40     -182   1413    67     -190   1986   57
-m80K:   -219  10435    50     -145   3950    87
-m160K:  -196  26655    60
-m320K:

======================================================================

Redo, with correlated hillclimbing
basedline is 
-Hit -uQ3 -i bank.csv -W1 -x1 -lDEBUG -n sin -n log -n exp

"baseline" is from the above (the -n sinexplog flags)

"40-corr" keeps only the top 40 instances for the next hill-climbing
round. Since each climb uses up one instance, its impossible to have
more than 40 climbs, so hill-climbing then terminates, and control returns
to the metapop loop.

The "re-up+trim" column does two things: 1) adds a new population control
mechanism, and 2) does a full nearest-neighbr search every tenth hill-climb.
For the other 9, it uses the same top-40 instances.

The "trim-only" column removes the top-40 code, keeps to trimmed metapop
size code, and also skips computation of behavioral scores.  Notice how
trim-only score match the baseline, but offer a 2x run-time improvement.
"trim-only" is exactly the code in bzr version 6616, no modifications.

The "curr+reup" column shows bzr rev 6616 plus the reup algo.  Its almost
identical to "re-up+trim", and slightly faster, which is good: nothing was
lost during the intervening hacking. (40/10 means: 40 instances from the
1-simplex, and a full nn scan every 10 generations.)  This is the current
winner.

The "100/20" column keeps the 100 top-scoring instances for the next
round, and does a full nearest-neighborhood search only every 20
rounds.

The "40/20" column keeps the 40 top-scoring instances for the next
round, and does a full nearest-neighborhood search only every 20
rounds.

The "40/6" column keeps the 40 top-scoring instances for the next
round, and does a full nearest-neighborhood search every 6
rounds.


             baseline              40-corr              re-up + trim
         score   time  plex    score   time  plex    score   time  plex
-m10K:   -274    128    47     -258    268    62     -185     245   57
-m20K:   -222    519    55     -142    972   115     -128     571   78
-m40K:   -182   1413    67     -135   3007   125     -116    2176   90
-m80K:   -145   3950    87                           -116    6042   92


                                      nn5                    nn6
             trim-only          curr+reup(40/10)           100/20
         score   time  plex    score   time  plex    score   time  plex
-m10K:    -273    91    48      -185    231   57      -214   233    80
-m20K:    -222   256    53      -128    543   78      -159  1082   104
-m40K:    -183   635    63      -116   2043   90      -146  2991   112
-m80K:    -148  1898    85
-m160K:   -125  7798   108
 

                 nn7                   nn8
                40/20                  40/6                  20/6
         score   time  plex    score   time  plex    score   time  plex
-m10K:    -240   175    59      -252   144   53
-m20K:    -150   544    89      -147   383   79
-m40K:    -115  1760   112      -116  1506  108
-m80K:    
-m160K:   


New baseline: below: bzr rev 6645  (actually, this, but without Nils's
most recent 12 changes... Nil's changes introduce something screwy...).

"baseline" is as code is checked in. -- Except -- GATHER_STATS is
defined, so this does introduce a sorting overhead that is hopefully
not too big ...  !?

"40/10" is the usual (40 instances, full nearest-neighbor refressh 
every 10 generations), with the 1-simplex.  

"40/rescan" evaluates 40 instances, but performs a full neighbor
refresh only when no improvement is seen.  Again, this is 1-simplex.

"40/10/2" is same as 40/10, but with the 2-simplex. (this is nn11, its
inferior to nn13, nn14 and nn15, according to a graph).

"40/10/3" is same as 40/10, but with the 3-simplex.

"40+40+40/10/1+2+3" is all three, combined, with full neighborhood
re-eval every 10 generations.  40 new instances per simplex.

"rescan/1+2+3" is all three, combined, with full neighborhood re-eval
only when there's no improvement.  40 new instances per simplex.

"r6660" is bzr rev 6660, but with the simplex&rescan code turned on,
GATHER_STATS turned off.  I was expecting this to be same as above,
but its not. Don't understand why... slower to get going than either of
last two, but breaks past the barrier... Partial answer: 1) the
min_score_improvement logic was accidentally broken in this rev, and
also 2) there is more random-seed variability than I expected...

"nolast" is 6660, with simplex and rescan, but the last-chance simplex
disabled. Seems to get started a bit faster, than the previous, but
stalls out sooner. Is this accidental i.e. due to random seed?

"r6660-r1" is same as r6660, but with -r1 random seed, instead of r0
(all others are r0).  at first faster, then false behind at 20K but
pulls ahead by 40K...  go figure... 

"r6660-r2" is same as r6660, but with -r2 random seed, instead of r0

"volume" explores 10/40/70 instances inthe 1/2/3-simplex. Total is still
120 as before.

Argghh. Many/most(???) of above have a min_score_improvement of 0.0 not
0.5, and thus suffer from crazy score tweaking. This should not affect 
the short runs, but could be ruinous for the long runs ...

"r6661-r1" is same as r6660-r1 but with the min_improvment regression
fixed.  Also slightly modified "last-chance" logic.

"r6663-r1" is same as above, but with two silly loop fixes, including
putting back "last chance" as before.  Probably will make very little
difference...

"rebase" is revision 6666, but without the -Z1 flag. This should
reproduce the old, full-neighbor-scan version of the code.  Poor
performance is expected... and, indeed, it seems to resemble "baseline"

"renew" is revision 6666, with the -Z1 flag. This should match
"r6663-r1" exactly... and indeed, it does.

"all-small" is revision 6670, with -r1 -Z1 flag. The change here is an
exhaustive search of the local neighborhood, when the local neighb is
small.  This gives very good results for the smaller exemplar sizes
for the first few exemplars.

"regress" is revision 6726, trying to make sure things still work.
There was a change in the loop termination criteria.  -r1 -Z1


              baseline            40/10 (nn9)         40/rescan (nn10)
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -255     65    50     -163    170   54      -179    207   59
-m20k:   -229    243    53     -139    688   71      -160    655   71
-m40k:   -187    798    64     -131   1829   80      -130   1490   87
-m80k:   -153   1933    86     -131   4797   79      -125   5124   92
-m160k:  -131   7476   104
-m320k:  -120  30129   119
-m640k:  -119  97385   117


           40/10/2 (nn11)        40/10/3 (nn12)    40+40+40/10/1+2+3 (nn13)
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -302     62   27      -282     75   45      -203     98   47
-m20k:   -208    196   47      -209    220   59      -145    305   62
-m40k:   -135    740   72      -167    718   76      -118   1112   87
-m80k:   -116   2451   89      -149   1986   85      -118   3502   85
-m160k:                                              -118  10326   85
-m320k:                                              -118  24011   85


        rescan/1+2+3 (nn14)     r6660 (nn15,20)         nolast (nn16)
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -248    138   44      -305     82   47      -309     80   47
-m20k:   -160    462   66      -184    381   86      -187    301   70
-m40k:   -125   1354   86      -125   1553  119      -109   1057  122
-m80k:   -115   4010  104      -100   5112  137      -104   4410  122
-m160k:                         -95  19500  137


        r6660-r1 (nn17,21)       r6660-r2 (nn18)        volume (nn19)
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -298     81   47      -305     82   48      -315     79   38 
-m20k:   -206    318   71      -206    322   71      -192    302   61
-m40k:   -117   1179  118      -118   1294  119      -118   1033   99
-m80k:    -94   8000  128                            -109   4490  111
-m160k:   -89  18000  129                            -108  16954  112
-m320k:   -89  48000  128                            -105  35820  115


          r6661-r1 (nn22)       r6661-r0 (nn23)       r6661-r2 (nn24)
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -298     85   46      -305     84   47      -302     81   48
-m20k:   -206    329   71      -183    382   86      -206    316   70
-m40k:   -119   1175  112      -130   1650  114      -120   1304  114
-m80k:   -104   5369  124      -101   4985  129       -97   4592  127
-m160k:   

          r6662-r1 (nn25)       r6663-r1 (nn26)        rebase (nn27)
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -298     82   47      -303     78   48      -248     57   59
-m20k:   -206    317   71      -210    304   68      -231    223   64
-m40k:   -117   1167  118      -125   1016  101      -190    839   75
-m80k:    -94   5001  128      -108   3637  113      -162   2150   85
-m160k:   -89  16603  130      -102  14793  120
-m320k:   -89  39974  129


           renew (nn28)         all-small (nn29)      regress (nn30)
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -303     76   48      -248     58   59      -248     57   59
-m20k:   -210    305   68      -206    266   74      -206    264   74
-m40k:   -125   1032  101      -132   1086  114      -132   1064  114
-m80k:   -108   3877  113      -122   6638  129
-m160k:  -102  14427  120      -121  20186  127
-m320k:  -101  41033  121      -121  45621  125

Note: the 2-simplex and 3-simplex cross-over code had a bug in it, which
got fixed in bzr rev 6996.  Data above that relies on 2- and 3-simplex may
improve; on the other hand, the 2- and 3-simplex code was corrupting the
deme, and so who knows what was really happening, or how big the effect is.
(The bug was fixed on 27 May 2012)

================================================================

BANK DATASET REGESSION TESTING
------------------------------
Trying to verify ongoing hapiness of the algos.

Bank dataset measurements  columns are final score.
This is circa bzr rev 6647 or so. (actually, 6647, with Nils most
recent 12 changes removed).

-mxx == different numbers of iterations.
Conclude: very little variation due to random number gen. This is very
different than the parity problem.

           -m10000                -m20000            -m40000
:      time  score  plex     time  score  plex    time  score  plex
-r0:     65  255.8    50     250   229.8    53     741  187.1    64
-r1:     65  255.8    51     246   229.8    53     771  187.1    64
-r2:     65  255.8    51     248   231.65   54     745  187.1    64
-r3:     64  255.8    50     247   229.8    53     749  187.1    65
-r4:     65  256.1    49     247   230.3    52     758  186.5    66
-r5:     65  257.5    51     248   230.3    53     760  184.1    67
-r6:     65  256.1    49     248   232.06   53     784  186.5    65
-r7:     66  259.8    51     249   230.3    54     743  184.5    67
-r8:     65  256.1    50     248   230.8    52     771  186.5    66
-r9:     64  255.8    48     247   229.87   53     793  186.1    65


"bzr7131-5K" is bzr rev 7131 (13 June 2012), so with above-mentioned bug-fix,
    and of course the new, rescaled, temperature scale (mmerged occam code),
    also evaluation clamping to 5K evals per instance NN's.  This command line:
    moses-perf -Hit -uQ3 -i bank.csv -W1 -x1 -n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000  

"bzr7131-1K" as above, but --hc-max-nn-evals=1000


           bzr7131-5K             bzr7131-1K           bzr7131-2K
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -249     64   58      -249     63   58      -249     65   58
-m20k:   -205    314   72      -201    435  103      -202    410   98
-m40k:   -138   1134  107      -187   2970  122      -175   2635  118
-m80k:   -120   6130  131      -185   9714  122      -169   9022  126  
-m160k:  -


           bzr7131-10K             bzr7131-20K           
         score  time  plex     score  time  plex     score  time  plex
-m10k:   -249     65   58
-m20k:   -205    314   72
-m40k:   -138   1136  107
-m80k:   -120   6077  131
-m160k:  -120  27310  131

======================================================================
Regression test parity performance

The "budge" column shows the effect of exhaustive nearest-neighbor
search in hill-climbing.  Conclusion: this is a sure-fire win! Woot!
commited in bzr revision 6589/6590.

The "trim" column shows the effect of triming the deme, by score,
before merging it into the metapop.  It also uses a score-weighted
metapop population control. (previous metapop posize control was
indirect and wonky, and less effective).  committed in bzr revision 
6611 (this revision).  Notice that this is worse than before, 
which is unearthed below ... the reason turns out to be the introduction
of "min_score_improvement" in rev 6594.

The "tops" column shows the effect of trim, plus the top-score approach.
It does seem better than trim ... 

The "wtf" column is bzr rev 6610, trying to figure out what broke.
Answer: rev 6593 was OK but 6594 broke. this is due to 
min_score_improvement added in optimization/optimization.h lin 535
(i.e. min_score=0.5 was introduced then.)

The "r6594" column is performance of revision 6594.  Note the
horrible degradation of performance, compared to "trim".
Using this rev, and setting min_score=0.0 gets us the nice 
desirable "budge" numbers.  The reason that r6594 is worse than
"wtf" is because "wtf" contains a copy optimization, introduced in
revision 6598.

"boost" is rev 6596 (i.e. uses boost for the compare ops) but with
min_score=0. Compare this to "budge", which differs from this mostly in
the use of boost for the inequality compares.  Is it justified to
conclude that boost is harmful? If so, then just barely...

The "min0" column is latest rev 6611 i.e. budge+trim, and min_score=0.0
OK, best yet.

The "prim" column is the pre-trim revision 6610, with the min_score=0.0
Clearly, this is nearly identical to the "boost" column. The only real
difference between these two is the no-copy fix, introduced in rev 6598.
We conclude that the no-copy change gives us: (98-94)/94 = 4% performance.

As above -Hpa -k4 -z3.5 -v5 -m350000

         orig  budge  trim   tops   wtf   r6594  boost   min0  prim
-r0:      24    23     55     55    236     57    137     18    134
-r1:      61    60    120     24    248     34     30     81     29
-r2:     227   144     50     50     49     72     86     35     83
-r3:     207    36     23     23    129     91    103    199     99
-r4:      47    45     31     31     58    131    573    165    554
-r5:      22    22    185    229    319    105    121     80    115
-r6:     129   115     32    179    296    133     29     26     28
-r7:     222    58     21     36     24     15     76     43     73
-r8:     234   521    162    184     61    225     61    308     58
-r9:     125    54    297    319    142    216     72     80     69
-r10:     34    33     41     40     90    269     86     71     83
-r11:    135    44     37     37     32     80     43     39     41
-r12:     21    20    206    109    197    226     46     16     44
-r13:    140    70    510     35    185    220     93     24     90
-r14:     88   116    223    453    190    378      5      5      5
-r15:     85    70     76     54     83     31     50     35     48
-r16:     55   107    258    135    229     58    113     73    108
-r17:    132   195     22     22     44    115    103     89    101
-r18:    114   104     39     31     63    398     67     81     64
-r19:      5     5     37    195     57    270     65    276     64

avg:     105    93    121    112    137    156     98     87     94
double-checked:         y                                  y      y

Below:
"min0" as above: revision 6611, with min_score = 0.0

"weigh" is rev 6611, with min_score = 0.5*weight/(weight+1)
Compare this to "trim" above, which is exactly the same code, but
without this weight correction (i.e. min_score = 0.5)

"no-bs" is rev 6616: don't compute the behavioural score, because it's
not needed unless one is doing domination-based merging.  (Note scores
are consistently 1-2 seconds slower than "weigh", this seems to be due
to other background jobs on this machine, and/or different cache patterns.
Reversing flag does not change time.)

"corr" is rev 6619: a correlated neighboring instance search. A bit
of a regression from previous results, but seems to be due entirely
to one outlier.  Further experiments/tuning to follow...

"no-cache" is rev 6620: correlated search disabled, but also -s0 on
command line to disable caching.  This results in much much lower
mem usage.  No perf benefit within error bars: (73-72)/72 < 1%

"nored" is as above, but with -d0 on command line: skips the reduction
of exemplars before scoring. (scores the knob-turned exemplar directly).
Not quite twice as slow, as a result.

"erase" is a revised low-score erasure algorithm.  Cause of the disasterous
slowdown is .. unclear. I think it is due to the erase() triggering a sort
of the metapop, which then damages performance.


         min0  weigh  no-bs  corr  no-cache nored   erase
-r0:      18     35     37    116     37      71     345
-r1:      81     52     54     98     54     281     120
-r2:      35    107    112     34    112      23      74
-r3:     199     15     16     56     16      31      24
-r4:     165     15     16     17     16      81     242
-r5:      80     38     41     23     40     107      41
-r6:      26    111    115     60    115      37     104
-r7:      43     67     68     17     69      28     122
-r8:     308    170    112    181    113     187      67
-r9:      80     39     41     40     41      61     141
-r10:     71    150    193     31    195      84      58
-r11:     39     18     19     18     19      57      22
-r12:     16    129    133     98    133     124      77
-r13:     24     98     99    145    100     102     160
-r14:      5    155    151    195    151     405      91
-r15:     35     42     43    150     42      49     279
-r16:     73     72     75    343     75     365      52
-r17:     89     32     33     51     33     461      22
-r18:     81     36     37     31     37      77     103
-r19:    276     51     53     29     53      22      23

avg:      87     72     72     87     73     133     109


Again:
"baseline": revision 6616 (same as no-bs column above)

"wtf": revision 6629 -- why is this not identical to baseline ?? Oh,
because recent changes broke neighborhood size counting.

"refix": revision 6630 -- why is this not identical to baseline ?? 
Didn't 6630 restore the old logic?  Anyway, scratching the outlier
gets us a reasonable number for the avg ... 

"r6642" is bzr rev 6642 -- clearly something is very broken here.

"r6634" is bzr rev 6634 -- seems to be identical to refix/6630

"r6636" is bzr rev 6636 -- seems to be same as 6642; very broken.

"improv" is rv 6636 with scoring.cc min_improv() set to 0.0, not 0.5
OK, this has a different profile, but a passble time.  Ahh. Bug: 
main/moses_exec.h:    opt_params.min_score_improv =
bb_score.min_improv();  this is just plain wrong.
setting to 0.4 gives r6634 results.

"check" is revision 6651 -- simplex code disabled.  Very interesting,
seems to be identical distribution to r6642, but 15% slower.  (this is
probably because I did not comment out all of the simplex code. whoops)

"reimp" is rev 6652, but with the min_improv logic fixed.  (and the 
simplex code commented out; this time, all of it).  Looks like we are
back to refix performance levels.  Again, ditching the outlier, we
get comparable perf to before.

"zippo" is same as above, but with min_improv set to zero. Seems to be
identical to improv, which is expected.  But why are all of these slower
than the baseline ?? Answer: fluke, see immediately below.

"120avg": Are the avg stats trustworthy? To determine this, run with
120 different random seeds.  This should give signficantly better
results the effect of outliers.  Hmm. Graphing the 120 and fitting
an exponential decay shows a very clear exp(90*p) trend line, ruined
by 7-8 outliers.  Both reimp and zippo have exactly the same trend.
Redo of baseline 6616 for 120 also gives same graph, and even slightly
worse, overall. Yayyy! This gives bzr rev 6653 a clean bill of health!
Woot!

(This also justifies discarding outliers...)

As above -Hpa -k4 -z3.5 -v5 -m350000

      baseline  wtf   refix  r6642 r6634 r6636 improv check reimp zippo
-r0:      37     56     36    174    37   170    23    213    37    22
-r1:      54    118    128     89   130    86    58    113   132    56
-r2:     112     39     38     70    38    67    89     87    39    85
-r3:      16    116     38    137    39   133   165    169    39   163
-r4:      16     72     81    226    83   218    43    281    85    42
-r5:      41     67     67    260    67   253    21    316    70    21
-r6:     115     23     83    130         126   111    159    87   108
-r7:      68     15     15     14          14   195     18    15   188
-r8:     112     86     70    374         360   195    450    72   189
-r9:      41    112    104    172                75    209   108    74
-r10:    193     61     60     32                32     42    62    31
-r11:     19     89     84     39               133     50    87   129
-r12:    133     81     53    126                20    156    55    20
-r13:     99     74     74    351               121    429    77   118
-r14:    151     39     37    149               113    202    38   108
-r15:     43    137    506     29                70     35   521    67
-r16:     75     51     51    248                49    302    52    48
-r17:     33    125    118    112               247    137   122   238
-r18:     37    107    116    192               100    240   120    97
-r19:     53     57     56    297                 5    365    58     5

avg:      72     77     91    162                93    198    94    90
120avg:  111                                                 102   107

-------------------------------------
See notes above: revision 6653 gets a clean bill of health!  Good to go!

"simp" is 6653, simplex enabled, but rescan disabled.  Default
min_improv=0.5 setting (weighted down to 0.28)

"enab" is 6653, with simplex and rescan enabled, using flag -L1 to
terminate search immediately.  Clearly, this doesn't bypass the core
problem.

"rech" is 6666; expect this to be comparable to previous runs.  In fact,
its identical to "zippo" above. No regression, no mystery, good to go.

"rchk" is r6726.1.2; some minor changes in termination criteria, should not
have a big effect...  Hmm, numbers seems to be identical to "rech"+20%.
Which means the termination was not affected (random numbers are drawn
in the same order), but something added to the execution time ... !?

As above -Hpa -k4 -z3.5 -v5 -m350000

      simp   enab  rech   rchk
-r0:   101    188    22     28
-r1:   284     -2    58     71
-r2:   200    384    88   -109
-r3:   133    245   167    206
-r4:    32    154    43     53
-r5:    24    173    21     26
-r6:    63    189   111    139
-r7:   114     57   191    247
-r8:    84    139   193    242
-r9:    62     43    75     94
-r10:   80    404    33     40
-r11:  199    855   134    165
-r12:   13    423    20     25
-r13:  122     87   120    151
-r14:   43     98   115    139
-r15:   97    170    71     85
-r16:  396    127    49     60
-r17:   43    587   246    200
-r18:   47     99   100    124
-r19:  102    910     5      6

avg:   112    318    93    111


===========================================================
===========================================================
===========================================================
===========================================================
===========================================================
Iris clustering dataset, May 2012

Data collected on the iris.data test case (in the example-data directory).
This is a famous dataset going back to 1936, for categorizing Iris species,
based on the dimensions of their leaves, etc.

Basic command line:
moses -i iris.data -u5  -n sin -n log -n exp -n div
i.e. we don't want any of the higher math functions.
Data collected early May, circa bzr revision 6889 or a little earlier.

           -m10K           -m20K           -m40K            -m80K
:     time scor plex   time scor plex  time scor plex   time scor plex
-r0:    63    5   8     143    5   8    305   5    8     695    5   8
-r1:    53   11   9     183   11   9    438  11    9    1021   11   9
-r2:    44   46   9     128    7  10    323   7   10     918    7  10
-r3:    80   50   5     179   50   5    381  50    5     951    8  13
-r4:    58   54   6     121   54   6    251  54    6     643   16  10
-r5:    83    6   7     204    6   7    417   6    7     900    6   7
-r6:    40   50   4      82   50   4    171  50    4     374   50   4
-r7:    82   50   5     174   50   5    355  50    5     808   50   5
-r8:    69   41  10     203   41  10    512  40   13    1342   40  13
-r9:    55    6   7     128    6   7    282   6    7     645    6   7


          -m160K           -m320K
:     time scor plex   time scor plex  time scor plex   time scor plex
-r0:  1463    5   8    2945    5   8
-r1:  2070   11   9    4487   11   9 
-r2:  2272    7  10    5121    7  10
-r3:  3170    8  13    8129    8  13
-r4:  2179   13  13    5586   13  13
-r5:  2054    6   7    4415    6   7
-r6:   884   50   4    3149    7  16
-r7:  1856   50   5    4046   50   5
-r8:  3238   40  13    7191   40  13
-r9:  1513    6   7    3304    6   7

Wow.. the lack of forward progress is stunning.


Idea: If the leading conditions in a cond acheive perfect score on
their target, leave them alone, and stop evolving them.  Instead,
focus on what's left.  This could/should improve convergence
significantly.  .. Doesn't have to be perfect; just better than
some threshold..

Idea two: The scoring function 'enum_table_bscore' used for the above
failed to promote accurate conditional statements.  That is, it gave
exactly the same score to two inequivalent cases: case A, where a 
predicate evals to true, but its consequent is wrong, and case B,
where the predicate evaluates to false, leaving the final determination
to the rest of the cond clauses, which also happen to provide a wrong
answer.  By giving these two the same score, MOSES cannot tell them
apart.  But in fact, case B has the more accurate predicate (even if
its a tad ineffective), and so should be scored higher.  This is
implemented in the 'enum_graded_bscore' function, and measured below.

Below is the measurement of the implementation of bzr rev 6933, on
19 May 2012.  It uses the enum_graded_bscore scorer.  To make it
directly comparable, the "straight score" was also computed via a
hack tacked onto the code.


          -m10K    straight      -m20K    straight     -m40K     straight
:    time  score plex score  time score plex score  time score plex score   
-r0:  129  3.23   11    5     279  3.23  11    5     610  3.23  11    5
-r1:   87  0.95   14    4     301  0.95  14    4     759  0.95  14    4
-r2:  119  4.16   11    6     274  4.16  11    6     605  4.16  11    6
-r3:   99  2.95   15   43     311  1.21  19   43     821  1.12  19   50
-r4:   88  2.06   10    6     225  2.06  10    6     521  2.16   9    6
-r5:   90  3.69   15   43     298  1.51  18   43     787  1.12  19   50
-r6:  139  3.54   12    5     326  3.54  12    5     721  3.54  12    5
-r7:  151 17.28   12   27     360 17.28  12   27     839 17.28  12   27
-r8:   78  3.43   14   50     301  1.33  19   20     829  0.34  18    5
-r9:   92  2.01   10    5     224  2.01  10    5     512  2.01  10    5

Some amount of forward progress is seen.  Biggest problem seems to be
that complexity-ratio works funny on this, since the scoring steps
vary in size.  The complexity should probably be scored using the same
grading ratio, in order to keep things constant... XXX TODO this.

Yeah, that's gotta be it ... a good start, and then lack of progress: 
which means the complexity is too expensive for a given small score step,
so the complexity measure is not allowing the complexity to get larger
for the later terms...

I also suspect some of these have completely ineffective clauses ...
e.g. -r3 !?  ... Hmmm... If these occur early in the cond clause sequence,
they do nothing but drop the score ... but if one day, they mutate to get
a few things right, well, that's mostly harmlesss, yeah?

          -m80K    straight       -m160K   straight               straight
:    time  score plex score  time  score plex score  time score plex score   
-r0: 1329  3.23   11    5    2939  3.23   11    5 
-r1: 1704  0.95   14    4    3657  0.99   13    4
-r2: 1327  4.16   11    6    2907  4.16   11    6
-r3: 2058  1.40   18   50    4340  0.94   16   11
-r4: 1248  2.06   10    6    2745  2.29    8    6
-r5: 2179  0.24   22   11    4846  0.75   17   11
-r6: 1687  3.54   12    5    3557  3.54   12    5 
-r7: 2005 10.72   12   14    3885 10.72   12   14
-r8: 1912  0.37   17    5    3700  1.15   12    5
-r9: 1228  2.01   10    5    2570  2.01   10    5 


OK, so how the hell does occam work? For boolean:
http://groups.google.com/group/opencog/browse_thread/thread/a4771ecf63d38df
OK, a better explanation now in moses/scoring.h

 *     score(M) = - [ LL(M) - |D| log(1-p) ] / log(p/(1-p))
 *              = -|D_ne| + |M|*log|A| / log(p/(1-p))

Note that log(p) is negative, so the total contribution to the score is
negative: more complex models are penalized.  This works, I guess,
because it prevents the algorithm from exploring complex systems, which
I guess are unlikely to evolve into high-scoring ones..

Anyway: the 'occam bias' code in moses/scoring.cc:

    complexity_coef = discrete_complexity_coef(alphabet_size, p);
                    = -log((double)alphabet_size) / log(p/(1-p));

    score += tree_complexity(tr) * complexity_coef

Note: at this time, tree_complexity returns a negative number, so
complex trees are penalized.

Compare to complexity temperature:  in moses/metapopulation.h:

     composite_score::weight = params.complexity_ratio;

and in moses/types.cc:

     // compare weight*score + complexity

Based on this, we have: 

     complexity-ratio = -log(p/(1-p)) / log |A|

as the equivalent to the -z flag for moses-exec.  For p small this is

     complexity-ratio = -log(p) / log|A|

The way I use it/describe this, is "the model has to get N bits more
complex to improve the score by 1 point" where N == complexity_ratio

=============================================================
Regression test: parity performance complexity-ratio vs. occam.

Goal is to verify that the occam code is really doing what we expect it
to do.  Which means that running with with the -z flag and the -p flag
should give the same results, for appropriate values.

Summary: all code was converted over to the occam-style, during
19-23 May 2012.  It all works.  The new random-number generator is
giving me fits.

So:
     complexity-ratio = -log(p/(1-p)) / log |A|    == 3.5

     p/(1-p) = exp (-complexity-ratio log |A| )    == 0.001101937

     p = exp / (1+exp)                             == 0.001100724

Where: alphabet_size = 3+arity for boolean = 7 for parity-4
double-check: log 7 = 1.9459  ln p/1-p = 6.811786852

Need to adjust temperature as well: 
Currently:
      prob = exp (weighted_score *100 / temp);
           = exp ((weight * raw_score + complexity) * 100 / (temp*(weight+1)))
           = exp ((raw_score + complexity/weight) * 100 * weight / (temp*(weight+1)))
           = exp ( occam_score * 100 */ (temp*(weight+1)/weight))

so temp should be 5*4.5/3.5 = 6.428571429


"rech" is bzr rev 6666; its copied from tables above.
       As above, moses-perf -Hpa -k4 -z3.5 -v5 -m350000

"r6935" is bzr rev 6935; not sure why average performance is so much
       worse;  there's no obvious reason for the regression.
       As above, moses-perf -Hpa -k4 -z3.5 -v5 -m350000
       r6666 -- OK
       r6867 -- (pre-rand-gen change) -- about 30% slower, entry for
                entry except for one, which is faster: net: 25% slower
                (what change made things slower??)
                 r0-r19 avg: 116   \
                r20-r39 avg: 109    \
                r40-r59 avg: 117     >--  grand avg = 109 secs
                r60-r79 avg: 111    /
                r80-r99 avg:  94   /

       r6873 -- identical to r6935 -- that confirms that the rand-gen
                is behind things.  Perhaps it's just an outlier?  Again:

                 r0-r19 avg: 146 secs  \
                r20-r39 avg: 111 secs   \
                r40-r59 avg: 121 secs    >-- grand avg = 146 secs
                r60-r79 avg: 180 secs   /
                r80-r99 avg: 174 secs  /
 
                wtf .. so its not an outlier... try disabling the new
                rand gen.. see next...

"deran" is bzr rev 6873, but with the std:mt19337 reverted to ise the
       internal implmentation.  And we seem to get exactly the same
       numbers as r6867 .. which is not surprising.  What is surprising
       is why std::mt19337 is so much more noticably slower!?!?  That
       is... bizarre .. 

"occam" is bzr rev 6969, with the command line:
       moses-perf -Hpa -k4 -m350000 -z3.5 -v6.428571429 -x1
       Curious -- scores are sometimes identical to r6935 and sometimes not.
       Scores kind suck: but its an outlier!?
          r0-r19 avg:  153 secs  \
         r20-r39 avg:  100 secs   \
         r40-r59 avg:  127 secs    >-- grand avg = 150
         r60-r79 avg:  180 secs   /
         r80-r99 avg:  189 secs  /

"unran" is bzr rev 6969, same as "occam" but with the old rand-gen.
       Very nearly identical to "deran", "r6867", differing oddly
       in 2 of 20 entries. Curious.

         r100-r199 avg: 131
         r200-r399 avg: 136.7

       rech  r6935  r6867  deran  occam unran
-r0:     22    305    30     30    137    31
-r1:     58     40    75     73     62    77
-r2:     88    407   114    112    304   117
-r3:    167     25   215    212     25   221
-r4:     43     28    56     57     28    57
-r5:     21     27    27     27     28    28
-r6:    111     29   146            30   150
-r7:    191    232   258           236   406
-r8:    193     42   253            42   154
-r9:     75    360    99           216   102
-r10:    33     79    42           258    43
-r11:   134     66   173           220   178
-r12:    20    151    26           155    27
-r13:   120     23   160            24   164
-r14:   115    132   146           135   151
-r15:    71    357    90           329    93
-r16:    49    136    63           138    65
-r17:   246    136   211           304   216
-r18:   100    172   130           213   135
-r19:     5    179     6           182     6

avg:     93    146   116           153   121

===========================================================
Resume with Iris clustering dataset, May 2012

OK, the new restructured occam-razor code now allows a custom scoring
penalty to be written for the enum_graded_scorer. So will it do better?

The "old-10K" and "old-m160K" columns are the previous bests: graded 
scorer, but ungraded complexity penalty. 

The "new-10K" is one scoring method, clearly dudnt work to well.


       old-m160K   straight     old-m10K   straight     new-m10K  straight
:    time  score plex score  time  score plex score  time score plex score   
-r0: 2939  3.23   11    5     129  3.23   11    5     110  5.37  27    50 
-r1: 3657  0.99   13    4      87  0.95   14    4     106  8.95  30    57
-r2: 2907  4.16   11    6     119  4.16   11    6     172  7.36  10    10 
-r3: 4340  0.94   16   11      99  2.95   15   43     145  5.37  32    50  
-r4: 2745  2.29    8    6      88  2.06   10    6     186  4.61  31    43  
-r5: 4846  0.75   17   11      90  3.69   15   43     102  4.29  25    50
-r6: 3557  3.54   12    5     139  3.54   12    5     190  3.32  15     6
-r7: 3885 10.72   12   14     151 17.28   12   27     161 32.00   9    50
-r8: 3700  1.15   12    5      78  3.43   14   50     157  2.15  28    10
-r9: 2570  2.01   10    5      92  2.01   10    5     119  5.37  26    50


"alt-m10K": new, graded complexity scorer.  Hmm, a bit disappointing out
      of the gate;  will it do better on the longer runs?

OK, what it's doing is inserting ineffective predicates, which lengthens
the cond and thus improves the score simply from the grading effect.
Conclude: should only grade the score if there's been an effective
predicate before-hand.  This way, such chains do get a complexity penalty.

However, inserting an ineffective predicate can lower the overally
complexity when a late predicate has a large complexity B.  Then 

    delta_cmplexity = (2 + 0.8 B) - B

where 0.8 is the graing, and 2 is the complexity of the ineffective pred.
then delta_complexity < 0 when 10 < B.   Yuck. 

Whoa .. maybe we had the grading going the wrong way the entire time?
Try grading=1.2 instead of 0.8, so earlier predicates get rewarded for
getting more things correct, and later predicates get punished.  This
also avoids inneffective predicates, no matter what the complexity
penalty used.

"tro-m10K" uses the retro-grade grading, i.e. grading=1.2 instead of 
       the previous 0.8 used for all earlier measurements.  This
       includes a graded complexity, too.


       alt-m10K    straight     alt-m20K   straight     tro-m10K  straight
:    time  score plex score  time  score plex score  time  score plex score   
-r0:  136   2.40   16   5     368   0.34  20    5      47   8.16   6    6
-r1:   88   5.37   14  50     300   2.75  17   50      46  57.20   7   50 
-r2:  119   4.16   11   6     272   4.16  11    6      51  50.00   5   50
-r3:  105   4.62   13  43     340   1.21  21   43      52  54.60   6   54
-r4:  101   5.37   16  50     289   0.88  15    6      51  61.80   5   61
-r5:  104   4.61   15  43     336   2.36  17   43      34  53.40   5   53
-r6:  136   3.54   11   5     307   3.54  11    5      35  50.00   4   50
-r7:  121   4.16   11   6     276   4.16  11    6      50  50.00   5   50
-r8:   77   3.44   15  50     275   0.35  21    8      54  55.40   6   50
-r9:  122   5.73   17  28     331   3.54  18   33      53  55.4    6   50

"ung-m10K" -- go back to the regular complexity scorer, but keep the
        grading at 1.2. This made no diff.

"pun-m10K" -- go back to grading=0.8 and but use a retrograde complexity
        scorer.  The goal is to get back to using the grading to
        encourage the precision of the predicates, while using the
        penalty to .. I dunno. something... it actually improves on the
        runtime of the old best... one case ran faster, another discovered
        a better scorer.  Not clear if the same thing couldn't have been
        acheived with a different complexity temp and/or complexity ratio.
        But lest run with this and see where it goes.

        Certainly, this scorer could start pushing the complexity penalty
        enough so that its cheaper to have early predicates make mistakes,
        in exchange for less complex later predicates.  Hmm. 

        Lowering the temperature may work too .. making temp proportional
        to min_score_improve.. oh wait ..!? min_score_improv is ??


       tro-m20K    straight     ung-m10K   straight     pun-m10K  straight
:    time  score plex score  time  score plex score  time  score plex score   
-r0:  112   8.16   6    6      49   8.16   6    6      86   3.92   7    5
-r1:  168  52.80  10   52      46  52.40   6   50      86   0.95  14    4
-r2:   99  50.00   5   50      51  50.00   5   50     114   4.16  11    6
-r3:  117  54.60   6   54      52  54.60   6   54     101   5.77  12   43
-r4:  113  61.80   5   61      51  61.8    5   61      86   2.16   9    6
-r5:   76  53.40   5   53      35  53.4    5   53      92   7.21  11   43
-r6:   81  50.00   4   50      42  50.0    4   50     136   3.53  12    5
-r7:  114  50.00   5   50      56  50      5   50     149  17.28  12   27
-r8:  124  55.40   6   50      54  55.4    6   50      79   0.80  14    4
-r9:  131  53.20   8   50      53  55.4    6   50      89   2.25   9    5

main/moses_exec.h:    opt_params.set_min_score_improv(c_scorer.min_improv());
optimization/optimization.h:    inline void set_min_score_improv(score_t s)
        min_score_improvement = s;

Negative values indicate a percent improvement:
         big_step = (best_score >  prev_hi + imp * fabs(prev_hi));

OK, some buginess in the min_score_improv area now fixed.

"msc-m10K": min_score_improv fixes, per bzr rev 6978.  This is using the
       ordinary, non-graded complexity, and the grading=0.8 as before.
       Expect little or no change from baseline ... and there is none.
       Default temp of -v6

"v4-m10K": as above, but -v4 for the complexity temperature.
       One score improved: -r4

"v2-m10K": as above, but -v2 for the complexity temperature.
       One score worsened: -r4

       msc-m10K    straight      v4-m10K   straight      v2-m10K   straight
:    time  score plex score  time  score plex score  time  score plex score   
-r0:  125   3.23  11    5     125   3.23  11    5     119   3.23  11    5
-r1:   91   0.95  14    4      92   0.95  14    4      90   0.95  15    4
-r2:  115   4.16  11    6     117   4.16  11    6     114   4.16  11    6
-r3:  107   2.95  15   43     101   2.95  15   43      96   2.95  15   43
-r4:   93   2.28  10    8      93   2.82   9    5      87   4.62  13   43
-r5:   98   3.36  15   44      92   3.36  15   44      87   3.36  15   44
-r6:  138   3.54  12    5     138   3.54  12    5     136   3.54  12    5
-r7:  148  17.28  12   27     151  17.28  12   27     149  17.28  12   27
-r8:   85   3.44  14   50      88   3.44  14   50      82   3.44  14   50
-r9:   90   2.25   9    5      92   2.25   9    5      90   2.25   9    5


"v4-m40K": as above, but -v4 for the complexity temperature.
       The back-slide in scores suggests that the complexity ratio should
       go down, i.e. the complexity penalty should go up.

"v4-retro": as above, but with retrograde penalty. Disappointing.

"v4-g0.9": as above but with grading=0.9. Much better. This is forward
        progress.


        v4-m40K    straight      v4-retro   straight      v4-g0.9  straight
:    time  score plex score  time  score plex score  time  score plex score   
-r0:  650   3.23  11    5     387   3.92   7    5     540   4.43   9    5
-r1:  685   1.24  12    4     663   1.86  11    4     647   3.09  10    4
-r2:  572   4.16  11    6     587   4.16  11    6     581   5.04  11    6
-r3:  822   1.41  18   50     583   7.21  11   43     455  22.68   6   28
-r4:  501   2.82   9    5     533   7.21  11   43     489   4.83   8    7
-r5:  775   1.15  19   51     506   7.62  11   44     529   4.70   8    6
-r6:  690   3.53  12    5     655   3.53  12    5     686   4.19  12    5
-r7:  806  17.28  12   27     801  17.28  12   27     780  21.87  12   27
-r8:  790   1.41  18   50     478   2.25   9    5     804   2.30  14    4
-r9:  498   2.25   9    5     402   3.52   7    5     426   4.23   7    5


"v6-g0.9": as above, but with temperature -v6 (the default).  Wow. OK,
      that's the best yet, right? So retro-complexity scoring is the best. 

"probe-m10K": as above, but with a fix to disc_probe. and -m10K
       (to recap: grading=0.9 and retrograde complexity penalty.)
       (which are now permanent in the code as of bzr rev 6985)

"probe-m40K": as above, but -m40K

        v6-g0.9    straight    probe-m10K  straight    probe-m40K  straight
:    time  score plex score  time  score plex score  time  score plex score   
-r0:  531   4.43   9    5     96    4.43   9    5     483   4.43   9    5
-r1:  655   2.78  11    4     56   21.52  10   50     629   2.02  14    4
-r2:  590   5.60  10    6     99    5.04  11    6     552   5.04  11    6
-r3:  435  22.70   6   28     75   22.68   7   28     355  22.68   6   28
-r4:  505   4.83   8    7     67    4.47   9    7     431   4.34   9    7
-r5:  559   5.08   8    6     72    4.63  11    8     535   4.10  10    6
-r6:  700   4.18  12    5    112    4.19  12    5     605   4.19  11    5 
-r7:  576   5.04  11    6    113    5.04  11    6     658   5.04  11    6
-r8:  886   1.86  16    4     84   19.37  13   50     786   1.51   8    4
-r9:  415   4.23   7    5     70    3.43   9    5     376   3.81   8    5


"eff-m10K": using the enum_effective_score, which does not weight
       ineffective predicates.  (as before: grading=0.9 and retrograde
       complexity penalty.)  The score are not quite as good as before,
       but wow, the results are much, much nicer, the innefective 
       predicates are now gone!.

"eff-m40K": as above but -m40K.
"eff-m160K": as above but -m160K.  Clearly got stuck.

         eff-m10K  straight      eff-m40K  straight     eff-m160K  straight
:    time  score plex score  time  score plex score  time  score plex score   
-r0:   46   5.05   6    6     250   4.23   8    5    1308   4.23   8    5
-r1:   50  45.87  12   55     380   4.23   9    5    1921   4.23   9    5
-r2:   61  10.3    9   11     340  10.3    8   11    1697  10.3    8   11
-r3:  109   3.90  10    4     546   3.90  10    4    2707   3.90  10    4
-r4:   74  26.64  12   28     535   3.90  10    4    2995   3.90  10    4
-r5:   82  42.78  10   50     343   5.22   6    6    1872   5.22   6    6
-r6:   68  10.3    9   11     402  10.30   9   11    2274  10.3    9   11
-r7:  144   5.5   10    6     670   5.5   10    6    3670   5.5   10    6
-r8:  107   3.81  10    4     496   3.81  10    4    2607   3.81  10    4
-r9:   59   5.05   7    6     292   5.05   7    6    1578   5.05   7    6

Damn. Should have been setting the -Z1 flag to enable cross-over.
How could I have forgotten?  The results are completely different!

BUG FIXES:

Major bug fix in bzr rev 7009: sometimes, during reduction, the 
logical_not in front of a pred was being incorrectly stripped out.
In addition, the cross-over in the metapopulatino was buggy, and
that was fixed in bzr rev 6996.  And knob-probing was wrong, that was
fixed in bzr rev 6985. So that's three fairly serius bug fixes.

The result: some of the ABOVE DATA MAY BE INVALID!
The data below contains these bug fixes.

"cro-m10K": as above but with -Z1 flag.  Instantly better than before.
       Using bzr rev 7009, with notable bug fixes described above.

"cro-m40K": as above, 40K.  One tough case improved!  This is unique,
       quite different than past experience.

"cro-160K": as above, 160K. No improvement, disappointing. Perhaps
       the complexity penalty is too high. Or the temperature is too low.
       Or need to use well-enough algo.

         cro-m10K  straight      cro-m40K  straight     cro-m160K  straight
:    time  score plex score  time  score plex score  time  score plex score   
-r0:   52   7.43   6    8     302   7.43   6    8    1428   7.43   6    8
-r1:   86   5.24  10    6     432   5.24  10    6    2007   5.24  10    6
-r2:   85   9.10   7   10     495   9.10   7   10    2398   9.10   7   10
-r3:  115   3.90  10    4     672   3.90  10    4    3391   3.90  10    4
-r4:  171   8.24  13    9     822   8.24  13    9    4124   8.24  13    9
-r5:   64  43.05   8   50     384  24.29   9   26    2183  24.29   9   26
-r6:   49   9.10   6   10     346   4.00  11    4    3175   4.00  11    4
-r7:  154   5.5   10    6     806   5.5   10    6    4200   5.5   10    6
-r8:   43  43.05   8   50     345  24.29   9   26    2076  24.29   9   26
-r9:   56   5.05   7    6     314   5.05   7    6    1663   5.05   7    6
        avg time==88===       avg time===492===       ag time===2664===

"well-m10K":  The well-enough alone algo. At bzr rev 7025. 
        Not expecting much, but lets give it a whirl.  This includes
        the -Z1 flag.  However, this data includes a bug and a "feature":
        the bug is that best candidates were sorted by complexity, in
        the wrong direction.  The "feature" is that only the best candidates
        were considered; in fact, we should consider all of them.
        These are fixed in bzr rev 7038 and 7040, respectively.

        pfx == number of prefixes found (i.e. perfectly accurate clauses)
        tbl == size of scoring table, after trimming away everything that
               the perfect scorer got right.

"well-m40K":  As above.

              well-m10K     straight              well-m40K     straight  
:    time  pfx tbl  score plex score     time  pfx tbl  score plex score 
-r0:   41   1  100  45     20   50        359   1  100   8.64  27   10
-r1:  224   -   -    3.62  21    4       1304   -   -    3.62  30    4
-r2:  151   -   -    5.60  22    6       1064   -   -    5.6   27    6
-r3:  111   -   -    7.05  21    8        934   -   -    7.05  30    8
-r4:  106   -   -   57.5   17   58        822   -   -    7.05  35    8
-r5:   56   -   -   54.8   13   55        490   -   -   54.8   22   55
-r6:   41   -   -  100     14  100        566   1  100   4.97  30    6
-r7:   99   -   -   50     18   50        644   -   -   50     25   50
-r8:   63   2   71  17.01  27   21        337   2   71  17.01  31   21
-r9:  124   -   -    9.24  20   10        691   -   -    9.24  24   10
           avg time===102====                 avg time===721===


"rewell-m10K": As above, but using all candidates, not just top scorers.
          Remarkably little change.

"rewell-m40K": As above.  A touch better.

            rewell-m10K     straight              well-m40K     straight  
:    time  pfx tbl  score plex score     time  pfx tbl  score plex score 
-r0:   74   1  100  21.87  11   26        643   1  100  21.87  11   26
-r1:  183   -   -    3.81  10    4       1217   -   -    3.81  10    4
-r2:  176   -   -    3.80  17    4       1410   -   -    3.8   16    4
-r3:   80   -   -   49.8    6   55        749   -   -    7.05  10    8
-r4:  117   1  100   6.75   9    8        617   1  100   6.75   8    8
-r5:   60   -   -   54.8    6   55        580   -   -   54.8    6   55
-r6:   49   -   -  100      0  100        997   1  100   5.4   17    6
-r7:  117   -   -   50      5   50        795   -   -   50      5   50
-r8:  101   2   71  17.01  12   21        702   2   71  17.01  12   21
-r9:  139   -   -    9.24   9   10        691   -   -    5.62  11    6
           avg time===110====                 avg time===872===


"newell-m10K":  All new, simplified algorithm as of bzr rev 7045.
       This should be competitive with the regular code, when
       perfectly accurate predicates cannot be found, and superior
       when they can be.  *should be* ....  But seems to be
       struggling.  Why?   The test cases could maybe run a little
       longer, due to the need to evaluate pre-merge candidates. 
       But overall, if no prefixes are dound, results should be 
       identical, yeah?  But they're not...

       Should be similar to cro-m10K above, but its not ...
       Instead it resembles the other well-enough profiles. WTF.
       I realllly don't get it.  Is this a RandGen effect?

"newall-m40K": as above. Could probably improve run-time a lot by
       not evaluating all of the candidates, but only those that
       won't be trimmed away after merging.

            newell-m10K                  newell-m40K
:    time  pfx tbl  plex score     time  pfx tbl  plex score 
-r0:   94   1  100   11   22       1211   1  100   25    8
-r1:  280   -   -     8   50       1611   1  100   19    4
-r2:  163   1  106   12    6       1343   1  106   12    6
-r3:   76   -   -     3   56        509   -   -     3   56
-r4:  104   -   -    10   51       1427   -   -     9   51
-r5:   60   -   -     6   55        598   -   -    14   54
-r6:   50   -   -     0  100       1097   1  106   17    6
-r7:  108   -   -     6   50        768   -   -    14    6
-r8:  154   1  100   21    5       1156   1  100   19    5
-r9:  139   -   -     9   10        647   -   -    18    7
        avg time===123====            avg time===1028===

I give up.  I suspect that the contin canonical form is not very good,
or incomplete.  Or something; its not exploring as widely as it should.

========================================================================
Other datasets:

These should work with PM, as designed:

 * Wine Quality  5K records, 12 features
   http://archive.ics.uci.edu/ml/datasets/Wine+Quality
   Purely numeric, predict rating 1-10

 * DBWorld email bag of words, classify, is it announce, or not?
   4.7K attributes (features) but only 64 records.
   http://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails


These should work with the enum classifier:
 * WDBC Breast Cancer: 569 records, 32 features
   http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
   Linearly seperable, 97% accuracy

 * Yeast  1.5K records, 8 features.
   http://archive.ics.uci.edu/ml/datasets/Yeast
   Need to ignore one column.
   Other systems get 55% accuracy. (per ascii description)

 * Wine
   http://archive.ics.uci.edu/ml/datasets/Wine   178 records. Small...

These require sparse input support:
 * Farm Ads   4K records 55K features
   needs   SVMlight sparse vector format
   http://archive.ics.uci.edu/ml/datasets/Farm+Ads

These require support for equ, for enum inputs:
http://archive.ics.uci.edu/ml/datasets/Adult    49K tabular records
http://archive.ics.uci.edu/ml/datasets/Car+Evaluation  1728 records
http://archive.ics.uci.edu/ml/datasets/Abalone           4K records
http://archive.ics.uci.edu/ml/datasets/Bank+Marketing   45K records

========================================================================

1 June 2012:
magic04.data
 * Precision test (i.e. minimize false positives)
   11 features 19K records.  All numeric inputs, enum output.
   enum is acutally boolean, so remap g(gamma)->T  h(hadron)->F

   http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope

Run as:
moses -i magic04.data -u11  -n sin -n log -n exp -x1 -Z1 
took 270 secs, to get score=-4767  complexity=33

But should be precision, so:
moses -i magic04.data -u11  -n sin -n log -n exp -x1 -Z1 -Hpre --min-rand-input=0.5 --alpha=1.0
took 339 secs, got accuracy of 0.84857


time ./opencog/learning/moses/example-progs/moses-perf -i magic04.data
-u11 -n sin -n log -n exp -x1 -Z1 -Hpre --min-rand-input=0.5 --alpha=1.0 | tee magic-m10K.out; 

"magic-10K": magic dataset, above command line.  i.e. the minimum
        activation is 0.5. (i.e. reject no more than half of true events)


          magic-m10K              magic-m40K
:     time  score   cpxy      time  score   cpxy
-r0:   218  0.7877   13       3182  0.8068   81
-r1:   341  0.8486   20       2265  0.8606   32
-r2:   234  0.8181   14       1965  0.8216   29
-r3:   306  0.7991   34       2552  0.8115   41
-r4:   232  0.7415   16       1684  0.7901   24
-r5:   381  0.8231   31       3011  0.8270   37
-r6:   245  0.7852   22       1862  0.8320   42
-r7:   274  0.7363   25       2217  0.7708   36
-r8:   181  0.6983    7       2547  0.7984   44
-r9:   338  0.7651   25       2354  0.8535   43
      avg time===275====     avg time===2364===


Run multi-threading tests on the Cray.  This machine has 12-core Xeon,
and 24GB RAM.  Will repeat tests for -j1, -j2, etc. through -j16.
This required a bugfix; the below is bzr rev 7068 (the fix was bzr
rev 7065 and 7067)

nohup time ./opencog/learning/moses/example-progs/moses-perf -i magic04.data -u11 -n sin -n log -n exp -x1 -Z1 -Hpre --min-rand-input=0.5 --alpha=1.0 -r0  -m40000 | tee magic-j-r0.out

"threading": multi-threading test.  Captions: "time", raw time in secs.
       "slow": slowdown, percent.  For N threads, this is just 
        100* (N * jN_time - j1_time) / j1_time, i.e. the fraction of
        the runtime spent in a serial bottlneck.  score and cpxy as usual.
        Looks like exactly the same exemplars were found each time, so
        looks like none of the random-number generation stuff sits in
        a thread.

"thead-r1" as above, but with -r1 random seed.  Seems to find same
    exemplars, score as the AMD results above.
    "pfrac" == parallelizable fraction == (1-1/su) / (1-1/N)
         where su == measured speedup == baseline_time / run_time
    "maxsu" == speedup for N=infty processors
            == 1/(1-p)

Oddities: there seems to be a bit of a penalty associated with an odd
number of threads.  There seems to be some strange contention when
num threads excedes num cpus.


              threading                     thread-r1
        time   slow  score   cpxy    time   slow    su  maxsu
-j1:    3077    -    0.8068   81     1845     -      -     -
-j2:    1575   2.36  0.8068   81      938   1.77   1.96   56
-j3:    1071   4.44  0.8068   81      633   2.94   2.91   68
-j4:     811   5.50  0.8068   81      481   4.45   3.83   67
-j5:     650   5.62  0.8068   81      386   4.63   4.78   86
-j6:     542   5.79    "      "       326   6.07   5.66   82
-j7:     466   6.19    "      "       285   8.31   6.46   72
-j8:     413   7.49    "      "       247   7.25   7.46   96
-j9:     386  12.87    "      "       226  10.60   8.14   75
-j10:    337   9.59    "      "       200   8.81   9.19  102
-j11:    367  31.23    "      "       202  20.83   9.10   48
-j12:    295  15.31    "      "       175  14.29  10.50   77
-j13:    433  83.84    "      "       274  93      6.73   13
-j14:    447 103.65    "      "       267 102      6.90   13
-j15:    426 108.85    "      "       252 105      7.30   13
-j16:    424 120.65    "      "       246 114      7.48   13


Note that the Cray is almost the same speed as the 5-year-old AMD
Athalon: -m40K took 3182 on the Ath, and 3077 seconds on the Cray.
Wow!.  The Intel Xeon is ...!?

model name      : Intel(R) Xeon(R) CPU   X5650  @ 2.67GHz
cache size      : 12288 KB
cpu cores       : 6

vs.
model name      : AMD Athlon(tm) 64 X2 Dual Core Processor 6000+
cache size      : 1024 KB
cpu cores       : 2

------
20 July 2012: Redo moses scaling tests, this time with boolean, not
contin learning, via PM interfaces.  Dataset: magic-16-sect.data
19020 rows, 150 boolean input features.

pm moses -d magic-16-sect.data -m m.model -p targ -n2 -e160000 -M"-j12"
record the elapsed time, all other calcs done by hand.

           time1            time2        avg
 -j1:  0:58:31.579784   0:58:50.374713  
 -j2:  0:28:10.411133   0:28:07.570191
 -j3:  0:18:01.457073   0:17:58.062739
 -j4:  0:13:11.109083   0:13:16.838854
 -j5:  0:10:28.439161   0:10:31.166558
 -j6:  0:08:33.932223   0:08:37.904997
 -j7:  0:07:19.550359   0:07:18.878978
 -j8:  0:06:23.496197   0:06:21.677673
 -j9:  0:05:42.928437   0:05:40.982504
-j10:  0:05:09.037658   0:05:03.645573
-j11:  0:04:36.562135   0:04:38.079406
-j12:  0:04:19.169721   0:04:13.499484  256.33
-j12:  0:04:16.516100   0:04:16.119379
-j13:  0:05:25.515190   0:05:16.550722
-j14:  0:05:22.836662   0:05:25.458669
-j15:  0:05:15.643609   0:05:09.794156
-j16:  0:05:06.838044   0:05:06.326885

Same data as above:
Somewhat insane, as speedup is greater than N, which indicates
that the j=1 case is broken somehow. However, four measurements
for -j1 give the same thing, indicating that the current code
is likely to have some significant cache collision problem with
the -j1 case.  This makes it impossible to get a measurement of
the parallelizable fraction, and the true N=infinity speedup.

  N     avg time   speedup
 -j1:  3520.97  
 -j2:  1688.99      2.08
 -j3:  1079.75      3.26
 -j4:   793.97      4.43
 -j5:   629.80      5.59
 -j6:   515.92      6.82
 -j7:   439.21      8.02
 -j8:   382.59      9.20
 -j9:   341.95     10.30
-j10:   306.34     11.49
-j11:   277.32     12.70
-j12:   256.32     13.74
-j13:   321.03     10.97
-j14:   324.15     10.86
-j15:   312.72     11.26
-j16:   306.58     11.48

script scaling.pl to compute averages above.

#! /usr/bin/perl

$base = 0;
$j = 1;
while(<>) {
   ($a, $b, $c) = split;
   ($ha, $ma, $sa) = split (/:/, $b);
   ($hb, $mb, $sb) = split (/:/, $c);
   # print "duude $b $c  $ha $ma $sa \n";

   $avg = $ha*3600 + $ma*60 + $sa;
   $avg += $hb*3600 + $mb*60 + $sb;
   $avg *= 0.5;
   if (0 == $base) { $base = $avg; }

   # speedup
   $su = $base / $avg;

   $pf = 0;
   # paralllizable fraction
   if ($j != 1) {
      $pf = (1 - 1/$su) / (1 - 1/$j);
   }

   # maximum speedup
   $maxsu = 1 / (1-$pf);
   # print "$j  avg=$avg speedup = $su maxspeedup = $maxsu\n";
   print "$j  $avg   $su\n";

   $j++;
}

Again, this time with -r1 random seed:
Again, same problem: greater than unity drive.

           time1            time2        avg
 -j1:  0:55:34.438427   0:57:40.722962
 -j2:  0:28:09.940727   0:28:08.055481
 -j3:  0:18:07.668825   0:17:59.532231
 -j4:  0:13:07.527517   0:13:07.980217
 -j5:  0:10:34.328574   0:10:19.094884
 -j6:  0:08:37.897093   0:08:36.165487
 -j7:  0:07:14.747906   0:07:21.580980
 -j8:  0:06:22.907118   0:06:19.043655
 -j9:  0:05:36.325852   0:05:38.498216
-j10:  0:05:08.184284   0:05:02.946471
-j11:  0:04:39.090906   0:04:38.483967
-j12:  0:04:15.044312   0:04:17.156179   
-j13:  0:05:38.029001   0:05:20.919572
-j14:  0:05:29.199905   0:05:20.543640
-j15:  0:05:07.448976   0:05:12.333979
-j16:  0:05:04.920271   0:05:03.272574


------------------------------
Early June 2012:
Below is a run with escalating m values, to see how/if score improves
with more effort.  Tested with bzr rev 7068.  This is on the Cray, with
12 threads.

 Command line is:
    nohup time ./opencog/learning/moses/example-progs/moses-perf -i magic04.data -u11 -n sin -n log -n exp -x1 -Z1 -Hpre --min-rand-input=0.5 --alpha=1.0 -r1 -j12 | tee magic-j12-m.out

"longrun": As above. Note improvement completely stalls.

"temp-v12": As above, but with higher temperature: -v12  Note an
        immediate improvement in scores.

"v12-popsz": As above, but with much much smaller popsize
        (i.e. bzr rev 7079)  A slight regression in scores, but
        perhaps this is just random chance.  Some minor speed
        improvement, initially.

             longrun               temp-v12            v12-popsz
-m      time  score   cpxy    time  score   cpxy    time  score   cpxy
10K:      28  0.8486   20       26  0.8618   14      25   0.8618   14
20K:      74  0.8492   21       64  0.8618   14      62   0.8618   14
40K:     179  0.8606   32      173  0.8618   14     167   0.8618   14
80K:     404  0.8626   36      437  0.8658   34     351   0.8618   14
160K:    848  0.8626   36      952  0.8659   34     923   0.8624   19
320K:   1873  0.8670   35     2291  0.8738   61    2625   0.8624   19 
640K:   4131  0.8670   35     6260  0.8768   66    7616   0.8643   82 
1.28M:  9088  0.8673   28      
2.56M: 20318  0.8676   38      
5.12M: 47993  0.8676   38      
10.2M:

"v24-newsz": as above but -v24 and also a less cramped posize (bzr rev
           7081) 

"v36-newsz": as above but -v36

"v12-newsz": as above but -v12
         Footprint: 8.5GB/48605 = 175KB/instance at cpxy avg=100 maybe..

            v24-newsz            v36-newsz             v12-newsz
-m      time  score   cpxy    time  score   cpxy    time  score   cpxy
10K:      23  0.8080   19       28  0.7775   28       26  0.8618   14 
20K:      57  0.8136   25       56  0.7775   28       63  0.8618   14
40K:     132  0.8299   29      144  0.8299   34      142  0.8641   23
80K:     343  0.8643   40      373  0.8330   44      333  0.8641   23
160K:    939  0.8659   46      814  0.8437   42      743  0.8641   23
320K:   2342  0.8659   46     2089  0.8437   42     1978  0.8641   23
640K:   5525  0.8703   48     5599  0.8499   75     6599  0.8660   93 
1.28M:                                             24057  0.8816  129
2.56M:                                             63340  0.8900  146


"v12-accu": traditional -Hit accuracy maximization.  On Cray, using:
        moses-perf -i magic04.data -u11 -n sin -n log -n exp -x1 -Z1 -Hit -r1 -j12 -v12 

"bisect": using the bisected data. On localhost.
        moses-perf -imagic-bisect.data -u1 -x1 -Z1 -Hit
        Clearly more than one orders-of-magnitude faster than contin,
        at first, but bogs down.   But coparable complexity... 
        scoring contins takes 12x longer !?  Or are there more
        cands, so merge takes much much longer!?

"bisect-v12": using the bisected data. On localhost.  As above, but v12.


            v12-accu               bisect              bisect-v12
-m      time  score   cpxy    time  score  cpxy    time  score   cpxy
10K:      28   4717    31       18   4083   28       18   4084    27
20K:      80   4421    49       62   4054   37       64   4054    39
40K:     250   4039    91      181   3995   54      188   4018    55
80K:     794   3735   109      573   3980   71      536   3984    68
160K:   2058   3669   119     1789   3974   75     1725   3982    71
320K:   4934   3656   126                          4357   3982    70     
640K:  11001   3652   131                         10546   3965    78
1.28M: 24055   3651   133                         25971   3965    77
2.56M: 54509   3637   147


"trisect-v12": using the trisected data. On localhost.  As above, but v12.
     Tri-sected with -s2 and default -D1
     Note that the trisected data score is beating the contin variant. :-(

"quadsect-v12": using the quadsected data. On localhost.  As above, but v12.
     Quad-sected with -s3 and default -D1   Looks like hill-climbing
     is taking 20-30 steps for each generation.

"hexsect-v12": using the hexsected data. On localhost.  As above, but v12.
     Hex-sected with -s5 and -D0.8 because some data is not normally
     distributed.  Total of 47 features.  Results: Wow. 
     Hill-climber steps: 20, 36, 61, 37, 34


           trisect-v12           quadsect-v12        hexsect-v12
-m      time  score   cpxy   time  score  cpxy    time  score   cpxy
10K:      30   3808    36      54   3701   21      108   3303    25
20K:      79   3748    50     138   3537   36      284   3263    43
40K:     242   3718    60     383   3429   47      804   3209    57
80K:     656   3681    76    1086   3364   76     1865   3079    75 
160K:   1966   3654    95    3338   3313  110     4404   3030   112
320K:   7040   3629   119                        12327   2970   156
640K:                                            39062   2899   205


"hexsect-mmi-C24": as above, but uses feature-selection to trim down:
         feature-selection -imagic-hexsect.data -omagic-hex-mmi-C24.data -utarg -ammi -C24
         this took 85:30 minutes to run!  Ouch!  Note that hexsect-v12 
         above, the highest scorer used all 47 features. All. of. them. 

         Here, we've selected 24 out of the 47.  Note that scores are
         equal, or even better (initially, at least), but that run times
         are halved.

"hexsect-inc-C24": as above, but uses the -ainc algo in feature-selection
         to trim 47 to 24 features.   Feature selection took 35 seconds.
         So that's wayyy faster, but significantly crappier score.

"hex-inc-U2-C24": as above, but used -ainc -U2 for feature selection.
         Well, its a little better, but nothing to write home about.

          hexsect-mmi-C24     hexsect-inc-C24       hex-inc-U2-C24
-m      time  score   cpxy   time  score  cpxy    time  score   cpxy
10K:      49   3314    27      33   3570   25       42   3486    29
20K:     137   3221    40     102   3552   37      106   3441    46
40K:     404   3180    55     250   3509   50      355   3422    62
80K:    1048   3125    86     656   3478   70      898   3393    79
160K:   2919   3057   127    1968   3456   84     2316   3364    99
320K:                        5277   3374  110      
640K:                       16467   3274  145      


"hex-inc-U3-C24": as above, but used -ainc -U3 for feature selection.
         Remarkably crappier than the above.  How can that be?

          hex-inc-U3-C24
-m      time  score   cpxy   time  score  cpxy    time  score   cpxy
10K:      30   3525    27 
20K:      88   3499    40
40K:     239   3470    52  
80K:     608   3441    69
160K:   1715   3410    88
320K:   5454   3254   120   
640K:      

Time to perform feature selection, using -ainc -Unn.  The code is supposed
to be such that the higher the -Unn the better the feature selection.  Why
the heck is -U2 faster!?

-U1:   0m34.902s
-U2:   0m23.298s
-U3:   5m0.509s
-U4:


"16-sect-v12-j12": as above, but with 16 sections, on the cray (-j12)
        The 16 sections done with -s15 -D0.1 Looks like 151 columns
        resulted, including the target column.  Looks like hill-climbing
        is hard at work, taking many steps: 30, 105 
        First  Field set contains 1901 bits,
        Second Field set contains 14412 bits
        Third  Field set contains 61027 bits
        Crashes on the third, I think the stack recursion for 
        vary_n_knobs goes to 61K stack frames deep and croaks.

        After first bug fix (bzr rev 7091), still croaks. Slightly 
        different results: second run goes 126 steps, not 105, and so
        the new exemplar is: 70479 bits  

        second fix: bzr rev 7092, Field set contains 57124 bits,
        optim steps: 113  Uhh, wait .. if its 57124 bits, then
        why does full search take 71698 evals? Ohhh because its an
        estimate!   Fourth Field set contains 141546 bits

"16-sect-C48-j12: On the cray, pick out 48 features out of the 150
        (the hexsect raw had 47 features).  Do this with
          feature-selection -imagic-16-sect.data 
            -omagic-16-sect-mmi-C48.data -utarg -ammi -C48 -j12
        That took 3:47 hours on 12-way! Ouch! 162752 secs total.
        Field set contains 608 bits
        Field set contains 2471 bits
        Field set contains 8007 bits
        Field set contains 17453 bits
        Field set contains 30868 bits
        Field set contains 45600 bits

"16-sect-clamp": same as 16-sect-v12-j12, except that NN exploration
        is clamped to 20K nearest neighbors.  Since the second 
        generation has only 18K neighbors (actual, not estimated),
        the results will be identical until the third gen, which
        kicks in at -m640K ... Well, they're not identical, there'd
        been some screwing around, before. But close enough.

        Afterwards ... seems to be even slightly better.  Is this
        because an exhaustive search is not needed to get good
        cross-over results?  And at 1.28M it holds the all-time record!

       Note clamping is equivalent to --hc-max-nn-evals=20000


         16-sect-v12-j12      16-sect-C48-j12       16-sect-clamp
-m      time  score   cpxy   time  score  cpxy    time  score   cpxy
10K:       9   3706    12       8   3378   19        9   3706    12
20K:      24   3403    26      21   3157   31       24   3403    26
40K:      73   3362    28      58   3069   53       72   3362    28
80K:     176   3158    49     158   2905   70      175   3211    42
160K:    459   3082    69     428   2751  120      454   3098    66
320K:   1157   2789   114    1193   2684  155     1136   2886   103
640K:   3212   2718   136    3404   2534  236     3260   2661   158
1.28M:  7919   2530   197    9316   2434  310    13333   2285   356
2.56M: 21671   2430   268
5.12M: 
10.2M:

"16-sect-nn-10K": as above, but with --hc-fraction-of-nn=0.5 and
        --hc-max-nn-evals=10000   Wow! kickbutt!  Anyway, the frac=0.5
        has little effect, since gen 1 is done by -m22K and gen 2 has 
        18K NN and so is promptly clamped.

        N.B.: These flags were added in bzr rev 6099.

"16-sect-nn-5K": as above, but without the frac, and with 
        --hc-max-nn-evals=5000 It is setting all new word records.
        Note: big jump in time between -m80K and -m160K seems to
        be due to some single-threaded bottleneck in the code;
        it happens in between demes (during either deme close, or
        during knob-building, or somewhere nearby).  Hmm. Parallism
        does not immediately drop to 1x. For a while, it runs at 7x to
        9x, so I guess lock contention?  And then, maybe there is one 
        single very-long-running thread that everything else waits on?

        The single-theaded part is in between -lDEBUG prints
        "After knob" -> "Created proto" takes 1:45  single-threaded.
        Fixed this in representation.cc in bzr commit 7129

"16-sect-nn-2K": as above, but --hc-max-nn-evals=2000  and with the
        single-threaded fix of bzr rev 7129.  The fix won't affect the
        scores, it will affect the run-times.  It is now clear that
        knob-building is now a major bottleneck, as evidenced at the
        huge jump between -m40K and -m80K in this dataset (previously
        seen in the -m80K-m160K jump for -nn=5K)

         16-sect-nn-10K        16-sect-nn-5K        16-sect-nn-2K
-m      time  score   cpxy   time  score  cpxy    time  score   cpxy
10K:      10   3711    23       9   3706   12       10   3721    19
20K:      27   3388    29      24   3403   26       26   3404    28
40K:      81   3311    35      79   3171   38       86   3114    69
80K:     193   3005    58     217   2990   84      489   2757   156
160K:    493   2877   101    1006   2824  149     3063   2453   350
320K:   1736   2662   156    2735   2436  326    35775   2167   607
640K:                       38720   2092  651
1.28M: 13334   2285   356
2.56M:


"16-diverse":  bzr rev 7178 with diversity penalty manually enabled.
    Otherwise, as above, using the 16-sector dataset so that above
    provides a baseline.
    moses-perf -Hit -utarg -i magic-16-sect.data -W1 -x1 -n sin -n log
    -n exp -Z1 -v12 --hc-max-nn-evals=5000 -j12 
    Curious: up to -m320K, there is no diff from "16-sect-nn-5K" except
    that run-times are longer, due to the copying of the metapop.

            16-diverse
-m      time  score   cpxy   time  score  cpxy    time  score   cpxy
10K:       9   3706    12   
20K:      26   3403    26
40K:      87   3171    38 
80K:     238   2990    84
160K:   1058   2824   149
320K:   2920   2436   326
640K:   
1.28M: 
2.56M:


Is ther overfitting?  Lets find out:
../src/pm.py accuracy -f5 -dmagic04.data -ptarg -e80000 -n2 -s15 -M "-Z1 -v12 -j12 --hc-max-nn-evals=5000"
Oddly enough, above creates only 87 features!? wtf?   Ohh, forgot the
-D0.1 flag...  Anyway:

    Train matrix:
    Classifier results in columns, expected results in rows.
    19209   7543
    6421    42907
    Accuracy: 0.816456361725 (62116 correct out of 76080 total)
    ----
    Test matrix:
    Classifier results in columns, expected results in rows.
    4745    1943
    1646    10686
    Accuracy: 0.811303890641 (15431 correct out of 19020 total)
    ----

Clearly, no over-fitting here.  The five folds had these scores &
complexities, again, for 87 binary features, each fold is 4/5ths of
the total dataset.

score cpxy
 2854  56
 2698  55
 2828  70
 2798  65
 2778  68

----------------------------------------------------------
Do it again, this time -e160000 -s16 to get 81 features(!)

score cpxy
 2951  82
 2902  82
 2971  76
 2972  64
 2905  80
The above gives me:

    Train matrix:
    Classifier results in columns, expected results in rows.
    16554	10198
    4506	44822
    Accuracy: 0.806729758149 (61376 correct out of 76080 total)
    ----
    Test matrix:
    Classifier results in columns, expected results in rows.
    4030	2658
    1188	11144
    Accuracy: 0.797791798107 (15174 correct out of 19020 total)
    ----

Pfft.

----------------------------------------------------------
Do it again, this time -e320000 -s15 -D0.1 to get 150 features, this
time making it directly comparable to runs previous.  Wow. Scored below
are record-setting.

score cpxy
 1876  332
 1942  352
 1943  329
 2016  319
 1857  340

These are great scores, better than any seen for this problem.
I assume they're better because only 4/5ths of the data set is being
used.  i.e. these should be 4/5/ths of the prvious best score,
which was 2436 for -m320K -nn5K so yes, these are right.

Train matrix:
Classifier results in columns, expected results in rows.
19644	7108
2717	46611
Accuracy: 0.870859621451 (66255 correct out of 76080 total)
----
Test matrix:
Classifier results in columns, expected results in rows.
4628	2060
962	11370
Accuracy: 0.841114616193 (15998 correct out of 19020 total)

Very little evidence of overtraining in the above.  Clearly
much more could be done.
=======================================================================
=======================================================================
=======================================================================
=======================================================================

Lets give this one a whirl:
 * Yeast  1.5K records, 8 features.
   http://archive.ics.uci.edu/ml/datasets/Yeast
   Need to ignore one column.
   Other systems get 55% accuracy. (per ascii description)


./opencog/learning/moses/main/moses -Hit -Y1 -u10 -i yeast.data -W1 -x1 -n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 

Dataset has 1484 rows.  The authors, in the "yeast.names" file state:
  "Results: 55% for Yeast data with an ad hoc structured probability"
  "model. Also similar accuracy for Binary Decision Tree and Bayesian"
  "Classifier methods applied by the same authors in unpublished results."
The above dates to 1996. Not clear how many 1996-era cpu-hours were
spent on this, to obtain above results.  For us, a 55% accuracy score
is then -668 or better.  We beat this on the 80K evaluation run, with
almost 56% accuracy.  Yahoo!  The 5.12M run is hitting 78% accuracy.

"yeast-nn-5K": As above, viz:  moses-perf -Hit -Y1 -u10 -i yeast.data
         -W1 -x1 -n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000
         This is, of course, using the troubled cond scoring system.
         And the troubled contin-learning system.  Nevertheless, a
         baseline.  Note: this run truncates the first row, mis-interpret
         first row as labels :-(

"yeast-j12-nn-5K": As above, on Cray, -j12, fixed first row labels.

"yeast-diverse": as above, but with a diversity penalty enabled.
         A promising jump, and then it stalls. Hmm. Need to think
         about this one... is this due to the cond structure?  
         Perhaps the penalty is too large?

         yeast-nn-5K         yeast-j12-nn-5K       yeast-diverse
-m      time  score  cpxy   time  score  cpxy    time  score   cpxy
10K:      40   768     9       5   769     9        6   769      9
20K:     160   768     9      16   769     9       22   769      9
40K:     569   713    21      57   713    21       53   656     20
80K:    1920   656    39     178   657    39      175   653     22
160K:                        397   563    20      493   617     33
320K:                       1147   425    42     1354   613     38
640K:                       3310   390    54     3309   606     43
1.28M:                      8831   379    56     7731   604     42
2.56M:                     23594   345    61    17755   603     42
5.12M:                     53584   330    63

=======================================================================
=======================================================================
=======================================================================
=======================================================================
Another dataset: WDBC Wisconsin Diagnostic Breast Cancer

Ignore: column 1
Predict: column 2
30 features, 569 records.
linearly seperable, resulting in 97.5% accuracy (using 10-fold cross-validation)
This translates to scor 569 x 2.5% = 14 or better


wdbc-base:
    run as moses-perf -Hit -Y"id" -u"diag" -i wdbc.data -W1 -x1 -n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 

wdbc-base-j12: as above, but -j12  Clearly, this stalls out above
    160K evals.  I think work on contin and linearization is needed.

wdbc-div-j12: as above, but diversity penalty turned on (by mistake).

wdbc-canon: bzr rev 7242, fix of the build_knobs::rec_canonize
        warning for id::times canonization.  Doing this exhibits
        an explosion of the size of an instance, and a resulting
        slowdown of the search.


           wdbc-base          wdbc-div-j12         wdbc-base-j12
-m      time  score  cpxy   time  score  cpxy    time  score   cpxy
10K:      44    37    13       3    37    13        3    37     13
20K:     217    27    19      13    27    19       11    27     19
40K:     736    27    18      86    25    23       38    27     18
80K:    1562    24    25     506    21    31       88    24     25
160K:   3267    23    28    1411    20    32      224    23     28
320K:                       2732    19    32      667    22     29  
640K:                       4036    17    38     1590    22     29
1.28M:                      6734    17    37     3516    22     29
2.56M:                                           7559    22     29
5.12M:                                          16033    22     29

           wdbc-canon
-m      time  score  cpxy   time  score  cpxy    time  score   cpxy
10K:      44    37    13
20K:     430    27    19
40K:    4002    27    18
80K:    6884    25    21
160K:  24303    25    21
320K:  56074    25    21                    
640K:                       
1.28M:                      
2.56M:       

=======================================================================
=======================================================================
=======================================================================
=======================================================================
Distributed moses:

Initial sniff tests, on the cray.  This version computes behavioural
scores, fixing this would make it go faster.
  moses -Hpa -k5 -j12  -m400000 -r1

Just looking at top, its clear that its only 450% of total cpu, and not
the full 12 threads.  Its blasting along at maybe 1 generation ever few
seconds.  Best score is -6   Divive (user+sys)/real = 4.41 wayyy below
12x.

Actual time:
real	9m59.974s
user	41m56.854s
sys	2m14.725s

moses -Hpa -k5   -m400000 -r1 -j12:10.1.1.1 -j12:10.1.1.2 -j12:10.1.1.3
-j12:10.1.1.4 -j12:10.1.1.5

Applingly slow; there is forward progres, but cpu utilization extremely
low; like maybe 8%.  Crash with  dup() in/out/err failed
Repeating with same -j arguments fails to parse command line.
per cpu utilization, seems like only one remote host is ever
serviced...!?  and best score was -8 so something went very wrong here..
write total of 1013 generation files.

real	18m49.263s
user	1m23.451s
sys	0m29.472s

Above disaster turns out to be due to poorly structured
distributed_moses dispatch loop.  Re-worked the loop by bzr rev 7285
and fixed a few more bugs.  This gives better results (for 5 nodes;
and the best score is better: its -4) 

real	3m40.990s
user	0m39.928s
sys	0m18.466s

So above gives overall speedup of 3x on 5 nodes.  OK, not great.
CPU usage still never clears 25%... Ahh, algo still has a bug in it.
Was sleeping inappropriately.

Anyway, above is non-deterministic, too; second run like above, score=-6 and:
real	6m21.316s
user	2m21.796s
sys	0m38.137s

But then I got lucky: perfect score, and
real	1m18.370s
user	0m33.583s
sys	0m6.160s



After fixing sleep bug (bzr 7287):
... more crashiness....


-------------------------------
Try again: with WDBC dataset:

moses -Hit -Yid -udiag -i /home/linas/src/opencog/build/wdbc.data -W1 -x1 -n
sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 -r1  -j12:10.1.1.1
-j12:10.1.1.2 -j12:10.1.1.3 -j12:10.1.1.4 -j12:10.1.1.5 -m320000

Seems to be averaging: 37% 36% 30% 30% 56% for the five nodes 
(out of 100% not out of 1200%)

real	4m18.260s
user	1m29.876s
sys	0m4.715s

Hmmm best score:  16 complex=26 which is all time best, so far ... and
elapsed time seems to be less than half of the normal -j12 time.

BTW: WDBC dataset has best accuracy of 97%, so 0.03x569 = 17 is the score
to beat.

Below is the 3-node version of above.  Best score is 20 plex=23 so not as
good, I think this is a rand-gen dependence.

real	5m15.666s
user	1m33.363s
sys	0m5.017s


Below is 1-node -j12:10.1.1.1  best score 23 cplxty=25
real	13m57.563s
user	1m28.678s
sys	0m5.590s

-------------
Try again, after bugfixes bzr 7287, get much higher cpu usage, got to
70% for cpu usage: best score 21 complex=20

real	2m1.692s    
user	0m18.434s
sys	0m2.378s

Wait ... above are getting 'Killed by signal 15.' messages,
but seemed to terminate healthy...

Weird: node 'compute-02' (10.1.1.4) crashed immediately after
completion.

Again with only 4 nodes:   20  cpy=28
real	2m18.883s    
user	0m39.331s
sys	0m2.677s

3 nodes:      22 cpy=26
real	3m28.292s
user	0m56.444s
sys	0m3.252s

2 nodes:       19 cpy=26
real	6m24.558s
user	1m30.821s
sys	0m4.699s

1 node:         21 cpxy=25
real	25m33.045s
user	0m33.190s
sys	0m2.614s



=======================================================================
=======================================================================
=======================================================================
=======================================================================
MPI moses.
OpenMPI
mpirun -n 6 --hostfile ~/mpd.hosts moses ...

Try parity:
moses -Hpa -k5 -x1 --mpi=1 -m4000000 -j12
Parity is very un-even in how long it takes to find a solution, so not
a good benchmark; but it does give hint of traffic:

procs   time  score  plex  bytes      bytes/sec
-----   ----  -----  ----  --------   ---------
   6    5m15    0     30   28625555   28.6MB/315 = 91KB/sec = 1Mbit/sec
   6   48m24   -4     24  214074796   214MB/2904 = 74KB/sec


Try again:
mpirun -n 6 --hostfile ~/mpd.hosts ./opencog/learning/moses/main/moses -Hit -Yid -udiag -i ./wdbc.data -W1 -x1 -n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 -r1  -j12 -m320000 --mpi=1

real	7m31.761s      score: -17 cpxy=23
user	60m41.797s
sys	10m35.159s

[2012-08-06 03:11:03:131] [INFO] Stats: 97      333813  31980   -17 23
[2012-08-06 03:11:03:134] [INFO] MPI: bytes sent=13637 bytes received=15533506

Do it again:
real	7m40.788s
user	62m36.163s
sys	10m21.553s

[2012-08-06 03:20:32:821] [INFO] Stats: 94	333137	45704	-20	23
[2012-08-06 03:20:32:824] [INFO] MPI: bytes sent=13338 bytes
received=15352014


Above seem to run with 60-85% cpu utilization across all five nodes.  So
why did it take sooo much longer than the ssl version? !? I'm confused.
Yuck. Two effects: firt, it really did seem to run longer; second,
master proces knew it was done, then waited a *long* time for other 
workers to wrap it up.

time ./opencog/learning/moses/main/moses -Hit -Yid -udiag -i
/home/linas/src/opencog/build/wdbc.data -W1 -x1 -n sin -n log -n exp -Z1
-v12 --hc-max-nn-evals=5000 -r1  -m320000  -j12:10.1.1.1 -j12:10.1.1.2
-j12:10.1.1.3 -j12:10.1.1.4 -j12:10.1.1.5
Segmentation fault

real	5m15.839s
user	0m12.409s
sys	0m1.749s

WTF.... processor utilization seems similar ... run times should be
similar, scores should be similar.  But the earlier runs were much
shorter, and had worse scores.  I think the earlier runs weren't
actually for -m320000 but for something else ... 

non-crash run:
real	8m44.040s       score 19 cpxy=25
user	0m41.264s
sys	0m2.925s

-----------------------------
Tables below are for cluster WDBC dataset.  Measured with bzr rev 7332.

"mpirun -n " + str(nodes) + " --hostfile ~/mpd.hosts
./opencog/learning/moses/main/moses -Hit -Yid -udiag -i ./wdbc.data -W1 -x1
-n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 -r1 -j12 -m320000
--mpi=1 -fmspeed.log"

speedup, "parallelizable fraction" and "max speedup" just as defined
before, but this time for nodes, not cpus.

"gens" is number of generations (aka number of demes expanded)

Wow. Greater than unity on n=3... this is presumably due to the 
poorer score and lower complexity: each instance can be evaluated
more quickly, on average.  So, ideally, we should take averages...
Note that for procs=6..10, there are two processes running on each
node, so they are sucking up any and all spare cycles that a single
process couldn't quite muster.

Anyway, its clear that whatever limit there is to scaling for this
case, its well beyond what is easily measured on this cluster. 
Why? Because this is essentially an "emarassingly parallel" problem:
once we have some reasonable initial metapopulation, the deme expansion
can be run in parallel more or less arbitrarily.

Data rates: typicaly, 18MB to 25MBytes received per run, so max
data rate is 25MB/300 sec < 1MBit/sec 

procs   time  score  plex   gens  speedup   parfrac  maxspeed
-----   ----  -----  ----   ----  -------   -------  --------
  1     3155   19     27      98     -         -        -
  2     1582   20     25     126    1.994     0.9972    357
  3      865   23     23     101    3.647     1.0887      -
  4      817   21     25      96    3.863     0.9882     84
  5      463   20     23      93    6.816     1.0666      -
  6      437   20     23      97    7.213     1.0336      -
  7      415   17     23      92    7.595     1.0130      -
  8      335   17     23      88    9.429     1.0216      -
  9      292   17     23      83   10.793     1.0208      -
 10      452   21     20      85    6.982     0.9520     21

Never-the-less, lets redo the abve results, averaging over 6 runs each.
Each run is with random seeds -r1 through -r6

The baseline, below.  Note the dramatic variation depending on the
random seed.  Hmm, I thought this dataset was more stable than that.

seed    hh:mm:ss         score   cpxy     again ...
----  --------------     -----   ----     --------------
-r1   0:55:06.980233      19       28     0:52:27.173870
-r2   0:39:14.148769      30       13     0:40:19.101815
-r3   0:28:05.324377      23       13     0:27:45.767175
-r4   1:36:40.991209      31       39     1:36:12.468238
-r5   0:42:33.128429      20       18     0:42:21.632165
-r6                       27       17     0:38:32.139468       


Still greater-than-unity speedup. I think this is because the algo
is distributing simpler, smaller work units when there are lots of
nodes, causing the jobs to run faster.  Its picking simpler, smaller
units because, at the time of work-unit distribution, the metapopulation
is smaller.

procs   time  speedup   parfrac  maxspeed
-----   ----  -------   -------  --------
  1     2976     -         -        -
  2     1383   2.152     1.0706     -
  3      776   3.837     1.1090     -
  4      529   5.723     1.1004     -
  5      455   6.545     1.0590     -
  6      416   7.159     1.0324     -
  7      354   8.417     1.0281     -
  8      378   7.883     0.9979    471
  9      332   8.953     0.9993   1512
 10      299   9.943     0.9994   1559

OK. These results are due to what I am measuring: "How long does it
take to perform a fixed number of evaluations of the scoring function?"
If instead, I measured "How long does it take to reach a given score?"
the results would be quite different. So lets try that ...

---------
Tables below are for clusters WDBC dataset.  Measured with bzr rev 7377.
From perf standpoint, results should be identical to the above.  Goal
is to fix score. the -A"-19" does this, and the -m flag uncaps the num evals.

"mpirun -n " + str(nodes) + " --hostfile ~/mpd.hosts
./opencog/learning/moses/main/moses -Hit -Yid -udiag -i ./wdbc.data -W1 -x1
-n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 -r1 -j12 -A"-19" -m555320000
--mpi=1 -fmscore.log"

Use the "scaling.py" script.


procs   time  speedup   parfrac  maxspeed    gens   evals    popsz
-----   ----  -------   -------  --------    ----   ------   -----
  1     2262     -         -        -         75    255295   44223
  2     2694   0.8395    -0.3823   0.7234    206    508788   53001
  3      777   2.9086     0.9843  63.6195    100    339266    7654
  4   >22200                               >1276  >4660566  >88191
  5      285   8.006      1.0939    -         64    244985   36028
  6      289   7.889      1.0479    -         59    235106   34927
  7      282   8.106      1.0227    -         60    237651   27709
  8      546   4.188      0.8700   7.69      125    467584   47955
  9      176  12.961      1.0382    -         52    261719    2774
 10      535   4.270      0.8509   6.71      149    549796   49567
 11      267   8.554      0.9714  34.97       97    386558   24651

Ugh, what a mess. Had to restart after #4, so the numbes skew slightly.

The 4-procs case stalled; it got to a best score of 21 and then never
got any farther (in over 6 hours, and then the gateway reset).

Note: for the six-random-seed test up above, one run took hour-n-half
to reach score of -31 By comparison, most of the other random seeds can
reach that score in a few minutes!  So re-running the above and averaging
with  different random seeds will deliver mostly garbage.

Note: The WDBC dataset uses contin learning.  Thus, the highly-variable
runtime behaviour previously seen for boolean problems is also occuring
for contin problems (even though previously, it seemed like contin didn't
suffer from this??? Somewhere before, I'd seen this, but where ??).

=======================================================================
MPI moses.
OpenMPI - magic gamma ray dataset.

Redo above, but with magic gamma 16-sectored dataset.
magic-16-sect.data  19020 data rows,  150 bloolean input features
measured with bzr rev 7398, which uses an older "greedy feeder"
algorithm that pushes stale exemplars to workers... So this data
is not ideal, but still should be representatitive...

cmd = "mpirun -n " + str(nodes) + " --hostfile ~/mpd.hosts 
/usr/local/bin/moses -Hit -utarg -i ./magic-16-sect.data -W1 -x1
-n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 -r" +
str(rseed) + " -j12 -m360000 --mpi=1 -fhexadeca.log"

Each work unit is about 1/2 hour long, so, on average, each proc
does about 2 work units, and aftr that, we are waiting for the
shutdown to complete.  So the measurements below are never actually
free-running, but rate-limited by the shutdown sequence.  Each work
unit also takes about 1/2 of the total alloted eval cycles...

The 1-hour runs 4,5 had each worker do 2 work units.  The others,
e.g. 3, 7, 10 had some workers do 3 work units.

"gens" is the total number of work units processed.  So avg work
units per run would be gens/procs which is under 2.5 always.


procs   time  speedup   parfrac  maxspeed    score  cpxy   gens
-----   ----  -------   -------  --------    -----  ----   ----
  1     6669    -          -        -         2489   273     5
  2     4970   1.342      0.509    2.04       2440   292     5
  3     5213   1.279      0.327    1.49       2375   353     7
  4     3721   1.792      0.589    2.44       2446   316     9
  5     3883   1.717      0.522    2.10       2416   339    10
  6     4103   1.626      0.462    1.86       2437   294    11
  7     4979   1.339      0.296    1.42       2422   333    12
  8     4233   1.575      0.417    1.72       2418   298    13
  9     4127   1.616      0.429    1.75       2418   298    12
 10     6236   1.070      0.072    1.08       2441   318    15

So, try again:
magic-16-sect-mmi-C48.data selects 48 features out of 150, those with
the highest mmi.  Hopefully, the length of each work unit will be 
considerably smaller. Lets see... OK, typical n_evals per unit is 50K
to 90K for the later units.   (again -m360K for stop condition.)

"tunit" column below is time, in seconds, for the last unit to run.


procs   time  speedup   parfrac  maxspeed    score  cpxy  gens   tunit 
-----   ----  -------   -------  --------    -----  ----  ----   -----
  1     2569    -          -        -         2652   215    9     929
  2     1896    1.355    0.524     1.35       2588   204   11     953
  3     1276    2.104    0.755     4.09       2664   158   15     708
  4     1026    2.502    0.801     5.01       2710   162   16     733
  5      866    2.966    0.829     5.83       2712   158   18
  6     1056    2.431    0.706     3.41       2673   155   19
  7     1267    2.028    0.591     2.45       2664   168   21
  8     1317    1.950    0.557     2.26       2676   158   23
  9      749    3.426    0.797     4.92       2752   152   24
 10      838    3.062    0.748     3.97       2727   150   27


OK, the size of the work-units are sharply increasing in size; evaluation
times are exploding: (now measured with bzr rev 7404):

Legend:
run_secs -- time to run one workload, received from MPI master.
evals -- number of scoring function evaluations
score -- best score obtained on this run.
cpxy -- information-theoretic complexity of highest-scoring tree.
fssz -- field-set size (information-theoretic bits in field set)

cnt   run_secs	 evals    score    cpxy    fssz
---   --------  -----    -----    ----    ----
 1         1     3793    -4305      7      608
 2         8     7756    -3414      11    1016
 3       169    65686    -3028      54    3256
 4       417    75641    -2755     125    9369
 5       944    80281    -2634     213   20767
 6      1899    80161    -2539     282   37757
 7      2116    52881    -2500     317   21087
 8      3498    79801    -2432     379   60220
 9      4985    89201    -2385     420   74484
 10     4282    15841    -2378     426   84479
 11     4072     5121    -2378     425   85170
 12     4339    15481    -2373     429   85005
 13     4416    15601    -2367     435   85835
 14     4479    10241    -2366     435   87638
 15     4622    15601    -2358     446
 16     5040    20601    -2351     455
 17     5097    10361    -2346     462

BTW, iteration 14 is using 17GB ram...
avg eval time is xxx secs per instance yeah?


seed= 1  elapsed wallclock time:  9:53:27.572213
Baseline time:  9:53:27.572213  seconds= 35607.572213

procs   time    speedup   parfrac  maxspeed    score  cpxy  gens 
-----   ----    -------   -------  --------    -----  ----  ----
  1     35607      -         -        -         2366   436   14
  2      5503     6.47       -        -         2512   298   13
  3      2464    14.45       -        -         2551   249   14
  4      2500    14.25       -        -         2540   259   16
  5      1834    19.41       -        -         2572   227   18
  6      1911    18.62       -        -         2659   160   21
  7      1370    25.98       -        -         2680   153   23
  8       979    36.35       -        -         2706   150   24
  9      1352    26.33       -        -         2650   181   27
 10      1232    28.90       -        -         2653   176   29


OK, here is another, this time, to reach the target score of 2500:
cmd = "mpirun -n " + str(nodes) + " --hostfile ~/mpd.hosts /usr/local/bin/moses
-Hit -utarg -i ./magic-16-sect-mmi-C48.data -W1 -x1 -n sin -n log -n exp
-Z1 -v12 --hc-max-nn-evals=5000 -r" + str(rseed) + " -j12
-A\"-2500\" -m1590000 --mpi=1 -fhexfixC48.log"

procs   time    speedup   parfrac  maxspeed    score  cpxy  gens 
-----   ----    -------   -------  --------    -----  ----  ----
  1     5553       -         -        -         2500   318     7
  2     7548     0.736       -        -         2500   312    14
  3     4227     1.314      0.36     1.56       2500   295    17
  4     4349     1.28       0.28     1.41       2498   299    20
  5     3592     1.55       0.44     1.79       2500   268    23
  6     5029     1.10       0.11     1.13       2499   285    31
  7     5006     1.11       0.11     1.13       2499   302    33
  8     4872     1.14       0.14     1.16       2500   310    37
  9     3566     1.56       0.49     1.67       2500   246    38
 10     3847     1.44       0.34     1.51       2499   278    41

Ughh. Looks ugly, too.  Problem is, most workers are working on 
stale data, while newer, break-through results are languishing in
the metapop, waiting to be further explored.   I think the answer is
to abort the current worker threads, and issue a new exemplar, once
there is a definitive break-through available.

The algo will need to be modified to occasionally see what's up...

=============================================================

Sparse file support.
-------------------
The reuters-test-10.data dataset is the smallest of the reuters
datasets, its the top-10 topic test (not training) dataset. 
It has 16935 contin-valued features (and one enum for the target)
and 4853 rows.   The file format itself is sparse, the table is not:
the resulting run takes 5.8GB to run.

moses -Hit -i ./reuters-test-10.data -u1 -W1 -x1 -n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 -r1 -j12 -m3200 

Fix required to build_knobs, disc_probe()
-m32   with probe:  12m6.669s  12m12.922s
-m32   without probe: 5m15.060s
-m3200 without probe: 6m27.400s
-m32K  without probe:

hashing:  207032 disc knobs in 25:36min = 134 knobs/sec ouch!
New code: as above but 0.50 seconds.  3000x faster. Woot!

==============================================================

logical_probe_rec(): the overhead of creating threads can vastly
overwhelm the number of logical knobs to probe... so: discrete knobs:

baseline:
single-threaded: moses -Hpa -k4 -m64000 -r7
real	0m30.856s
user	0m30.831s
sys	0m0.033s

logical_probe_rec(), disable the recursion by setting n_jobs to 1:

twelve-threaded:  moses -Hpa -k4 -m64000 -r7 -j12
real	0m33.739s
user	3m49.950s
sys	0m25.207s

logical_probe_rec(), recurse by setting n_jobs to 12:

twelve-threaded:  moses -Hpa -k4 -m64000 -r7 -j12
real	0m41.545s
user	4m53.910s
sys	0m29.970s

Yikes. Another case, with 3326 knobs is a clear looser, not even
close to the breakpoint...

moses -Hit -i ./reuters-test-10.data -u1 -W1 -x1 -n sin -n log -n exp -Z1 -v12 --hc-max-nn-evals=5000 -r1 -j12 -m3200 
[DEBUG] Representation size after knob building: 4512329
[DEBUG] Total number of field specs: 948360
[DEBUG] Number of disc knobs mapped: 948360

30K:
-2771.60009765625 cond(not(0<($billoin)) usa interest) 
real	6m28.830s
user	22m37.554s
sys	0m35.673s
last add: 6.5 secs

3K:
-2771.60009765625 cond(not(0<($billoin)) usa interest) 
real	10m0.951s
user	59m40.538s
sys	1m3.728s
last add: 13 secs

300K:
real	14m2.479s
user	28m13.708s
sys	0m26.570s
last add: 22 secs

15K:
real	7m11.449s
user	28m34.771s
sys	0m31.451s

==============================================================
Bag-of-words redux:

bzr rev 7609: this code includes deme size management, as well
as limited logical_probe funkiness.

Dataset: ./reuters-test-10.data which has 4852 rows, 16935 columns
Typical run:
 ./opencog/learning/moses/main/moses -Hit -i ./reuters-test-10.data 
    -u1 -W1 -x1 -n sin -n log -n exp -n div -Z1 -v2 -s0
   --hc-max-nn-evals=5000 -j12 -m132000 -freuters-test-10-m132K.log

Note: -s0 to disable caching.

Results: 
--real-time measures with "time"
--memusage by monitoring every 60 seconds
--deme/gen is the deme and generation that it got to.

       real
-m     time    score   cpxy  memusage   deme/gen
----  -----    -----  -----  --------   --------
132K   3917    -1543           ???       1/94   (this one had caching turned on...)
132K   3992    -1543   111    8.3G       1/94
264K   8635    -1505   164    8.4G       1/169
528K   ----    -1497   177    ----       1/190   (started next gen, ran out of memory)

Its crashing because there are too many knobs that get created (many
milions, which over-flows RAM).  Need to limit the number of knobs.

=================================================================
Knob building:
How many knobs get built?
Base knob building is 2xarity from sample_logical_perms()

3-parity: approx 5 x 6(base) = 30  
4-parity: approx 8 x 8(base) = 60
5-parity: aprox 3 x base early on, then 12 x base

This hints at arity-squared, similar hints from bag-of-words...

reuters-small: arity of 1617
1st generation: 430149 knobs = 133 x base (none of these are contin knobs)
                ahh, I think 133 == 4x number of enum targets! see below.
2nd generation: 8080179 knobs = 2500 x base (but these are almost all contin knobs)
                Number of disc knobs mapped: 200503 = 61 x base ...
                Number of contin knobs mapped: 7879676 = 3x arity-squared
Yeah, its arity squared... 

reuters-tiny: arity of 1219
1st generation: 19504 knobs = 8 x base  ahhh two enums!

reuters-tiny: 1259  
1st generation: 30216 knobs = 12x base  ahhh -- three enums!
2nd generation: 1632928 knobs
                disc knobs mapped: 42806 = 17x base
                contin knobs mapped: 1590122 = arity-squared 

OK all the extra knobs are from build_knobs::append_linear_combination()
... which was making polynomials, by accident... stop doing that, and get:

reuters-small: arity of 1617
1st generation: 90552 = 27 x base
2nd generation: exactly same as before... wtf? Ahhh  found it.
rec_canonize() was building a quadratic with arity-squared knobs.
Now fixed to kep it linear. In bzr rev 7633 or 7634 or so.

==============================================================
Bag-of-words redux: bzr rev 7635

bzr rev 7609: this code includes deme size management, as well
as limited logical_probe funkiness.

Dataset: ./reuters-test-10.data which has 4852 rows, 16935 columns
Typical run:
 ./opencog/learning/moses/main/moses -Hit -i ./reuters-test-10.data 
    -u1 -W1 -x1 -n sin -n log -n exp -n div -Z1 -v2 -s0
   --hc-max-nn-evals=5000 -j12 -m2048000 -freuters-test-10-m2048K.log

Note: -s0 to disable caching.

"final gens"      final number of generations explored, for this deme
"total knobs"     total knobs in the representation
"disc knobs"      number of disc knobs in the representation
"contin knobs"    number of contin knobs ...
"final score"     score at end of this deme run, ditto cpxy
"approx memusage" initial->final mem usage RSS, per top.
"deme time":      time to evaluate deme, only  (wall-clock)
"knob-bld time":  seconds to build knobs, only (wall-clock)


      final   tot    disc   contin   final         approx.     deme  knob-bld
deme  gens   knobs   knobs   knobs   score   cpxy  memusage    time    time
----  ----   -----   -----   -----   -----   ----  --------    -----   ----
  1   190   948360  948360     0     -1497   176   6->8->7 GB  9981    320
  2        3725760 3386980  338780                 14->16             2114


Using the old contin_to_raw_idx(), it takes 3.689 seconds per knob,
(for 186325 contin knobs, and 2167671 disc knobs)

Using the new contin_to_raw_idx() code in bzr rev 7642, it takes... 
(ran out of memory)

==============================================================
Bag of words, feature selection

feature-selection -i ./reuters-test-10.data -ainc -C1000 -o reut-reduced.csv -u1 -j10 -F reut-feat.log

Yow, feature selection took over an hour to run.  See below, the runtime
for -ainc feature selection goes as the square of the number of selected
features.

 num     elapsed      elapsed     div by
feats   time (secs)  div by N    N-squared
-----    -------      ------
   3       0.1         33 mS
  14       0.2         14 mS
  63       3.2         51           1 mS
 166      18.2        110          0.66 mS
 428     128          299          0.699 mS
 988     658          666          0.674 mS
2231    3480         1560          0.699 mS

Bug: feature selection is not writing sparse.

./opencog/learning/feature-selection/main/feature-selection -i ./reuters21578-train-10.dat -ainc -C2400 -o reut-reduced.csv -u1 -j12 -F reut-feat-train-10.log

The above requires 13.2GB to run ...
unique features count=27933
row count=13116

   num     elapsed       div by      RAM
  feats   time         N-squared    usage
           (secs)       (usecs)      
  -----    -------      ------      ------
    42         3         2265       13.2 GB
   127        33         2080       13.2 GB
   320       183         1795       13.3 GB
   695       794         1644     
  1500      3685         1637       13.8 GB
  3062     15316         1633       14.7 GB

==============================================================
Again, this time with 

Size: reut-test-10-reduced-1064.csv is 10.3MB

 ./opencog/learning/moses/main/moses -Hit -i ./reut-test-10-reduced-1064.csv
    -u1 -W1 -x1 -n sin -n log -n exp -n div -Z1 -v2 -s0
   --hc-max-nn-evals=5000 -j12 -m2048000 -freut-red-1064.log


[INFO] Inferred arity = 1064

      final   tot    disc   contin   final         approx.     deme  knob-bld
deme  gens   knobs   knobs   knobs   score   cpxy  memusage    time    time
----  ----   -----   -----   -----   -----   ----  --------    -----   ----
  1    76    59584    59584     0    -1504    80     2.2 GB      829      3
  2    85   285213   240441   44772  -1295   125     2.2GB      9898     12
  3    50   423545   363849   59696    608    87     2.2GB     10181     20
  4    25   361815   317043   44772    449    91     2.2GB      4322     17
  5     -  1517327   334066 1183261     -     -     >24GB  OOM kill      19  

Yikes! clearly the field set size must have exploded for the contin knobs !?

reut-train-10-reduced-2443.csv is 64MB
It has 13117 rows total.

   moses -Hit -i ./reut-train-10-reduced-2443.csv 
   -u1 -W1 -x1 -n sin -n log -n exp -n div -Z1 -v2 -s0
   --hc-max-nn-evals=5000 -j12 -m2048000 -freut-red-2443.log

Inferred arity = 2443

"inst size" == size of one instance, in bytes.
"prod MBytes" == above times 10K (since 10K is the max likely number of insts)

       tot    disc    contin     inst   prod   final  final  final   deme  
deme  knobs   knobs    knobs     size  MBytes   gens  score   cpxy   time  
----  -----   -----    -----     ----   ----    ----  -----   ----   ----- 
  1   136808  136808      0      34232   342     209   4748    107   4273
  2   772049  659579   112470   305512  3055     165   3375    187  55021
  3  1070130  898980   171150   438712  4387      80   1566    186  38662
  4  7067679  947843  6119836  7886784  78GB  bonk

looks like its about 10 bits per contin knob, 2 bits per disc knob

TODO: limit number of knobs being created...

Rerun the above, with RAM-limited number of instances (bzr rev 7880)
output to -freut-red-2443-ramcap.log
Arghhh .. mem usage check didn't prevent this bonk!  Problem was,
number of new insances was not limited.

Again, with git version of 25 October 2012:
Again, with  -freut-red-2443-reramcap.log

Cap max mem usage at 80%
Again, with  -freut-red-2443-rrramcap.log

       tot    disc    contin     inst   prod   final  final  final   deme  
deme  knobs   knobs    knobs     size  MBytes   gens  score   cpxy   time  
----  -----   -----    -----     ----   ----    ----  -----   ----   ----- 
  1   136808  136808      0      34232   342     209   4748    108   4303
  2   772049  659579   112470   305512  3055     165   3375    187  53454
  3  1070130  898980   171150   438712  4387      80   1566    186  37322
  4  7062789  947843  6114946  7880672  78GB  bonk

Why is it still bonking???

total=25268977664 = 23.5GB 
    so acceptable frac = 0.5*23.5 = 11.5GB
    so max lim = 0.8*23.5 = 18.8GB
new=39403360000 = 37GB
deme=7880672
cap=12634488832 = 12GB  --> 10947962470 = 10GB --> 9856670924
num_new=1603 --> 1389 --> 1250

Ahh bug: current_number_of_instances == num in deme but not num in meta... 
use getFreeRAM()

       tot    disc    contin     inst   prod   final  final  final   deme  
deme  knobs   knobs    knobs     size  MBytes   gens  score   cpxy   time  
----  -----   -----    -----     ----   ----    ----  -----   ----   ----- 
  1   136808  136808      0      34232   342     209   4748    108   4281
  2   772049  659579   112470   305512  3055     165   3375    187  53500
  3  1070130  898980   171150   438712  4387      80   1566    186  37234
  4  7062789  947843  6114946  7880672  78GB  bonk

34232 * 5001 = 163MB expected usage
20802281472 - 20713652224 = 84MB actual .. wtf ?

pre-sample free ram is 14214971392
num new instances requested be 801
post-sample free ram is 7889281024

Ahh ... RAM usage during scoring explodes:
Serializing the scorer with a lock, get this:
 post-sample free ram is 4736954368
 score pid=0 cnt=1 pre get cand free=4736954368
 score pid=0 cnt=1 post get cand free=74797056
 score pid=0 cnt=1 treesize =338 bytes=24
 score pid=0 cnt=1 post score free=75919360
 score pid=8 cnt=2 pre get cand free=75919360

OK, why did RAM not return back to normal?
Is there a leak ... !? surely not ... valgrind crashes ... I don't get it.

======================================================================

Try to train:
   poses moses -d reut-train-10-reduced-2443.csv -p targ -m model-binary 
      -V -e100000 -s0 -n333
      -M"-j12 --hc-max-nn-evals=5000 -n sin -n log -n exp -n div -Z1 -s1 -d1"

Again: this dataset is 64MBytes 2443 features, 13K rows.  Takes python
maybe 37 seconds and 350MB to read it.

vast majority of time is spent in reduct, which is single-threaded.
Something wrong here ... these expressions should not be so complicated...
logical_subtree_knob .. disc_probe ... reduct ... 
Ahh. that old bugaboo -- should have _skip_disc_probe=true, but it wasn't

argh, bug: second SIGHUP put moses to sleep.... wchan = stext

---------------------------------------------------
Time spent in logical_probe_rec: 

"Created 4886 logical knob subtrees"

first col is elapse time,   1 thread.
4th    col, elapsed time  n=2 threads.
5th    col, elapsed time  n=3 threads.
6th col, skip logical_disc_probe, n=1 threads.
7th col, probe, but reduct effort=1, n=1 threads
      wow .. is reduct effort 1 broken!? It must be ...

  time     subtree sz    time/node    n=2  n=3    n=1    n=1
  (secs)  (num nodes)    (msecs)     time  time  skip    r=1
  ------   ----------    -------     ----  ----  ----    ---
    11          2                      6    6     0.5    10
    33       23233         1.42       30   39     0.5    22
    75       46431         1.62       54   75     0.5    61
   121       69658         1.74      140  115     0.5   117 
   205       84238         2.43      135  201     0.5   541 (!!!!)
            107415                   170  257     0.5   781 (!!!)

-------------------------------------------------------------------
Try again:

   poses moses -d reut-train-10-reduced-2443.csv -p targ -m model-binary 
      -V -e123480000 -s0 -n333
      -M"-j12 --hc-max-nn-evals=10000 --max-time=300"

Data located in reut-2443-NNNs

Evaluate by: poses score -m model-binary-600s -p targ  -d test-10.dat

test-10.dat contains 4853 lines of data

Yikes! 14 hours to build knobs... 

  limit    time    CPU
  (secs)  actual    %
  ------  ------   ---
   300   1:36:04   633
   600   2:23:05   790
  1200  18:10:12   307
  2400


========================================================================
Unit tests, specifically mixedUTest for _skip_disc_probe

time ./opencog/learning/moses/main/moses -Hit -upred -nimpulse -nsin
-nexp -nlog -m150000 -rnnn -z1 -Z1 -i ../tests/learning/moses/predicates-1.3.csv

The -m flag limits execution time to about 2 minutes, single threaded.
Either it gets an answer or it fails.

Without the skip_disc_probe=true (i.e. the old way)
r0 -- 41 s
r1 fail
r2 -- 1m5.492s
r3 fail
r4 fail
r5 fail
r6 fail
r7 -- 56 s
r8 -- 1m59.511s
r9 -- 37 s
r10 fail
r11 -- 21 s
r12 -- 1m30


disc_probe completely disabled:  this really makes mixedUTest
suffer in a bad way -- I guess that proper probing really does
matter for this case ... Hmm, this neeeds to be studied more
r0 - fail
r1 - fail
r2 - fail
r3 - fail
r4 - fail
r5 - fail

r8 -fail
r11 - fail

Again, formally, via moses-perf:
"skip" -- disable disc probe, completely. "steps" -- number of
     iterations. 

"probe" -- disc_probe enabled

-fail- == the metapopulation was exhausted!! (metapop only got about 250
total, and that's it...)

  seed   skip  skip      probe
         time  steps     time
  ----   ----  -----     ----
   0      101   158K       40
   1    -fail-            132
   2      111              63
   3      123           -fail-
   4      153             130
   5    -fail-          -fail-
   6    -fail-            117
   7       60              54
   8      116             125
   9       53              39

So, for the mixedUTest, looks like disc_probe does help, but not by a
whole lot.

==================================================================

sparse-BA2012-10.dat

./opencog/learning/moses/example-progs/moses-perf
-Hit -W1 --hc-max-nn-evals=5000 -n sin -n log -n exp -n div -Z1
-uchange_percent -isparse-BA2012-10.dat --enable-fs=1

arity=6573 rows=864

Below are for -j1, except the last row, for -j12.  But there is
a bug ... -j12 runs only 10% faster than -j1 so wtf ... that's 
impossible... wtf.  I dunno ... conclude that its thrashing the cache
badly, and is completely memory-bus-bound?  Evidence for this: on
athalon, where the core is faster than the Xeon for any task that fits
in cache, evaluating 5K instances on single cpu takes 132 seconds.
On the xeon, whose cache is 12x larger, it takes 42 secs.  Wow!

xeon cache: 12288 KB
athalon cache: 1024 KB

I believe the xeon probably has bigger/wider/faster memory bus (?)

So although the athalon is faster, the xeon beats it by 3x.  This,
coupled with the fact that adding more cores results in a pathetic
speedup indicates that performance is memory-bus bound.

I am guessing that if feature-selection re-arranged the columns of the
CTable according to some feature-selection score, that locality of
reference would improve during scoring, and so evaluation time would
speed up. 
 
         -j1            -j12   speedup
  -m    time    score   time   percent
 ----   ----    -----   ----   -------
  10K     85    -2319     78      8.2%
  20K    175    -2110    154     12.0%
  40K    350    -1897    308     12.0%
  80K   1732    -1187   1281     26.0%
 160K            -235  15048


==================================================================

August 2013
-----------
Take the IrisVirginica.txt data file from the wiki page... this is
the one that attempts to separate the two hard-to-separate classifications.

It should be possible to over-fit this dataset. How hard can this be?
Appearently, its hard for moses...

            -j1            
     -m    time    score  
    ----   ----    ----- 
     10K     18      -4
     40K     90      -3
    160K    570      -3
    640K   5682      -3

