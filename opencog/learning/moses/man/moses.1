.\"                                      Hey, EMACS: -*- nroff -*-
.\" Man page for moses-exec
.\"
.\" Copyright (C) 2011,2012,2014 Linas Vepstas
.\"
.\" First parameter, NAME, should be all caps
.\" Second parameter, SECTION, should be 1-8, maybe w/ subsection
.\" other parameters are allowed: see man(7), man(1)
.pc
.TH MOSES 1 "October 6, 2014" "3.6.10" "OpenCog Learning"
.LO 1
.\" Please adjust this date whenever revising the manpage.
.\"
.\" Some roff macros, for reference:
.\" .nh        disable hyphenation
.\" .hy        enable hyphenation
.\" .ad l      left justify
.\" .ad b      justify to both left and right margins
.\" .nf        disable filling
.\" .fi        enable filling
.\" .br        insert line break
.\" .sp <n>    insert n+1 empty lines
.\" for manpage-specific macros, see man(7)
.SH NAME
moses \- meta-optimizing semantic evolutionary search solver
.SH SYNOPSIS
.\" The help & version command line
.B moses
.RB \-h | \--help
.br
.B moses
.RB \--version
.br
.\" The general command line
.B moses
.RB [ \-H
.IR problem_type ]
.RB [ \-i
.IR input_filename ]
.RB [ \-u
.IR target_feature ]
.RB [ \-\-enable\-fs=1 ]
.RB [ \-Y
.IR ignore_feature ]
.RB [ \-m
.IR max_evals ]
.RB [ \-\-max\-time
.IR seconds ]
.RB [ \-f
.IR logfile ]
.RB [ \-l
.IR loglevel ]
.RB [ \-r
.IR random_seed ]
.RB [ \-\-mpi=1 ]
.RB [ \-n
.IR ignore_operator ]
.RB [ \-N
.IR use_only_operator ]
.RB [ \-o
.IR output_file ]
.RB [ \-\-boost ]
.RB [ \-A
.IR max_score ]
.RB [ \-a
.IR algo ]
.RB [ \-B
.IR knob_effort ]
.RB [ \-b
.IR nsamples ]
.RB [ \-C1 ]
.RB [ \-c
.IR result_count ]
.RB [ \-D
.IR max_dist ]
.RB [ \-d1 ]
.RB [ \-E
.IR reduct_effort ]
.RB [ \-e
.IR exemplar ]
.RB [ \-F ]
.RB [ \-g
.IR max_gens ]
.RB [ \-I0 ]
.RB [ \-j
.IR jobs ]
.RB [ \-k
.IR problem_size ]
.RB [ \-L1 ]
.RB [ \-P
.IR pop_size_ratio ]
.RB [ \-p
.IR probability ]
.RB [ \-\-python=1 ]
.RB [ \-Q
.IR hardness ]
.RB [ \-q
.IR min ]
.RB [ \-R
.IR threshold ]
.RB [ \-S0 ]
.RB [ \-s ]
.IR cache_size ]
.RB [ \-\-score\-weight
.IR column ]
.RB [ \-T1 ]
.RB [ \-t1 ]
.RB [ \-V1 ]
.RB [ \-v
.IR temperature ]
.RB [ \-W1 ]
.RB [ \-w
.IR max ]
.RB [ \-\-well\-enough=1 ]
.RB [ \-x1 ]
.RB [ \-y
.IR combo_program ]
.RB [ \-Z1 ]
.RB [ \-z
.IR complexity_ratio ]
.SH DESCRIPTION
.PP
.\" TeX users may be more comfortable with the \fB<whatever>\fP and
.\" \fI<whatever>\fP escape sequences to invoke bold face and italics,
.\" respectively.
\fBmoses\fP is the command-line wrapper to the MOSES program learning
library. It may be used to solve learning tasks specified with file
inputs, or to run various demonstration problems.  Given an input table
of values, it will perform a regression, generating, as output,
a \fBcombo\fP program that best fits the data. \fBmoses\fP is
multi-threaded, and can be distributed across multiple machines to
improve run-time performance.
.PP
.\" ============================================================
.SH EXAMPLE
As an example, the input file of comma-separated values:

.nf
\& a,b,out
\& 0,0,0
\& 0,1,0
\& 1,0,0
\& 1,1,1
.fi

when run with the flags \fI\-Hit\ \-u\ out\ \-W1\fR will generate the combo
program \fIand($a\ $b)\fR. This program indicates that the third column
can be modelled as the boolean-and of the first two.  The \fI\-u\fR option
specifies that it is the third column that should be modelled, and the
\fI\-W1\fR flag indicates that column labels, rather than column numbers,
should be printed on output.

.PP
.\" ============================================================
.SH OVERVIEW
\fBmoses\fP implements program learning by using a meta-optimization
algorithm. That is, it uses two optimization algorithms, one wrapped inside
the other, to find solutions.  The outer optimization algorithm selects
candidate programs (called \fIexemplars\fP), and then creates similar
programs by taking an exemplar and inserting variables (called
\fIknobs\fP) in selected locations. The inner optimization algorithm
then searches for the best possible 'knob settings', returning a set
of programs (a \fIdeme\fP) that provide the best fit to the data. The
outer optimization algorithm then selects new candidates, and performs
a simplification step, reducing these to a normal form, to create new
exemplars.  The process is then repeated until a perfect score is
reached, a maximum number of cycles is performed, or forward progress
is no longer being made.
.PP
Program reduction is performed at two distinct stages: after inserting new
knobs, and when candidates are selected.  Reduction takes a program, and
transforms it into a new form, the so-called \fIelegant normal form\fP,
which, roughly speaking, expresses the program in the smallest, most
compact form possible.  Reduction is an important step of the algorithm,
as it prevents the evolved programs from turning into 'spaghetti code',
avoiding needless complexity that can cost during program evaluation.
.PP
The operation of \fBmoses\fP can be modified by using a large variety of
options to control different parts of the above meta-algorithm.
The inner optimization step can be done using one of several different
algorithms, including \fIhillclimbing\fP, \fIsimulated annealing\fP,
or \fIunivariate Bayesian optimization\fP.  The amount of effort
spent in the inner loop can be limited, as compared to how frequently
new demes and exemplars are chosen. The total number of demes or
exemplars maintained in the population may be controlled.  The effort
expended on program normalization at each of the two steps can be
controlled.
.PP
In addition to controlling the operation of the algorithm, many
options are used to describe the format of the input data, and to
control the printing of the output.
.PP
\fBmoses\fP also has a 'demo' mode: it has a number of built-in
scoring functions for various 'well-known' optimization problems,
such as parity, disjunction, and multiplex. Each of these typically
presents a different set of challenges to the optimization algorithm,
and are used to illustrate 'hard' optimization problems.  These demo
problems are useful both for learning and understanding the use of
\fBmoses\fP, and also for developers and maintainers of \fBmoses\fP
itself, when testing new features and enhancements.

.PP
.\" ============================================================
.SH COMBO PROGRAMS
This section provides a brief overview of the combo programming
language.  Combo is not meant so much for general use by humans
for practical programming, as it is to fit the needs of program
learning.  As such, it is fairly minimal, while still being expressive
enough so that all common programming constructs are simply represented
in combo.  It is not difficult to convert combo programs into
other languages, or to evaluate them directly.
.PP
In combo, all programs are represented as program trees. That
is, each internal node in the tree is an operation and leaves are
either constants or variables. The set of all of the variables in
a program are taken as the arguments to the program. The program
is evaluated by supplying explicit values for the variables,
and then applying operations until the root node is reached.
.PP
By convention, variables are denoted by a dollar-sign followed by
a number, although they may also be named. Variables are not
explicitly typed; they are only implicitly typed during program
evaluation. Thus, for example, \fIor ($1 $2)\fP is a combo
program that represents the boolean-or of two inputs,
\fI$1\fP and \fI$2\fP.   Built-in constants include \fItrue\fR and
\fIfalse\fR; numbers may be written with or without a decimal point.
.PP
Logical operators include \fIand\fR, \fIor\fR and \fInot\fR.
.PP
Arithmetic operators include +, - * and / for addition, subtraction,
multiplication and division. Additional arithmetic operators
include \fIsin\fR, \fIlog\fR, \fIexp\fR and \fIrand\fR. The \fIrand\fR
operator returns a value in the range from 0.0 to 1.0.  The predicate
operator \fI0<\fR (greater than zero) takes a single continuous-valued
expression, and returns a boolean. The operator \fIimpulse\fR does
the opposite: it takes a single boolean-valued expression, and returns
0.0 or 1.0, depending on whether the argument evaluates to false or true.
.PP
The \fIcond\fR operator takes an alternating sequence of predicates and
values, terminated by an 'else' value.  It evaluates as a chain of
if-then-else statements, so that it returns the value following the first
predicate to evaluate to \fItrue\fR.
.PP
Thus, as an example, the expression \fIand( 0<( +($x 3))  0<($y))\fR
evaluates to \fItrue\fR if \fI$x\fR is greater than -3 and \fI$y\fR
is greater than zero.
.PP
The above operators are built-in; combo may also be extended with
custom-defined operators, although C++ programming and recompilation
is required for this.
.PP
.\" ============================================================
.SH OPTIONS
.PP
Options fall into four classes: those for specifying inputs,
controlling operation, printing output, and running
\fBmoses\fP in demo mode.

.SS "General options"
.TP
.BI \-e\  exemplar
Specify an initial \fIexemplar\fR to use. Each exemplar is written in
combo. May be specified multiple times.
.TP
.B \-h, \-\-help
Print command options, and exit.
.TP
.BI \-j\  num \fR,\ \fB\-\-jobs= num
Allocate \fInum\fR threads for deme optimization.  Using multiple
threads will speed the search on a multi-core machine.
This flag may also be used to specify remote execution; see the section
\fBDistributed processing\fR below.
.TP
.B -\-version
Print program version, and exit.
.PP
.\" ============================================================
.SS "Problem-type options"
\fBmoses\fR is able to handle a variety of different 'problem types',
such as regression, categorization and clustering, as well as a number
of demo problems, such as parity and factorization.  The \fB\-H\fR
option is used to specify the problem type; the demo problem types are
listed in a later section. This section lists the various "table"
problems, where the goal is to train \fBmoses\fR on an input table
of values.

.TP
.BI \-H\  type \fR,\ \fB\-\-problem\-type= type
The
.I type
of problem may be one of:
.TS
tab (@);
l lx.
\fBit\fR@T{
Regression on an input table.  That is, the input table consists of a set
of columns, all but one considered 'inputs', and one is considered an
output.  The goal of regression is to learn a combo program that most
accurately predicts the output.  For boolean-valued and enumerated
outputs, the scoring function simply counts the number of incorrect
answers, and tries to minimize this score.  That is, this scorer
attempts to maximize accuracy (defined below). For contin-valued outputs,
the mean-square variation is minimized.
T}

\fBpre\fR@T{
Regression on an input table, maximizing precision, instead of accuracy
(that is, minimizing the number of false positives, at the risk of
sometimes failing to identify true positives).  Maximization is done
while holding activation constant.  This scorer is ideal for learning
"domain experts", which provide a result only when they are certain
that the result is correct.  So, for a binary classifier, the output
is meant to be interpreted as "yes, certain" or "no, don't know".
An ensemble of such combo expressions is commmonly called a "mixture
of experts".
T}

\fBprerec\fR@T{
Regression on an input table, maximizing precision, while attempting
to maintain the recall (sensitivity) at or above a given level.
T}

\fBrecall\fR@T{
Regression on an input table, maximizing recall (sensitivity) while
attempting to maintain the precision at or above a given level.
This scorer is most commonly used when it is
important to guarantee a certain level of precision, even if it
means rejecting most events. In medicine and physics/radio applications,
recall is exactly the same thing as sensitivity: this option searches
for the most sensitive test while holding to a minimum level of precision.
T}

\fBbep\fR@T{
Regression on an input table, maximizing the arithmetic mean of the
precision and recall, also known as the "break-even point" or BEP.
T}

\fBf_one\fR@T{
Regression on an input table, maximizing the harmonic mean of the
precision and recall, that is, the F_1 score.
T}

\fBselect\fR@T{
Regression on an input table, selecting a range of rows from a 
continuous-valued distribution.  The range of rows selected are
expressed as percentiles. That is, if the input table has N rows,
then rows that are selected will be rows N*lower_percentile through
N*upper_percentile, where the rows are ordered in ascending rank
of the output column.  That is, this scorer will rank (sort) the
rows according to the output column, and then select those only in
the indicated range.

The lower and upper percentile can be sepcified with the \fB\-q\fR
and the \fB\-w\fR options (the \fB\-\-min-rand\-input\fR and
\fB\-\-max\-rand\-input\fR options), and should lie between 0.0 and 1.0.
T}

\fBip\fR@T{
Discovery of "interesting predicates" that select rows from the
input table. The data table is assumed to consist of a number of
boolean-valued input columns, and a contin-valued (floating point)
target column. \fBmoses\fP will learn predicates that select the
most "interesting" subset of the rows in the table.  The values in
the output columns of the selected rows form a probability
distribution (PDF); this PDF is considered to be "interesting"
if it maximizes a linear combination of several different measures
of the the PDF: the Kullback-Leibler divergence, the skewness, and
the standardized Mann-Whitney U statistic.
T}

\fBkl\fR@T{
Regression on an input table, by maximizing the Kullback-Leibler
divergence between the distribution of the outputs.  That is, the
output must still be well-scored, but it is assumed that there are
many possible maxima.  (XXX???) Huh?
T}

\fBann-it\fR@T{
Regression on an input table, using a neural network.  (kind-of-like
a hidden Markov model-ish, kind of. XXX Huh???)
T}
.TE

.PP
For boolean-valued data tables, the scorers make use of the following
generally-accepted, standard defintions:
.TP
.B TP
True-positive: the sum of all rows for which the combo model
predicts T for a data-table row marked T.
.TP
.B FP
False-positive: the sum of all rows for which the combo model
predicts T for a data-table row marked F.
.TP
.B FN
False-negative: the sum of all rows for which the combo model
predicts F for a data-table row marked T.
.TP
.B TN
True-negative: the sum of all rows for which the combo model
predicts F for a data-table row marked F.
.TP
.B accuracy
Defined as the formula (TP + TN) / (TP + TN + FP + FN)
.TP
.B activation
Defined as the formula (TP + FP) / (TP + TN + FP + FN)
.TP
.B precision
Defined as the formula TP / (TP + FP)
.TP
.B recall
Also known as sensitivity, this is
defined as the formula TP / (TP + FN)
.TP
.B F_1
Harmonic mean of precision and recall,
defined as the formula (2 * precision * recall) / (precision + recall)
= 2TP / (2TP + FP + FN)
.TP
.B bep
Break-even point, the arithmetic mean of precision and recall,
defined as the formula (precision + recall) / 2



.PP
.\" ============================================================
.SS "Input specification options"
These options control how input data is specified and interpreted.
In its primary mode of operation, \fBmoses\fR performs regression on a
a table of input data. One column is designated as the target, the
remaining columns are taken as predictors.  The output of regression
is a \fBcombo\fR program that is a function of the predictors,
reproducing the target.
.PP
Input files should consist of ASCII data, separated by commas or
whitespace.  The appearance of \fB# ;\fR or \fB!\fR in the first
column denotes a comment line; this line will be ignored. The first
non-comment row, if it is also non-numeric, is taken to hold column
labels. The target column may be
specified using the \fB\-u\fR option with a column name. The printing of
column names on output is controlled with the \fB\-W1\fR flag.
.TP
.BI \-i\  filename \fR,\ \fB\-\-input\-file= filename
The \fIfilename\fR specifies the input data file. The input table must
be in 'delimiter-separated value' (DSV) format.  Valid separators
are comma (CSV, or comma-separated values), blanks and tabs
(whitespace). Columns correspond to features; there is one sample per
(non-blank) row. Comment characters are hash, bang and semicolon (#!;)
lines starting with a comment are ignored.
The \fB-i\fR flag may be specified multiple times, to indicate multiple
input files. All files must have the same number of columns.
.TP
.BI \-u\  column \fR,\ \fB\-\-target\-feature= column
The \fIcolumn\fR is used as the target feature to fit.  If no column
is specified, then the first column is used.  The \fIcolumn\fR may be
numeric, or it may be a column label.  If it is numeric, it is taken
to be the number of the column, with column 1 being the left-most.
If \fIcolumn\fR begins with an alphabetic character, it is taken to be
a column label.  In this case, the first non-comment row of the
input file must contain column labels.
.TP
.BI \-Y\  column \fR,\ \fB\-\-ignore\-feature= column
The \fIcolumn\fR should be ignored, and not used as input.  Columns
are specified as above.  This option may be used multiple times, to
ignore multiple columns.
.TP
.BI \-\-score\-weight= column
The \fIcolumn\fR is used to weight the score for each row. If this
option is not used, then each row in the table contributes equally
to the evaluation of the accuracy of the learned model. However, if
some rows are more important than others to get right, this option
can be used to indicate those rows. The accuracy of the model will
be weighted by this number, when evaluating the score.  A weight of
zero effectively causes the row to be ignored. A negative weight
enourages the system to learn models that get the row incorrect.
For boolean problems, this is the same as flipping the output
value.  This option can only be used once, and, if used, it should
specify a column containing an integer or floating-point value.
.TP
.BI \-b\  num \fR,\ \fB\-\-nsamples= num
The number of samples to be taken from the input file. Valid values
run between 1 and the number of rows in the data file; other values
are ignored. If this option is absent, then all data rows are used.
If this option is present, then the input table is sampled randomly
to reach this size.
.TP
.BI \-G1\fR,\ \fB\-\-weighted\-accuracy=1
This option is only used for the discretize_contin_bscore (when
--discretize-threshold is used), if enabled, then the score
corresponds to weighted accuracy Useful in case of unbalanced data.

.TP
.BI \-\-balance=1
If the table has discrete output type (like bool or enum), balance the
resulting ctable so all classes have the same weight.

.\" ============================================================
.SS "Algorithm control options"
These options provide overall control over the algorithm execution.
The most important of these, for controlling behavior, are the
\fB-A\fR, \fB\-a\fR, \fB\-m\fR, \fB\-\-max\-time\fR, \fB\-r\fR,
\fB\-v\fR and \fB\-z\fR flags.
.TP
.BI \-a\  algorithm \fR,\ \fB\-\-algo= algorithm
Select the algorithm to apply to a single deme.  This is the algorithm
used in the 'inner loop': given a single exemplar decorated with tunable
\fIknobs\fR, this algorithm searches for the best possible knob settings.
Once these are found (or a timeout, or other termination condition is
reached), control is returned to the outer optimization loop.
Available algorithms include:
.TS
tab (@);
l lx.
\fBhc\fR@T{
Hill-climbing. There are two primary modes of operation; each has
strengths and weaknesses for different problem types.
In the default mode, one begins with an initial collection of
knob settings, called an \fIinstance\fR. The settings of each knob is
then varied, in turn, until one setting is found that most improves
the score. This setting then becomes the new instance, and the
process is repeated, until no further improvement is seen. The
resulting instance is a local maximum; it is returned to
the outer loop.

The alternate mode of operation is triggered by using the
\fB\-L1\fR flag (usually with the \fB\-T1\fR flag). In this
case, as before, all knob settings are explored, one knob at a time.
After finding the one knob that most improves the score, the
algo is done, and the resulting instance is returned to the outer
loop. If no knob settings improved the score, then all possible
settings of two knobs are explored, and then three, etc. until
improvement is found (or the allotted iterations are exceeded).
In this alternate mode, the local hill is \fBnot\fR climbed to
the top; instead, any improvement is immediately handed back to the
outer loop, for another round of exemplar selection and knob-building.
For certain types of problems, including maximally misleading problems,
this can arrive at better solutions, more quickly, than the
traditional hill-climbing algorithm described above.
T}

\fBsa\fR@T{
Simulated annealing.  (Deprecated). The \fB\-D\fR flag controls the size
of the neighborhood that is searched during the early, "high-temperature"
phase.  It has a significant effect on the run-time performance of the
algorithm. Using \fB\-D2\fR or \fB\-D3\fR is likely to provide the best
performance.

The current implementation of this algorithm has numerous faults, making
it unlikely to work well for most problems.
T}

\fBun\fR@T{
Univariate Bayesian dependency.
T}
.TE

.TP
.BI \-A\  score \fR,\ \fB\-\-max\-score= score
Specifies the ideal score for a desired solution; used to terminate
search.  If the maximum number of evaluations has not yet elapsed
(set with the \fB\-m\fR option), and a candidate solution is found
that has at least this score, then search is terminated.
.TP
.BI \-m\  num \fR,\ \fB\-\-max\-evals= num
Perform no more than \fInum\fR evaluations of the scoring function.
Default value is 10000.
.TP
.BI \-\-max\-time=\ secs
Run the optimizer for no longer than \fIsecs\fR seconds.  Note that
timing is polled only in a small number of points in the algorithm;
thus, actual execution time might exceed this limit by a few seconds,
or even many minutes, depending on the problem type.  In particular,
knob-building time is not accounted for right away, and thus problems
with a long knob-building time will exceed this limit.  If using this
option, be sure to set the \fB\-m\fR option to some very large value.
Default value is 42 years.
.TP
.BI \-n\  oper \fR,\ \fB\-\-ignore\-operator= oper
Exclude the operator \fIoper\fP from the program solution.
This option may be used several times.  Currently, \fIoper\fP
may be one of \fBdiv\fP, \fBsin\fP, \fBexp\fP, \fBlog\fP,
\fBimpulse\fP
or a variable \fB#\fP\fIn\fP.
You may need to put variables under double quotes.
This option has the priority over the \-N option.
That is, if an operator is both be included and ignored,
then it is ignored.  This option does not work with ANN.
.TP
.BI \-\-linear\-regression= 1
When attempting to fit continuous-valued features, restrict searches
to linear expressions only; that is, do not use polynomials in the fit.
Specifying this option also automatically disables the use of div,
sin, exp and log.  Note that polynomial regression results in search
spaces that grow combinatorially large in the number of input features;
That is, for N features, a quadratic search will entail O(N^2)
possibilities, a cubic search will explore O(N^3) possibilities, and so
on.  Thus, for any problem with more than dozens or a hundred features,
linear regression is recommended.
.TP
.BI \-r\  seed \fR,\ \fB\-\-random\-seed= seed
Use \fIseed\fR as the seed value for the pseudo-random number generator.
.TP
.BI \-v\  temperature \fR,\ \fB\-\-complexity\-temperature= temperature
Set the "temperature" of the Boltzmann-like distribution used to
select the next exemplar out of the metapopulation. A temperature that
is too high or too low will make it likely that poor exemplars will be
chosen for exploration, thus resulting in excessively long search times.
The recommended temperature depends strongly on the type of problem
being solved.  If it is known that the problem has false maxima, and
that the distance from the top of the false maximum to the saddle
separating the false and true maximum is H, then the recommended
temerature is 30*H.  Here, H is the 'height' or difference in score
from false peak to saddle, the saddle being the highest mountain pass
between the false and true minumum. Varying the temperature by a factor
of 2 or 3 from this value won't affect results much.  Too small a
temperature will typically lead to the system getting trapped at a
local maximum.

The demo parity problem works well with a temperature of 5 whereas
the demo Ant trail problem requies a temperature of 2000.

.TP
.BI \-z\  ratio \fR,\ \fB\-\-complexity\-ratio= ratio
Fix the ratio of score to complexity, to be used as a penalty,
when ranking the metapopulation for fitness. This ratio is meant to
be used to limit the size of the search space, and, when used with
an appropriate temperature, to avoid gettting trapped in local
maxima.

Roughly speaking, the size of the search space increases exponentially
with the complexity of the combo trees being explored: more complex
trees means more of them need to be examined.  However, more complex
trees typically result in higher scores.  If an increase of N bits
in the complexity typically leads to an increase of s points of the
score, then the complexity ratio should be set to about N/s.  In
this way, the exploration of more complex tree is penalized by an
amount roughly comparable to the chance that such complicated trees
actually provide a better solution.

The complexity ratio is used to calculate a scoring penalty; the
penalty lowers the score in proportion to the solution complexity;
specifically, the penalty is set to the complexity divided by the
complexity ratio.

Setting the ratio too low causes the algorithm to ignore the more
complex solutions, ranking them in a way so that they are not much
explored. Thus, the algorithm may get trapped examining only the
simplest solutions, which are probably inappropriate.

Setting this ratio too high will prevent a good solution from being
found.  In such cases, the algorithm will spend too much time
evaluating overly-complex solutions, blithly leaving simpler, better
solutions unexplored.

The relationship between the score change and the complexity change
is very strongly data-dependent, and must (currently) be manually
determined (although it might be possible to measure it automatically).
Input data tables with lots of almost-duplicate data may have very low
ratios; complex problems with sparse data may have very high ratios.
Initial recommended values would be in the range from 1 to 5;
with 3.5 as the default.  The parity demo problem works well with
the 3.5 default, the Ant trail demo problem works well with 0.16.

.TP
.BI \-Z1\fR,\ \fB\-\-hc\-crossover=1
Controls hill-climbing algorithm behavior.  If false (the default),
then the entire local neighborhood of the current center instance is
explored. The highest-scoring instance is then chosen as the new center
instance, and the process is repeated.  For many datasets, however,
the highest-scoring instances tend to cluster together, and so an
exhaustive search may not be required. When this option is specified,
a handful of the highest-scoring instances are crossed-over (in the
genetic sense of cross-over) to create new instances.  Only these are
evaluated for fitness; the exhaustive search step is skipped.  For many
problem types, especially those with large neighborhoods (i.e. those with
high program complexity), this can lead to an order-of-magnitude
speedup, or more.  For other problem types, especially those with
deceptive scoring functions, this can hurt performance.

.TP
.BI \-\-hc\-crossover\-min\-neighbors= num
If crossover is enabled, and an instance is smaller than this size,
then an exhaustive search of the neighborhood will be performed.
Otherwise, the search will be limited to a cross-over of the highest
scoring instances.  Exhaustive searches are more accurate, but can
take much longer, when instances are large.  Recommended values for
this option are 100 to 500. Default is 400.

.TP
.BI \-\-hc\-crossover\-pop\-size= num
If crossover is enabled, this controls the number of cross-ever
instances that are created.  A few hundred is normally sufficient
to locate contain the actual maximum. Default value is 120.

.TP
.BI \-\-boost=1
Enables the use of boosting. Currently, only the AdaBoost algorithm
is implemented.  If boosting is enabled, then the system focuses on
learning combo programs that correctly classify those values that
were mis-classified in the previous round of learning.  The output
is a set of weighted combo trees.

At this time, boosting can only be used to learn binary classifiers:
that is, to learn problems in which the scorer can provide a yes/no
answer.

.PP
.\" ============================================================
.SS "Boosted dynamic feature selection"
Problems with a large number of input features (typically, hundreds
or more) can lead to excessively long run-times, and overwhelming
amounts of memory usage.  Such problems can be tamed in two different
ways: by static feature pre-selection (i.e. by giving \fBmoses\fR
fewer features to work with) or by reducing the number of features
considered during the knob-building stage.

Recall that, as \fBmoses\fR runs, it iterates over two loops: an outer
loop where an exemplar is selected and decorated with knobs, and an
inner loop, where the knobs are 'turned', searching for the best
knob-settings.  Each knob corresponds to one feature added to a
particular location in the exemplar tree.  A single, given feature
will then be used to build a number of different knobs, each in
a different place in the tree.  As trees get larger, there are more
places in each tree that can be decorated with knobs; thus, the
number of knobs, and the search space, increases over time.
If there are \fBM\fR places in the tree that can be decorated,
and there are \fBN\fR features, then \fBM*N\fR knobs will be created.
The search space is exponential in the number of knobs; so, e.g. for
boolean problems, three knob settings are explored: present, absent
and negated, leading to a search space of \fB3^(M*N)\fR.  Worse,
the resulting large trees also take longer to simplify and evaluate.
Clearly, limiting the number of knobs created can be a good strategy.

This can be done with dynamic, boosted feature selection.  When enabled,
a feature-selection step is performed, to find those features that are
most likely to improve on the examplar.  These are the features that
will be strongly (anti-)correlated with incorrect answers from the
currently-selected exemplar.  By using only these features during
knob building, the total number of knobs can be sharply decreased.
The resulting size ofthe decorated tree is smaller, and the total
search space is much smaller.

Dynamic, boosted feature selection differs from static feature
pre-selection in many important ways.  In dynamic selection, the total
number of features available to moses is not reduced: just because
a feature was not used in a given round of knob-decoration does not
mean that it won't be used next time.  Dynamic feature selection also
resembles boosting, in that it focuses on fixing the wrong answers
of the current exemplar.  By contrast, static pre-selection permanently
discards features, making them unavailable to moses; it is also unable
to predict which features are the most likely to be useful during
iteration.

Boosted, dynamic, feature selection is enabled with the
\fB\-\-enable\-fs=1\fR option.  The number
of features to use during knob building is specified using the
\fB\-\-fs\-target\-size\fR option.  A number of additional flags
control the behaviour of the feature selection algorithm; these are
best left alone; the defaults should be adequate for almost all
problems.  The man page for the \fBfeature\-selection\fR command
describes these in greater detail.

.TP
.BI \-\-enable\-fs=1
Enable integrated feature selection.  Feature selection is disabled
by default.
.TP
.BI \-\-fs\-target\-size= num
Select \fInum\fR features for use.  This argument is mandatory if
feature selection is enabled.
.TP
.BI \-\-fs\-algo
Choose the feature-selection algorithm.  Possible choices are
\fBsimple\fR, \fBinc\fR, \fBsmd\fR, \fBhc\fR and \fBrandom\fR.
The default value is \fBsimple\fR, which is the fastest and
best algo.
The \fBsimple\fR algorithm computes the pair-wise mutual information
(MI) between the target and each feature, and then selects the list
of \fInum\fR features with the highest MI.  It is strongly recommended
that this algo be used.
The \fBsmd\fR algorithm implements "stochastic mutual dependency",
by computing the MI between a candidate featureset and the target.
Starting with an initially empty featureset, features are randomly
added one-by-one, with the MI then computed.  Only the featureset
with the highest MI is kept; the process is then repeated until the
featureset has the desired number of features in it, or the MI has
stopped increasing.  Note that the \fBsmd\fR algorithm effectively
prevents redundant features from being added to the featureset.
Note that \fBsmd\fR runs orders of magnitude more slowly than
\fBsimple\fR, and probably does not provide better results.
The \fBinc\fR is similar to the \fBsmd\fR algo, except that it adds
many features, all at once, to the featureset, and then attempts to
find the redundant features in the featureset, removing those. This
is iteratively repeated until the desired number of features is
obtained.  Note that \fBinc\fR runs orders of magnitude more slowly
than \fBsimple\fR, and probably does not provide better results.
The \fBhc\fR algorithm runs moses hill-climbing to discover those
features most likely to appear.
The \fBrandom\fR algorithm selects features randomly.  It is useful
only for limiting the numberof knobs created, but not otherwise
slantnig the choice of features.
.TP
.BI \-\-fs\-threshold= num
Set the minimum threshold for selecting a feature.
.TP
.BI \-\-fs\-inc\-redundant\-intensity= fraction
When using the \fBinc\fR algorithm, set the threshold to reject
redundant features.
.TP
.BI \-\-fs\-inc\-target\-size\-epsilon= tolerance
When using the \fBinc\fR algorithm, set the smallest step size
used.
.TP
.BI \-\-fs\-inc\-interaction\-terms= num_terms
When using the \fBinc\fR algorithm, set the number of terms used
when computing the joint entropy.
.TP
.BI \-\-fs\-hc\-max\-score
TODO write description
.TP
.BI \-\-fs\-hc\-confidence\-penalty\-intensity
TODO write description
.TP
.BI \-\-fs\-hc\-max\-evals
TODO write description
.TP
.BI \-\-fs\-hc\-fraction\-of\-remaining
TODO write description

.PP
.\" ============================================================
.SS "Large problem parameters"
Problems with a large number of features (100 and above) often
evolve exemplars with a complexity of 100 or more, which in turn
may have instances with hundreds of thousands of knobs, and thus,
hundreds of thouands of nearest neighbors.
Exploring one nearest neighbor requires one evaluation of the
scoring function, and so an exhaustive search can be prohibitive.
A partial search can often work quite well, especially when
cross-over is enabled.  The following flags control such partial
searches.

Note, however, that usually one can obtain better results by
using dynamic feature selection, instead of using the options
below to limit the search.  The reason for this is that the
options below cause the search to be limited in a random
fashion: the knobs to turn are selected randomly, with a
uniform distribution, without any guidance as to whether they
might mae a difference.  By contrast, dynamic feature selection
also limits the search space, by creating knobs only for
those features most likely to make a difference.  Unless the
scoring function is particularly decietful, limiting the
search to the likely directions should perform better than
limiting it to a random subset.

.TP
.BI \-\-hc\-max\-nn\-evals= num
Controls hill-climbing algorithm behavior.  When exploring the
nearest neighborhood of an instance, \fInum\fP specifies
the maximum number of nearest neighbors to explore.  An
exhaustive search of the nearest neighborhood is performed
when the number of nearest neighbors is less than this value.
.TP
.BI \-\-hc\-fraction\-of\-nn= frac
Controls hill-climbing algorithm behavior.   When exploring the
nearest neighborhood of an instance,  \fIfrac\fP specifies
the fraction of nearest neighborhood to explore.  As currently
implemented, only an estimate of the nearest-neighborhood size
is used, not the true size.  However, this estimate is accurate
to within a factor of 2.  Thus, to obtain an exhaustive search
of the entire neighborhood, set this to 2.0 or larger.

.PP
.\" ============================================================
.SS "Algorithm tuning options"
These options allow the operation of the algorithm to be fine-tuned
for specific applications.  These are "advanced" options; changing
these from the default is likely to worsen algorithm behavior in
all but certain special cases.
.TP
.BI \-B\  effort \fR,\ \fB\-\-reduct\-knob\-building\-effort= effort
Effort allocated for reduction during the knob-building stage.
Valid values are in the range 0-3, with 0 standing for minimum effort,
and 3 for maximum effort. Larger efforts result in demes with fewer
knobs, thus lowering the overall dimension of the problem. This can
improve performance by effectively reducing the size of the problem.
The default \fIeffort\fR is 2.
.TP
.BI \-D dist \fR,\ \fB\-\-max\-dist= dist
The maximum radius of the neighborhood around the exemplar to explore.
The default value is 4.
.TP
.BI \-d1\fR,\ \fB\-\-reduce\-all=1
Reduce candidates before scoring evaluation. Otherwise, only dominating
candidates are reduced, just before being added to the metapopulation.
This flag may be useful if scoring function evaluation expense depends
strongly one the structure of the candidate. It is particularly important
to specify this flag when memoization is enabled (with \fB-s1\fR).
.TP
.BI \-E\  effort \fR,\ \fB\-\-reduct\-candidate\-effort= effort
Effort allocated for reduction of candidates. Valid values are
in the range 0-3, with 0 standing for minimum effort, and 3
for maximum effort. For certain very symmetric problems, such
as the disjunct problem, greater reduction can lead to significantly
faster solution-finding.  The default \fIeffort\fR is 2.
.TP
.BI \-g\  num \fR,\ \fB\-\-max\-gens= num
Create and optimize no more than \fInum\fR demes.  Negative numbers
are interpreted as "unlimited". By default, the number of demes is
unlimited.
.TP
.BI \fB\-\-discard\-dominated=1
If specified, the "dominated" members of the metapopulation will be
discarded.  A member of the metapopulation is "dominated" when some
existing member of the metapopulation scores better on *every* sample
in the scoring dataset. Naively, one might think that an individual that
does worse, in every possible way, is useless, and can be safely
thrown away.  It turns out that this is a bad assumption; dominated
individuals, when selected for deme expansion, often have far fitter
off-spring than the off-spring of the top-scoring (dominating) members
of the metapopulation. Thus, the "weak", dominated members of the
metapopulation are important for ensuring the vitality of the
metapopulation as a whole, and are discarded only at considerable
risk to the future adaptability of the overall population.  Put another
way: specifying this flag makes it more likely that the metapopulation
will get trapped in a non-optimal local maximum.

Note that the algorithms to compute domination are quite slow,
and so keeping dominated individuals has a double benefit: not only
is the metapopulation healthier, but metapopulation management
runs faster.

.TP
.BI \-L1\fR,\ \fB\-\-hc\-single\-step=1
Single-step, instead of hill-climbing to the top of a hill. That is,
a single uphill step is taken, and the resulting best demes are folded
back into the metapopulation.  Solving then continues as usual. By
default, the hill-climbing algorithm does not single-step; it instead
continues to the top of the local hill, before folding the resulting
demes back into the metapopulation.  If using this flag, consider
using the \fB\-T1\fR flag to allow the search to be widened, so that
if the initial exemplar is already at the top of a local hill, a search
is made for a different (taller) hill.
.TP
.BI \-N\  oper \fR,\ \fB\-\-include\-only\-operator= oper
Include the operator \fIoper\fP, but exclude others, in the solution.
This option may be used several times to specify multiple
operators.  Currently, \fIoper\fP may be one of
\fBplus\fP, \fBtimes\fP, \fBdiv\fP, \fBsin\fP,
\fBexp\fP, \fBlog\fP, \fBimpulse\fP
or a variable \fB#\fP\fIn\fP.
Note that variables and operators are treated separately, so
that including only some operators will still include all
variables, and including only some variables still include
all operators).  You may need to put variables under double
quotes.  This option does not work with ANN.
.TP
.BI \-P\  num \fR,\ \fB\-\-pop\-size\-ratio= num
Controls amount of time spent on a deme. Default value is 20.
.TP
.BI \-p\  fraction \fR,\ \fB\-\-noise= fraction
This option provides an alternative means of setting the complexity
ratio.  If specified, it over-rides the \fB\-z\fR option.  For
discrete problems, \fIfraction\fR can be interpreted as being
the fraction of score values that are incorrect (e.g. due to
noisy data).  As such, only values in the range 0 < \fIfraction\fR
< 0.5 are meaningful (i.e. less than half of the data values are
incorrect).  Typical recommended values are in the range of 0.001
to 0.05.  For continuous-valued problems, it can be interpreted
as the standard deviation of a Gaussian noise in the dependent
variable.

For the discrete problem, the complexity ratio is related to the
\fIfraction\fR  p by the explicit formula:

    complexity_ratio = - log(p/(1-p)) / log |A|

where |A| is the (problem-dependent) alphabet size.  See below for a
detailed explanation.

.TP
.BI \-T1\fR,\ \fB\-\-hc\-widen\-search=1
Controls hill-climbing algorithm behavior.  If false (the default),
then deme search terminates when a local hilltop is found. If true,
then the search radius is progressively widened, until another
termination condition is met.  Consider using the \fB\-D\fR flag to
set the maximum search radius.  Note that the size of the search
space increases exponentially with the search radius, so this kind
of search becomes very rapidly intractible.

.TP
.BI \-\-well\-enough=1
For problems with an enumerated ('nominal') output, the learned combo
program is always of the form \fBcond\fR\fI(pred_1 value_1 pred_2 value_2 ...
pred_n value_n else_val)\fR  where \fIpred_1\fR is a predicate, which,
if true, causes the output to be \fIvalue_1\fR.  If false, then
\fIpred_2\fR is tested, and so on.  If none of the predicates evaluate to
true, then the value of the \fBcond\fR expression is the \fIelse_val\fR.
The well-enough algorithm attempts to find predicates that maximize
precision, the point being that if a perfectly precise \fIpred_1\fR
can be found, then it can be left alone ('leave well-enough alone'),
thus simplifying the remainder of the search problem.  Performing this
evaluation is costly, and may lead to a slow-down, without improving
overall accuracy.

.\" ============================================================
.SS "Controlling RAM usage"
Deploying \fBmoses\fR on large problems can cause all available RAM on a
given machine to be used up.  Actual RAM usage depends strongly on
the dataset, and on various tuning paramters. Typical tasks require
systems with dozens or hundreds of gigabytes of RAM.

There are three primary consumers of RAM within \fBmoses\fR: the
metapopulation (the set of combo trees being evolved), the deme
(the set of instances describing the knob setttings, when a single
combo tree is being explored), and the score casche.  The following
options can be used to limit the size of each of these to a more
managable size.

.TP
.BI \-s\ num \fR,\ \fB\-\-cache\-size= num
Enable memoization of candidate scores.  This allows the number of scoring
function evaluations to be reduced, by maintaining a cache of recently
scored candidates. If a new candidate is found in the cache, that score
is used, instead of a scoring function evaluation.  The effectiveness of
memoization is greatly increased by also using the \fB\-d1\fR flag.
The maximum number of cached scores is set at \fBnum\fR, which defaults
to 3000. Careful: scores can be quite large, and use a lot of memory.
.TP
.BI \-\-cap\-coef= num
A coefficient used to control the size of the metapopulation. The 
metapopulation is trimmed, by removing a random subset of the lowest
scorers, so that the maximum size is given by the formula

    pop-size = cap_coef * (N + 250) * (1 + 2 * exp(-N/500))

where \fBN\fR is the number of metapop iterations (tree expansions)
so far.  The default value for this option is 50.

Note that the prefered way of keeping the metapopulation small is to
set a low temperature (the \fB\-\-complexity\-temperature\fR option).
For a given temperature, trees which have such a poor score that they
have a probability of less than one in a billion of being selected are
automatically discarded from the metapopulation.  In most cases, this
is enough to keep the metapopulation under control. However, if there
are a vast number of trees with almost exactly the same score, the 
temperature alone will not be enough to control the metapopulation
size.

If the metapopulation is growing out of control, then do verify that
the temerpature is set to an appropriatte value, before making use of
this flag.
.TP
.BI \-\-hc\-resize\-to\-fit\-ram=1
If set to true, this will cause \fBmoses\fR to try to limit memory
usage so that it will fit in available RAM (and thus avoid an OOM kill).
The RAM usage is controlled by automatically resizing the deme during
the hillclimbing search. (The deme contains instances of knob settings
that were explored).  Note that the use of this option may introduce
indeterminacy, when comparing runs from different machines, having
different amounts of installed RAM.


.\" ============================================================
.SS "Output control options"
These options control the displayed output.  Note that, be default, the
ten best solutions are printed. These are printed in penalized-score-sorted
order, but the actual printed score is the raw, un-penalized score. This
can lead to the printed list seeming to be in random order, which can
occur if the penalties are high.  The penalties can be too high, if the
complexity ratio is set too low, or the temperature is set too low.

.TP
.BI \-C1\fR,\ \fB\-\-output\-only\-best=1
Print only the highest-scoring candidates.
.TP
.BI \-c\  count \fR,\ \fB\-\-result\-count= count
The number of results to return, ordered according
to score. If negative, then all results are returned.
.TP
.BI \-f\  filename \fR,\ \fB\-\-log\-file= filename
Write debug log traces \fIfilename\fR. If not specified, traces
are written to \fBmoses.log\fR.
.TP
.BI \-F\fR,\ \fB\-\-log\-file\-dep\-opt
Write debug log traces to a filename constructed from the passed
option flags and values. The filename will be truncated to a maximum
of 255 characters.
.TP
.BI \-l\  loglevel \fR,\ \fB\-\-log\-level= loglevel
Specify the level of detail for debug logging. Possible
values for \fIloglevel\fR are \fBNONE\fR, \fBERROR\fR, \fBWARN\fR,
\fBINFO\fR, \fBDEBUG\fR, and \fBFINE\fR. Case does not matter.
Caution: excessive logging detail can lead to significant
program slowdown.  The \fBNONE\fR option disables log file creation.
This may make error debugging difficult.
.TP
.BI \-o\  filename \fR,\ \fB\-\-output\-file= filename
Write results to \fIfilename\fR. If not specified, results are written to
\fBstdout\fR.
.TP
.BI \-\-python=1
Output the highest-scoring programs as python snippets, instead of combo.
.TP
.BI \-S0\fR,\ \fB\-\-output\-score=0
Prevent printing of the score.
.TP
.BI \-t1\fR,\ \fB\-\-output\-bscore=1
Print the behavioral score.
.TP
.BI \-V1\fR,\ \fB\-\-output\-eval\-number=1
Print the number of evaluations performed.
.TP
.BI \-W1\fR,\ \fB\-\-output\-with\-labels=1
Use named labels instead of position place-holders when printing
candidates. For example, *("$temperature" "$entropy") instead
of *($3 $4). This option is effective only when the data file
contains labels in its header.
.TP
.BI \-x1\fR,\ \fB\-\-output\-complexity=1
Print the complexity measure of the model, and the scoring penalty.

.PP
.\" ============================================================
.SS "Precision, recall, BEP, F_1 and prerec problem types"
The prerec, recall, bep, f_one and precision problem types are used
to solve binary classification problems:  problems where the goal
is to sort inputs into one of two sets, while maximizing either
the precision, the sensitivity, or some other figure of merit
of the test.
.PP
In \fBmoses\fR, precision and recall (sensitivity) are defined as usual.
Precision is defined as the number of true positives, divided by the number
of true positives plus the number of false positives.  Classifiers
with a high precision make very few mistakes identifying positives:
they have very few or no false positives.  However, precise classifiers
may completely fail to identify many, if not most positives; they just
don't make mistakes when they do identify them.
.PP
Recall, also known as sensitivity, is defined as the number of true
positives divided by the sum of the number of true positives and false
negatives.  Classifiers with high recall will identify most, or maybe
even all positives; however, they may also identify many negatives,
thus ruining precision.
.PP
A trivial way to maximize precision is to have a very low recall rate,
and conversely, one can very easily have a good recall rate if one
does not mind a poor precision.  Thus a common goal is to maximize
one, while holding the other to a minimum standard.  One common
problem is find a classifier with the highest possible recall, while
holding precision to a fixed minimum level; this may be accomplished
with the \fB\-Hrecall\fR option.  Alternately, one may desire to
maximize precision, while maintaining a minimum sensitivity; this
may be accomplished with the \fB\-Hprerec\fR option.  Note that,
although these two proceedures seem superficially similar, they can
often lead to dramatically different models of the input data.
This is in part because, during early stages, \fBmoses\fP will choose
exemplars that maximize one or the other, thus causing dramatically
different parts of the solution space to be searched.
.PP
A common alternative to maximizing one or the other is to maximize
wither the arithmetic or the harmonic mean of the two.  The arithmetic
mean is sometimes called the "break-even point" or BEP; it is maximized
when the \fB\-Hbep\fR option is specified.  The harmonic mean is known
as the F_1 score, it is maximized when the \fB\-Hf_one\fR option is
specified.
.PP
\fBmoses\fR also provides a second way of maximizing precision, using
the \fB\-Hpre\fR option.  This option searches for the test with the
highest precision, while holding the 'activation' in a bounded range.
The definition of 'activation' is idiosyncratic to moses; it is defined
as the sum of true positives plus false positives: that is, it is
the fraction of rows for which the trial combo program returned
a positive answer, regardless of whether this was the right answer.
Activation ranges from 0.0, to 1.0.  It is never desirable to maximize
activation; rather, most commonly, one wants to peg activation at
exactly the fraction of positives in the training set.
.PP
The minimum level to which a fixed component should be held may be
specified with the \fB\-q\fR or \fB\-\-min\-rand\-input\fR option.
Thus, for the \fB\-Hrecall\fR problem, the \fB\-q\fR flag is used
to specify the minimum desired precision.  Similarly, for the
\fB\-Hprerec\fR problem, the \fB\-q\fR flag is used to specify the
minimum desired recall.
For the \fB\-Hpre\fR problem, the  \fB\-w\fR or
\fB\-\-max\-rand\-input\fR option should be used to make sure the
activation does not get too high.
.PP
The \fB\-q\fR  and \fB\-w\fR options also set lower and upper
bounds for the BEP problem as well.   When maximizing
BEP, the system attempts to keep the absolute value of the
difference between precision and recall less than 0.5.  This
maximum difference can be over-ridden with the \fB\-w\fR option.
.PP
Adherence to the bounds is done by means of a scoring penalty;
combo programs that fail to lie within bounds are penalized.
The harshness or hardness of the penalty may be specified by means
of the \fB\-Q\fR or \fB\-\-alpha\fR option.  Values much greater
than one enforce a hard boundary; values much less than one make
for a very soft boundary.  Negative values are invalid.

.PP
.\" ============================================================
.SS "Contin options"
Options that affect the usage of continuously-valued variables.
These options specify values that are used in a variety of different
ways, depending on the chosen problem type.  See appropriate sections
for more details.
.TP
.BI \-Q\  hardness \fR,\ \fB\-\-alpha= hardness
The harshness of hardness of a limit that must be adhered to.
Default 0.0 (limits disabled).
.TP
.BI \-q\  num \fR,\ \fB\-\-min\-rand\-input= num
Minimum value for continuous variables. Default 0.0.
.TP
.BI \-w\  num \fR,\ \fB\-\-max\-rand\-input= num
Maximum value for continuous variables.  Default 1.0.
.TP
.BI \-R\  num \fR,\ \fB\-\-discretize\-threshold= num
Split a continuous domain into two pieces. This option maybe be used
multiple times to split a continuous domain into multiple pieces:
that is, \fIn\fR uses of this option will create \fIn+1\fR domains.

.PP
.\" ============================================================
.SS "Demo options"
These options pertain to the various built-in demo and example problem
modes.  Such demo problems are commonly used to evaluate different
machine learning algorithms, and are thus included here to facilitate
such comparison, as well as to simplify moses regression and performance
testing.
.TP
.BI \-H\  type \fR,\ \fB\-\-problem\-type= type
A number of demonstration problems are supported. In each case, the top
results are printed to stdout, as a score, followed by a combo program.
.I type
may be one of:
.TS
tab (@);
l lx.
\fBcp\fR@T{
Combo program regression. The scoring function is based on the
combo program specified with the \fB-y\fR flag. That is, the goal of
the run is to deduce and learn the specified combo program.

When specifying combo programs with continuous variables in them, be
sure to use the \fB\-q\fR, \fB\-w\fR and \fB\-b\fR flags to specify
a range of input values to be sampled. In order to determine the fitness
of any candidate, it must be compared to the specified combo
program.  The comparison is done at a variety of different input
values. If the range of sampled input values is inappropriate, or if
there are not enough sampled values, then the fitness function may
select unexpected, undesired candidates.
T}

\fBdj\fR@T{
Disjunction problem. The scoring function awards a result that is a
boolean disjunction (\fIor\fR) of \fIN\fR boolean-valued variables.
The resulting combo program should be \fIor($1 $2 ...)\fR.
The size of the problem may be specified with the \fB\-k\fR option.
T}

\fBmux\fR@T{
Multiplex problem. The scoring function models a boolean digital
multiplexer, that is, an electronic circuit where an "address" of \fIn\fR
bits selects one and only one line, out of \fI2^n\fR possible lines. Thus,
for example, a single address bit can select one of two possible lines:
the first, if its false, and the second, if its true. The \fB\-k\fR
option may be used to specify the value of \fIn\fR.  The actual size
of the problem, measured in bits, is \fIn+2^n\fR and so increases
exponentially fast.
T}

\fBpa\fR@T{
Even parity problem.  The resulting combo program computes the parity of
\fIk\fR bits, evaluating to true if the parity is even, else evaluating
to false.
The size of the problem may be specified with the \fB\-k\fR option.
T}

\fBsr\fR@T{
Polynomial regression problem. Given the polynomial
\fIp(x)=x+x^2+x^3+...x^k\fR, this searches for the shortest program
consisting of nested arithmetic operators to compute \fIp(x)\fR,
given \fIx\fR as a free variable. The arithmetic operators would be
addition, subtraction, multiplication and division; exponentiation
is not allowed in the solution.  So, for example, using the
\fB\-k2\fR option to specify the order\-2 polynomial \fIx+x^2\fR,
then the shortest combo program is \fI*(+(1 $1) $1)\fR (that is,
the solution is \fIp(x)=x(x+1)\fR in the usual arithmetical notation).
T}
.TE

.TP
.BI \-k\  size \fR,\ \fB\-\-problem\-size= size
Specify the size of the problem.  The interpretation of \fIsize\fR
depends on the particular problem type.
.TP
.BI \-y\  prog \fR,\ \fB\-\-combo\-program= prog
Specify the combo program to be learned, when used in combination with
the \fB-H cp\fR option.  Thus, for example, \fB-H cp -y "and(\\$1 \\$2)"\fR
specifies that the two-input conjunction is to be learned.  Keep in mind
that $ is a reserved character in many shells, and thus must be escaped
with a backslash in order to be passed to moses.
.PP
.\" ============================================================
.SH Complexity Penalty
The speed with which the search algorithm can find a reasonable solution
is significantly affected by the complexity ratio specified with the
\fB\-z\fR or \fB\-p\fR options. This section provides the theoretical
underpinning for the meaning of these flags, and how they affect the
the algorithm.  The complexity penalty has two slightly different
interpretations, depending on whether one is considering learning
a discretely-valued problem (i.e. boolean-valued) or a continuously-valued
problem.  The general structure of the argument is broadly similar
for both cases; they are presented below.  Similar arguments
apply for classification problems (learning to classify data into
one of N categories), and for precision maximization.

.\" ============================================================
.SS "Discrete case"

Let M be the model to be learned (the combo program).  Let D be the
data, assumed to be a table of n inputs i_k and one output o, with
each row in the form:

    i_1 ... i_n o

Here, i_k is the k'th input and o the output.  In the below, we write
o = D(x) where x=(i_1, ..., i_n) is an input data row.

We want to assess the probability P(M|D) of the model M conditioned
on the data D.  In particular, we wish to maximize this, as it
provides the fitness function for the model.  According to Bayes
theorem,

    P(M|D) = P(D|M) * P(M) / P(D)

Consider the log likelihood LL(M) of M knowing D.  Since D is constant,
we can ignore P(D), so:

    LL(M) = log(P(D|M)) + log(P(M))

Assume each output of M on row x has probability p of being wrong.  So,

    P(D|M) = Prod_{x\\in D} [p*(M(x) != D(x)) + (1-p)*(M(x) == D(x))]

where D(x) the observed result given input x.  Then,

    log P(D|M) = Sum_{x\\in D} log[p*(M(x) != D(x)) + (1-p)*(M(x) == D(x))]

Let D = D_eq \\cup D_ne  where D_eq and D_ne are the sets

    D_eq = {x \\in D | M(x) == D(x) }
    D_ne = {x \\in D | M(x) != D(x) }

Then

    log P(D|M) = Sum_{x\\in D_ne} log(p) + Sum_{x\\in D_eq} log(1-p)
               = |D_ne| log(p) + |D_eq| log(1-p)
               = |D_ne| log(p) + |D| log(1-p) - |D_ne| log(1-p)
               = |D_ne| log(p/(1-p)) + |D| log(1-p)

Here, |D| is simply the size of set D, etc.  Assuming that p is
small, i.e. much less than one, then, to second order in p:

   log(1-p) = -p + p^2/2 + O(p^3)

So:

   log P(D|M) = |D_ne| log(p) - p (|D| - |D_ne|) + O(p^2)

Next, assume P(M) is distributed according to Solomonoff's Universal
Distribution, approximated by (for now)

    P(M) = |A|^-|M|
         = exp(-|M|*log(|A|))

where A is the alphabet of the model, |A| is the alphabet size,
and |M| is the complexity of the model.  Note that this
distribution is identical to the Boltzmann distribution, for an
inverse temperature of log(|A|). Putting it all together,
the log-likelihood of M is:

    LL(M) = -|M|*log(|A|) + |D_ne| log(p/(1-p)) + |D| log(1-p)

To get an expression usable for a scoring function, just bring
out the |D_ne| by dividing by -log(p/(1-p)), to get

    score(M) = - [ LL(M) - |D| log(1-p) ] / log(p/(1-p))
             = -|D_ne| + |M|*log|A| / log(p/(1-p))
             = -|D_ne| - |M| |C_coef|

Note that, since p<1, that log(p) is negative, and so the second
term is negative.  It can be understood as a \fBcomplexity penalty\fR.
That is, we define the complexity penalty as

   complexity_penalty = |M| |C_coef|

The complexity ratio, as set by the \fB\-z\fR option, is given by

   complexity_ratio = 1 / |C_coef|

By contrast, the \fB\-p\fR option may be used to set p directly, as
given in the formulas above.  The value of |A| is computed internally,
depending on the specific problem type (discrete vs. continuous,
number of included-excluded operators, etc.)  The complexity of each
solution is also computed, using an ad-hoc complexity measure.

.\" ============================================================
.SS "Continuous case"

A similar argument to the above holds for the case of a
continuously-valued observable.

Let dP(..) be the notation for a probability density (or measure).
As before, start with Bayes theorem:

    dP(M|D) = dP(D|M) * P(M) / P(D)

Since D is constant, one may ignore the prior P(D), and write the
log likelihood of M knowing D as:

    LL(M) = log(dP(D|M)) + log(P(M))

Assume the output of of the model M on input x has a Gaussian
distributions, of mean M(x) and variance V, so that dP(D|M),
the probability density of the data D given the modem M is:

    dP(D|M) = Prod_{x\\in D} (2*Pi*V)^(-1/2) exp(-(M(x)-D(x))^2/(2*V))

As before, assume a model distribution of

    P(M) = |A|^-|M|

where |A| is the alphabet size and |M| the complexity of the model.
After simplification, and dropping a constant term that does not depend
on either the model complexity or the dataset itself (the dataset size is a
constant), one then can deduce a scoring function:

    score(M) = -|M|*log(|A|)*2*V - Sum_{x\\in D} (M(x)-D(x))^2

As before, |M|*log(|A|)*2*V can be interpreted as a scoring penalty.
Alternately, one may interpret each row x as a feature; then the
penalty term |M|*log(|A|)*2*V can be interpreted as an additional
feature that must be fit.

.\" ============================================================
.SH Distributed processing
.PP
\fBmoses\fR provides two different styles of distributed processing for
cluster computing systems.  One style is to use MPI (as implemented in
the OpenMPI/MPICH2 systems), the second is to use SSH. The first style
is best suited for local area networks (LANs) and compute clusters. The
second style allows operation over the open Internet, but is more
problematic, as it may require manual cleanup of log files and failed
jobs.  Because MPI is easy to install and manage, it is the recommended
method for distributing moses operation across many machines.
.PP
When \fBmoses\fP is run in distributed fashion, one single node, the
root node, maintains control over a pool of workers that execute on
remote nodes.  The root node maintains the set of candidate solutions,
and assigns these to the workers for additional exploration, as the
workers become free.  The results are automatically collected by the
root, and are automatically merged into the candidate population.  When
termination criteria are met, processing will terminate on all nodes,
and the root node will report the merged, best results.
.PP
.SS "MPI"
Using MPI requires a \fBmoses\fR binary with MPI support compiled in, with
either the OpenMPI or the MPICH2 implementations.  MPI support in \fBmoses\fR
is enabled with the \fB\-\-mpi=1\fR command-line flag.  When this flag
is specified, \fBmoses\fR may be run as usual in an MPI environment.
Details will vary from one system configuration to another, but a
typical usage might resemble the following:

.\" .TP
.BI mpirun\ \-n \ 15 \ \-\-hostfile \ mpd.hosts \ moses\ \-\-mpi=1\ \-j \ 12\ <other\ moses\ params>

.PP
The above specifies that the run should be distributed over fifteen nodes,
with the node names specified in the \fImpd.hosts\fP file.  The
\fB\-\-mpi=1\fR flag indicates to \fBmoses\fP that it is being run in an
MPI environment.  The \fB\-j12\fP flag tells \fBmoses\fP to use up to
twelve threads on each node, whenever possible; this example assumes
each node has 12 cores per CPU.
.PP
To maximize CPU utilization, it seems best to specify two MPI instances per
node.  This is because not all parts of moses are parallelized, and some
parts are subject to lock contention.  Thus, running multiple instances
per node seems to be an effective way to utilize all available compute
power on that node.  It is almost always the case that moses RAM usage
is relatively small, and so RAM availability is rarely a problem.  The
network utilization by moses is also very modest: the only network
traffic is the reporting of candidate solutions, and so the network
demands are typically in the range of 1 Megabit per second, and are thus
easily supported on an Ethernet connection.  The moses workload
distributes in an 'embarrassingly parallel' fashion, and so there is no
practical limit to scaling on small and medium compute clusters.
.PP
When performing input data regression, be sure that the input data file
is available on all nodes.   This is most easily achieved by placing the
input data file on a shared filesystem.  Each instance of moses will
write a log file.  In order to avoid name collision on the log files,
the process id (PID) will automatically be incorporated into the
log-file name when the \fB\-\-mpi=1\fR option is specified.  Log file
creation can be disabled with the \fB\-lNONE\fR option; however, this is
not recommended, as it makes debugging and progress monitoring
difficult.

.SS "SSH"
The SSH style of distributed processing uses the \fBssh\fP command to
establish communications and to control the pool of workers.  Although
\fBmoses\fP provides job control when the system is running normally, it
does not provide any mechanism for cleaning up after hung or failed
jobs;  this is outside the scope of the ssh implementation.  The use of
a job manager, such as LSF, is recommended.

.PP
Remote machines are specified using the \fB\-j\fP option, using the
notation \fB\-j\fR \fIN:REMOTE_HOST\fR.  Here, \fIN\fR is the number
of threads to use on the machine \fIREMOTE_HOST\fR.  For instance,
one can enter the options \fB\-j\fR4 \fB\-j\fI16:my_server.org\fR
(or \fB\-j\fI16:user@my_server.org\fR if one wishes to run the remote
job under a different user name), meaning that 4 threads are allocated
on the local machine and 16 threads are allocated on \fImy_server.org\fP.
Password prompts will appear unless \fBssh\-agent\fR is being used.
The \fBmoses\fR executable must be on the remote machine(s), and
located in a directory included in the \fBPATH\fR environment variable.
Beware that a lot of log files are going to be generated on the
remote machines.

.\" ============================================================
.SH TODO
Finish documenting these algo flags:
--fs-scorer


  -M
--diversity-pressure
--diversity-exponent
--diversity-normalize
--diversity-dst
--diversity-p-norm
--diversity-dst2dp
.PP
-R discretize target var
.PP
These input flags: -G
.PP
Interesting patterns flags: -J -K -U -X

.SH SEE ALSO
.br
The \fBeval-table\fR man page.
.PP
More information is available at
.B http://wiki.opencog.org/w/MOSES
.SH AUTHORS
.nh
\fBmoses\fP was written by Moshe Looks, Nil Geisweiller, and many others.
.PP
This manual page is being written by Linas Vepstas. It is INCOMPLETE.
