.\"                                      Hey, EMACS: -*- nroff -*-
.\" Man page for moses-exec
.\"
.\" Copyright (C) 2011,2012 Linas Vepstas
.\"
.\" First parameter, NAME, should be all caps
.\" Second parameter, SECTION, should be 1-8, maybe w/ subsection
.\" other parameters are allowed: see man(7), man(1)
.pc
.TH MOSES 1 "Sept 7, 2013" "3.6.8" "OpenCog Learning"
.LO 1
.\" Please adjust this date whenever revising the manpage.
.\"
.\" Some roff macros, for reference:
.\" .nh        disable hyphenation
.\" .hy        enable hyphenation
.\" .ad l      left justify
.\" .ad b      justify to both left and right margins
.\" .nf        disable filling
.\" .fi        enable filling
.\" .br        insert line break
.\" .sp <n>    insert n+1 empty lines
.\" for manpage-specific macros, see man(7)
.SH NAME
moses \- meta-optimizing semantic evolutionary search solver
.SH SYNOPSIS
.\" The help & version command line
.B moses
.RB \-h | \--help
.br
.B moses
.RB \--version
.br
.\" The general command line
.B moses
.RB [ \-H
.IR problem_type ]
.RB [ \-i
.IR input_filename ]
.RB [ \-u
.IR target_feature ]
.RB [ \-\-enable\-fs=1 ]
.RB [ \-Y
.IR ignore_feature ]
.RB [ \-m
.IR max_evals ]
.RB [ \-\-max\-time
.IR seconds ]
.RB [ \-f
.IR logfile ]
.RB [ \-l
.IR loglevel ]
.RB [ \-r
.IR random_seed ]
.RB [ \-\-mpi=1 ]
.RB [ \-n
.IR ignore_operator ]
.RB [ \-N
.IR use_only_operator ]
.RB [ \-o
.IR output_file ]
.RB [ \-A
.IR max_score ]
.RB [ \-a
.IR algo ]
.RB [ \-B
.IR knob_effort ]
.RB [ \-b
.IR nsamples ]
.RB [ \-C1 ]
.RB [ \-c
.IR result_count ]
.RB [ \-D
.IR max_dist ]
.RB [ \-d1 ]
.RB [ \-E
.IR reduct_effort ]
.RB [ \-e
.IR exemplar ]
.RB [ \-F ]
.RB [ \-g
.IR max_gens ]
.RB [ \-I0 ]
.RB [ \-j
.IR jobs ]
.RB [ \-k
.IR problem_size ]
.RB [ \-L1 ]
.RB [ \-P
.IR pop_size_ratio ]
.RB [ \-p
.IR probability ]
.RB [ \-\-python=1 ]
.RB [ \-Q
.IR hardness ]
.RB [ \-q
.IR min ]
.RB [ \-R
.IR threshold ]
.RB [ \-S0 ]
.RB [ \-s1 ]
.RB [ \-T1 ]
.RB [ \-t1 ]
.RB [ \-V1 ]
.RB [ \-v
.IR temperature ]
.RB [ \-W1 ]
.RB [ \-w
.IR max ]
.RB [ \-\-well\-enough=1 ]
.RB [ \-x1 ]
.RB [ \-y
.IR combo_program ]
.RB [ \-Z1 ]
.RB [ \-z
.IR complexity_ratio ]
.SH DESCRIPTION
.PP
.\" TeX users may be more comfortable with the \fB<whatever>\fP and
.\" \fI<whatever>\fP escape sequences to invoke bold face and italics,
.\" respectively.
\fBmoses\fP is the command-line wrapper to the MOSES program learning
library. It may be used to solve learning tasks specified with file
inputs, or to run various demonstration problems.  Given an input table
of values, it will perform a regression, generating, as output,
a \fBcombo\fP program that best fits the data. \fBmoses\fP is
multi-threaded, and can be distributed across multiple machines to
improve run-time performance.
.PP
.\" ============================================================
.SH EXAMPLE
As an example, the input file of comma-separated values:

.nf
\& a,b,out
\& 0,0,0
\& 0,1,0
\& 1,0,0
\& 1,1,1
.fi

when run with the flags \fI\-Hit\ \-u\ out\ \-W1\fR will generate the combo
program \fIand($a\ $b)\fR. This program indicates that the third column
can be modelled as the boolean-and of the first two.  The \fI\-u\fR option
specifies that it is the third column that should be modelled, and the
\fI\-W1\fR flag indicates that column labels, rather than column numbers,
should be printed on output.

.PP
.\" ============================================================
.SH OVERVIEW
\fBmoses\fP implements program learning by using a meta-optimization
algorithm. That is, it uses two optimization algorithms, one wrapped inside
the other, to find solutions.  The outer optimization algorithm selects
candidate programs (called \fIexemplars\fP), and then creates similar
programs by taking an exemplar and inserting variables (called
\fIknobs\fP) in selected locations. The inner optimization algorithm
then searches for the best possible 'knob settings', returning a set
of programs (a \fIdeme\fP) that provide the best fit to the data. The
outer optimization algorithm then selects new candidates, and performs
a simplification step, reducing these to a normal form, to create new
exemplars.  The process is then repeated until a perfect score is
reached, a maximum number of cycles is performed, or forward progress
is no longer being made.
.PP
Program reduction is performed at two distinct stages: after inserting new
knobs, and when candidates are selected.  Reduction takes a program, and
transforms it into a new form, the so-called \fIelegant normal form\fP,
which, roughly speaking, expresses the program in the smallest, most
compact form possible.  Reduction is an important step of the algorithm,
as it prevents the evolved programs from turning into 'spaghetti code',
avoiding needless complexity that can cost during program evaluation.
.PP
The operation of \fBmoses\fP can be modified by using a large variety of
options to control different parts of the above meta-algorithm.
The inner optimization step can be done using one of several different
algorithms, including \fIhillclimbing\fP, \fIsimulated annealing\fP,
or \fIunivariate Bayesian optimization\fP.  The amount of effort
spent in the inner loop can be limited, as compared to how frequently
new demes and exemplars are chosen. The total number of demes or
exemplars maintained in the population may be controlled.  The effort
expended on program normalization at each of the two steps can be
controlled.
.PP
In addition to controlling the operation of the algorithm, many
options are used to describe the format of the input data, and to
control the printing of the output.
.PP
\fBmoses\fP also has a 'demo' mode: it has a number of built-in
scoring functions for various 'well-known' optimization problems,
such as parity, disjunction, and multiplex. Each of these typically
presents a different set of challenges to the optimization algorithm,
and are used to illustrate 'hard' optimization problems.  These demo
problems are useful both for learning and understanding the use of
\fBmoses\fP, and also for developers and maintainers of \fBmoses\fP
itself, when testing new features and enhancements.

.PP
.\" ============================================================
.SH COMBO PROGRAMS
This section provides a brief overview of the combo programming
language.  Combo is not meant so much for general use by humans
for practical programming, as it is to fit the needs of program
learning.  As such, it is fairly minimal, while still being expressive
enough so that all common programming constructs are simply represented
in combo.  It is not difficult to convert combo programs into
other languages, or to evaluate them directly.
.PP
In combo, all programs are represented as program trees. That
is, each internal node in the tree is an operation and leaves are
either constants or variables. The set of all of the variables in
a program are taken as the arguments to the program. The program
is evaluated by supplying explicit values for the variables,
and then applying operations until the root node is reached.
.PP
By convention, variables are denoted by a dollar-sign followed by
a number, although they may also be named. Variables are not 
explicitly typed; they are only implicitly typed during program
evaluation. Thus, for example, \fIor ($1 $2)\fP is a combo
program that represents the boolean-or of two inputs,
\fI$1\fP and \fI$2\fP.   Built-in constants include \fItrue\fR and 
\fIfalse\fR; numbers may be written with or without a decimal point.
.PP
Logical operators include \fIand\fR, \fIor\fR and \fInot\fR.
.PP
Arithmetic operators include +, - * and / for addition, subtraction,
multiplication and division. Additional arithmetic operators
include \fIsin\fR, \fIlog\fR, \fIexp\fR and \fIrand\fR. The \fIrand\fR
operator returns a value in the range from 0.0 to 1.0.  The predicate 
operator \fI0<\fR (greater than zero) takes a single continuous-valued
expression, and returns a boolean. The operator \fIimpulse\fR does 
the opposite: it takes a single boolean-valued expression, and returns
0.0 or 1.0, depending on whether the argument evaluates to false or true.
.PP
The \fIcond\fR operator takes an alternating sequence of predicates and
values, terminated by an 'else' value.  It evaluates as a chain of 
if-then-else statements, so that it returns the value following the first
predicate to evaluate to \fItrue\fR.
.PP
Thus, as an example, the expression \fIand( 0<( +($x 3))  0<($y))\fR
evaluates to \fItrue\fR if \fI$x\fR is greater than -3 and \fI$y\fR
is greater than zero.
.PP
The above operators are built-in; combo may also be extended with
custom-defined operators, although C++ programming and recompilation
is required for this.
.PP
.\" ============================================================
.SH OPTIONS
.PP
Options fall into four classes: those for specifying inputs,
controlling operation, printing output, and running
\fBmoses\fP in demo mode.

.SS "General options"
.TP
.BI \-e\  exemplar
Specify an initial \fIexemplar\fR to use. Each exemplar is written in
combo. May be specified multiple times.
.TP
.B \-h, \-\-help
Print command options, and exit.
.TP
.BI \-j\  num \fR,\ \fB\-\-jobs= num
Allocate \fInum\fR threads for deme optimization.  Using multiple
threads will speed the search on a multi-core machine.
This flag may also be used to specify remote execution; see the section
\fBDistributed processing\fR below.
.TP
.B -\-version
Print program version, and exit.
.PP
.\" ============================================================
.SS "Problem-type options"
MOSES is able to handle a variety of different 'problem types',
such as regression, categorization and clustering, as well as a number
of demo problems, such as parity and factorization.  The \fB\-H\fR
option is used to specify the problem type; the demo problem types are
listed in a later section.

.TP
.BI \-H\  type \fR,\ \fB\-\-problem\-type= type
The 
.I type
of problem may be one of:
.TS
tab (@);
l lx.
\fBit\fR@T{
Regression on an input table.  That is, the input table consists of a set
of columns, all but one considered 'inputs', and one is considered an
output.  The goal of regression is to learn a combo program that most
accurately predicts the output.  For boolean-valued and enumerated
outputs, the scoring function simply counts the number of incorrect
answers, and tries to minimize this score.  For contin-valued outputs,
the mean-square variation is minimized.
T}

\fBpre\fR@T{
Regression on an input table, maximizing precision, instead of accuracy
(that is, minimizing the number of false positives, at the risk of
sometimes failing to identify true positives).  Maximization is done
while holding activation (the hit rate) constant.
T}

\fBprerec\fR@T{
Regression on an input table, maximizing precision, while attempting
to maintain the recall (sensitivity) at or above a given level.  Recall,
also known as sensitivity, is the ratio of true positives to the sum of
true positives and false negatives.
T}

\fBrecall\fR@T{
Regression on an input table, maximizing recall (sensitivity) while
attempting to maintain the precision at or above a given level.
This scorer is most commonly used when it is
important to guarantee a certain level of precision, even if it
means rejecting most events. In medicine and physics/radio applications,
recall is exactly the same thing as sensitivity: this option searches
for the most sensitive test while holding to a minimum level of precision.
T}

\fBbep\fR@T{
Regression on an input table, maximizing the arithmetic mean of the
precision and recall, also known as the "break-even point" or BEP.
T}

\fBf_one\fR@T{
Regression on an input table, maximizing the harmonic mean of the
precision and recall, that is, the F_1 score.
T}

\fBip\fR@T{
Discovery of "interesting predicates" that select rows from the
input table. The data table is assumed to consist of a number of 
boolean-valued input columns, and a contin-valued (floating point)
target column. \fBmoses\fP will learn predicates that select the 
most "interesting" subset of the rows in the table.  The values in
the output columns of the selected rows form a probability
distribution (PDF); this PDF is considered to be "interesting"
if it maximizes a linear combination of several different measures
of the the PDF: the Kullback-Leibler divergence, the skewness, and
the standardized Mann-Whitney U statistic. 
T}

\fBkl\fR@T{
Regression on an input table, by maximizing the Kullback-Leibler
divergence between the distribution of the outputs.  That is, the
output must still be well-scored, but it is assumed that there are
many possible maxima.  (XXX???) Huh?
T}

\fBann-it\fR@T{
Regression on an input table, using a neural network.  (kind-of-like
a hidden Markov model-ish, kind of. XXX Huh???)
T}
.TE
.PP
.\" ============================================================
.SS "Input specification options"
These options control how input data is specified and interpreted.
In its primary mode of operation, \fBmoses\fR performs regression on a
a table of input data. One column is designated as the target, the
remaining columns are taken as predictors.  The output of regression
is a \fBcombo\fR program that is a function of the predictors,
reproducing the target.
.PP
Input files should consist of ASCII data, separated by commas or
whitespace.  The appearance of \fB# ;\fR or \fB!\fR in the first
column denotes a comment line; this line will be ignored. The first
non-comment row, if it is also non-numeric, is taken to hold column
labels. The target column may be
specified using the \fB\-u\fR option with a column name. The printing of
column names on output is controlled with the \fB\-W1\fR flag.
.TP
.BI \-i\  filename \fR,\ \fB\-\-input\-file= filename
The \fIfilename\fR specifies the input data file. The input table must
be in 'delimiter-separated value' (DSV) format.  Valid separators 
are comma (CSV, or comma-separated values), blanks and tabs 
(whitespace). Columns correspond to features; there is one sample per
(non-blank) row. Comment characters are hash, bang and semicolon (#!;)
lines starting with a comment are ignored.
The \fB-i\fR flag may be specified multiple times, to indicate multiple
input files. All files must have the same number of columns.
.TP
.BI \-u\  column \fR,\ \fB\-\-target\-feature= column
The \fIcolumn\fR is used as the target feature to fit.  If no column
is specified, then the first column is used.  The \fIcolumn\fR may be
numeric, or it may be a column label.  If it is numeric, it is taken
to be the number of the column, with column 1 being the left-most.
If \fIcolumn\fR begins with an alphabetic character, it is taken to be
a column label.  In this case, the first non-comment row of the
input file must contain column labels.
.TP
.BI \-Y\  column \fR,\ \fB\-\-ignore\-feature= column
The \fIcolumn\fR should be ignored, and not used as input.  Columns
are specified as above.  This option may be used multiple times, to
ignore multiple columns.
.TP
.BI \-b\  num \fR,\ \fB\-\-nsamples= num
The number of samples to be taken from the input file. Valid values
run between 1 and the number of rows in the data file; other values
are ignored. If this option is absent, then all data rows are used.
If this option is present, then the input table is sampled randomly
to reach this size.
.TP
.BI \-G\  num \fR,\ \fB\-\-weighted\-accuracy= num
Huh ???

.\" ============================================================
.SS "Algorithm control options"
These options provide overall control over the algorithm execution.
The most important of these, for controlling behavior, are the 
\fB-A\fR, \fB\-a\fR, \fB\-m\fR, \fB\-\-max\-time\fR, \fB\-r\fR,
\fB\-v\fR and \fB\-z\fR flags.
.TP
.BI \-a\  algorithm \fR,\ \fB\-\-algo= algorithm
Select the algorithm to apply to a single deme.  This is the algorithm
used in the 'inner loop': given a single exemplar decorated with tunable
\fIknobs\fR, this algorithm searches for the best possible knob settings.
Once these are found (or a timeout, or other termination condition is
reached), control is returned to the outer optimization loop.
Available algorithms include:
.TS
tab (@);
l lx.
\fBhc\fR@T{
Hill-climbing. There are two primary modes of operation; each has
strengths and weaknesses for different problem types.
In the default mode, one begins with an initial collection of 
knob settings, called an \fIinstance\fR. The settings of each knob is
then varied, in turn, until one setting is found that most improves
the score. This setting then becomes the new instance, and the 
process is repeated, until no further improvement is seen. The 
resulting instance is a local maximum; it is returned to
the outer loop.

The alternate mode of operation is triggered by using the
\fB\-L1\fR flag (usually with the \fB\-T1\fR flag). In this
case, as before, all knob settings are explored, one knob at a time.
After finding the one knob that most improves the score, the
algo is done, and the resulting instance is returned to the outer
loop. If no knob settings improved the score, then all possible
settings of two knobs are explored, and then three, etc. until
improvement is found (or the allotted iterations are exceeded).
In this alternate mode, the local hill is \fBnot\fR climbed to
the top; instead, any improvement is immediately handed back to the
outer loop, for another round of exemplar selection and knob-building.
For certain types of problems, including maximally misleading problems,
this can arrive at better solutions, more quickly, than the 
traditional hill-climbing algorithm described above.
T}

\fBsa\fR@T{
Simulated annealing.  (Deprecated). The \fB\-D\fR flag controls the size 
of the neighborhood that is searched during the early, "high-temperature"
phase.  It has a significant effect on the run-time performance of the
algorithm. Using \fB\-D2\fR or \fB\-D3\fR is likely to provide the best
performance.

The current implementation of this algorithm has numerous faults, making
it unlikely to work well for most problems.
T}

\fBun\fR@T{
Univariate Bayesian dependency.
T}
.TE

.TP
.BI \-A\  score \fR,\ \fB\-\-max\-score= score
Specifies the ideal score for a desired solution; used to terminate
search.  If the maximum number of evaluations has not yet elapsed
(set with the \fB\-m\fR option), and a candidate solution is found
that has at least this score, then search is terminated.
.TP
.BI \-m\  num \fR,\ \fB\-\-max\-evals= num
Perform no more than \fInum\fR evaluations of the scoring function.
Default value is 10000.
.TP
.BI \-\-max\-time=\ secs
Run the optimizer for no longer than \fIsecs\fR seconds.  Note that
timing is polled only in a small number of points in the algorithm;
thus, actual execution time might exceed this limit by a few seconds,
or even many minutes, depending on the problem type.  In particular,
knob-building time is not accounted for right away, and thus problems
with a long knob-building time will exceed this limit.  If using this
option, be sure to set the \fB\-m\fR option to some very large value.
Default value is 42 years.
.TP
.BI \-n\  oper \fR,\ \fB\-\-ignore\-operator= oper
Exclude the operator \fIoper\fP from the program solution.
This option may be used several times.  Currently, \fIoper\fP
may be one of \fBdiv\fP, \fBsin\fP, \fBexp\fP, \fBlog\fP,
\fBimpulse\fP
or a variable \fB#\fP\fIn\fP.
You may need to put variables under double quotes.
This option has the priority over the \-N option.
That is, if an operator is both be included and ignored,
then it is ignored.  This option does not work with ANN.
.TP
.BI \-\-linear\-regression= 1
When attempting to fit continuous-valued features, restrict searches
to linear expressions only; that is, do not use polynomials in the fit.
Specifying this option also automatically disables the use of div,
sin, exp and log.  Note that polynomial regression results in search
spaces that grow combinatorially large in the number of input features;
That is, for N features, a quadratic search will entail O(N^2)
possibilities, a cubic search will explore O(N^3) possibilities, and so
on.  Thus, for any problem with more than dozens or a hundred features,
linear regression is recommended.
.TP
.BI \-r\  seed \fR,\ \fB\-\-random\-seed= seed
Use \fIseed\fR as the seed value for the pseudo-random number generator.
.TP
.BI \-s1\fR,\ \fB\-\-enable\-cache=1
Enable memoization of candidate scores.  This allows the number of scoring
function evaluations to be reduced, by maintaining a cache of recently
scored candidates. If a new candidate is found in the cache, that score
is used, instead of a scoring function evaluation.  The effectiveness of
memoization is greatly increased by also using the \fB\-d1\fR flag.
.TP
.BI \-v\  temperature \fR,\ \fB\-\-complexity\-temperature= temperature
Set the "temperature" of the Boltzmann-like distribution used to 
select the next exemplar out of the metapopulation. A temperature that
is too high or too low will make it likely that poor exemplars will be
chosen for exploration, thus resulting in excessively long search times.
Recommended values lie in the range of 2 to 10, with a default of 6.
.TP
.BI \-z\  ratio \fR,\ \fB\-\-complexity\-ratio= ratio
Fix the ratio of score to complexity, to be used as a penalty, 
when ranking the metapopulation for fitness.  Typically, improving
the score by one point requires that the complexity of the model to
increase by (approximately) \fIN\fR bits; the value of \fIN\fR being
data-dependent.   The algorithm will usually learn most easily
if the complexity ratio is set between 1 and 2 times the value of
\fIN\fR.

The complexity ratio is used to calculate a scoring penalty; the
penalty lowers the score in proportion to the solution complexity;
specifically, the penalty is set to the complexity divided by the 
complexity ratio.

Setting the ratio too low causes the algorithm to ignore the more 
complex solutions, ranking them in a way so that they are not much
explored. Thus, the algorithm may get trapped examining only the
simplest solutions, which are probably inappropriate.

Setting this ratio too high will add too much noise to the 
metapopulation, preventing a solution from being found.  That is, the
algorithm may spend too much time evaluating overly-complex solutions,
being unaware of good, simple solutions. 

The relationship between the score change and the complexity change
is very strongly data-dependent, and must (currently) be manually
determined (although it might be possible to measure it automatically).
Input data tables with lots of almost-duplicate data may have very low
ratios; complex problems with sparse data may have very high ratios.
Initial recommended values would be in the range from 1 to 5;
with 3.5 as the default.

.TP
.BI \-Z1\fR,\ \fB\-\-hc\-crossover=1
Controls hill-climbing algorithm behavior.  If false (the default),
then the entire local neighborhood of the current center instance is
explored. The highest-scoring instance is then chosen as the new center
instance, and the process is repeated.  For many datasets, however, 
the highest-scoring instances tend to cluster together, and so an
exhaustive search may not be required. When this option is specified,
a handful of the highest-scoring instances are crossed-over (in the
genetic sense of cross-over) to create new instances.  Only these are
evaluated for fitness; the exhaustive search step is skipped.  For many
problem types, especially those with large neighborhoods (i.e. those with
high program complexity), this can lead to an order-of-magnitude
speedup, or more.  For other problem types, especially those with 
deceptive scoring functions, this can hurt performance.

.PP
.\" ============================================================
.SS "Integrated feature selection"
Problems with a large number of input features (typically, hundreds
or more) can lead to excessively long run-times, and overwhelming
amounts of memory usage.  Such problems can be tackled by limiting
the number of features used during knob building, resulting in smaller
demes that can be searched more quickly.  Integrated feature selection 
is enabled with the \fB\-\-enable\-fs=1\fR option.  The number of 
features to use during knob bilding is specified using the 
\fB\-\-fs\-target\-size\fR option.  A number of additional flags
control the behaviour of the feature selection algorithm; these are
best left alone; the defaults should be adequate for almost all
problems.  The man page for the \fBfeature\-selection\fR command
describes these in greater detail.

.TP
.BI \-\-enable\-fs=1
Enable integrated feature selection.  Feature selection is disabled
by default.
.TP
.BI \-\-fs\-target\-size= num
Select \fInum\fR features for use.  This argument is mandatory if
feature selection is enabled.
.TP
.BI \-\-fs\-algo
Choose the feature-selection algorithm.  Possible choices are 
\fBinc\fR, \fBsmd\fR and \fBhc\fR.  The default value is \fbinc\fR,
which is the fastest bu least accurate algo.
.TP
.BI \-\-fs\-threshold= num
Set the minimum threshold for selecting a feature.
.TP
.BI \-\-fs\-inc\-redundant\-intensity= fraction
When using the \fBinc\fR algorithm, set the threshold to reject
redundant features.
.TP
.BI \-\-fs\-inc\-target\-size\-epsilon= tolerance
When using the \fBinc\fR algorithm, set the smallest step size
used.
.TP
.BI \-\-fs\-inc\-interaction\-terms= num_terms
When using the \fBinc\fR algorithm, set the number of terms used
when computing the joint entropy.
.TP
.BI \-\-fs\-hc\-max\-score
TODO write description
.TP
.BI \-\-fs\-hc\-confidence\-penalty\-intensity
TODO write description
.TP
.BI \-\-fs\-hc\-max\-evals
TODO write description
.TP
.BI \-\-fs\-hc\-fraction\-of\-remaining
TODO write description

.PP
.\" ============================================================
.SS "Large problem parameters"
Problems with a large number of features (100 and above) often 
evolve exemplars with a complexity of 100 or more, which in turn
may have instances with hundreds of thousands of nearest neighbors.
Exploring one nearest neighbor requires one evaluation of the 
scoring function, and so an exhaustive search can be prohibitive.  
A partial search can often work quite well, especially when 
cross-over is enabled.  The following flags control such partial
searches.
.TP
.BI \-\-hc\-max\-nn\-evals= num
Controls hill-climbing algorithm behavior.  When exploring the
nearest neighborhood of an instance, \fInum\fP specifies 
the maximum number of nearest neighbors to explore.  An 
exhaustive search of the nearest neighborhood is performed 
when the number of nearest neighbors is less than this value.  
.TP
.BI \-\-hc\-fraction\-of\-nn= frac
Controls hill-climbing algorithm behavior.   When exploring the
nearest neighborhood of an instance,  \fIfrac\fP specifies
the fraction of nearest neighborhood to explore.  As currently
implemented, only an estimate of the nearest-neighborhood size
is used, not the true size.  However, this estimate is accurate
to within a factor of 2.  Thus, to obtain an exhaustive search
of the entire neighborhood, set this to 2.0 or larger.

.PP
.\" ============================================================
.SS "Algorithm tuning options"
These options allow the operation of the algorithm to be fine-tuned
for specific applications.  These are "advanced" options; changing
these from the default is likely to worsen algorithm behavior in
all but certain special cases.
.TP
.BI \-B\  effort \fR,\ \fB\-\-reduct\-knob\-building\-effort= effort
Effort allocated for reduction during the knob-building stage.
Valid values are in the range 0-3, with 0 standing for minimum effort,
and 3 for maximum effort. Larger efforts result in demes with fewer
knobs, thus lowering the overall dimension of the problem. This can
improve performance by effectively reducing the size of the problem.
The default \fIeffort\fR is 2.
.TP
.BI \-D dist \fR,\ \fB\-\-max\-dist= dist
The maximum radius of the neighborhood around the exemplar to explore.
The default value is 4.
.TP
.BI \-d1\fR,\ \fB\-\-reduce\-all=1
Reduce candidates before scoring evaluation. Otherwise, only dominating
candidates are reduced, just before being added to the metapopulation.
This flag may be useful if scoring function evaluation expense depends
strongly one the structure of the candidate. It is particularly important
to specify this flag when memoization is enabled (with \fB-s1\fR).
.TP
.BI \-E\  effort \fR,\ \fB\-\-reduct\-candidate\-effort= effort
Effort allocated for reduction of candidates. Valid values are
in the range 0-3, with 0 standing for minimum effort, and 3
for maximum effort. For certain very symmetric problems, such
as the disjunct problem, greater reduction can lead to significantly
faster solution-finding.  The default \fIeffort\fR is 2.
.TP
.BI \-g\  num \fR,\ \fB\-\-max\-gens= num
Create and optimize no more than \fInum\fR demes.  Negative numbers
are interpreted as "unlimited". By default, the number of demes is
unlimited.
.TP
.BI \-I0\fR,\ \fB\-\-include\-dominated=0
Disable the merging of dominated candidates into the metapopulation.
When this flag is specified, the metapopulation will consist entirely
of the highest scoring candidates.  Specifying this flag can (severely)
degrade performance, as this will make it more likely that the
algorithm will get trapped in a local maximum. In addition, culling
the dominated candidates takes a significant amount of CPU time and
complexity.
.TP
.BI \-L1\fR,\ \fB\-\-hc\-single\-step=1
Single-step, instead of hill-climbing to the top of a hill. That is,
a single uphill step is taken, and the resulting best demes are folded
back into the metapopulation.  Solving then continues as usual. By
default, the hill-climbing algorithm does not single-step; it instead
continues to the top of the local hill, before folding the resulting
demes back into the metapopulation.  If using this flag, consider
using the \fB\-T1\fR flag to allow the search to be widened, so that
if the initial exemplar is already at the top of a local hill, a search
is made for a different (taller) hill.
.TP
.BI \-N\  oper \fR,\ \fB\-\-include\-only\-operator= oper
Include the operator \fIoper\fP, but exclude others, in the solution.
This option may be used several times to specify multiple
operators.  Currently, \fIoper\fP may be one of
\fBplus\fP, \fBtimes\fP, \fBdiv\fP, \fBsin\fP,
\fBexp\fP, \fBlog\fP, \fBimpulse\fP
or a variable \fB#\fP\fIn\fP.
Note that variables and operators are treated separately, so
that including only some operators will still include all
variables, and including only some variables still include
all operators).  You may need to put variables under double
quotes.  This option does not work with ANN.
.TP
.BI \-P\  num \fR,\ \fB\-\-pop\-size\-ratio= num
Controls amount of time spent on a deme. Default value is 20.
.TP
.BI \-p\  fraction \fR,\ \fB\-\-noise= fraction
This option provides an alternative means of setting the complexity
ratio.  If specified, it over-rides the \fB\-z\fR option.  For
discrete problems, \fIfraction\fR can be interpreted as being
the fraction of score values that are incorrect (e.g. due to 
noisy data).  As such, only values in the range 0 < \fIfraction\fR
< 0.5 are meaningful (i.e. less than half of the data values are
incorrect).  Typical recommended values are in the range of 0.001
to 0.05.  For continuous-valued problems, it can be interpreted
as the standard deviation of a Gaussian noise in the dependent
variable.

For the discrete problem, the complexity ratio is related to the
\fIfraction\fR  p by the explicit formula:

    complexity_ratio = - log(p/(1-p)) / log |A|

where |A| is the (problem-dependent) alphabet size.  See below for a
detailed explanation.

.TP
.BI \-T1\fR,\ \fB\-\-hc\-widen\-search=1
Controls hill-climbing algorithm behavior.  If false (the default),
then deme search terminates when a local hilltop is found. If true,
then the search radius is progressively widened, until another
termination condition is met.  Consider using the \fB\-D\fR flag to
set the maximum search radius.

.TP
.BI \-\-well\-enough=1
For problems with an enumerated ('nominal') output, the learned combo
program is always of the form \fBcond\fR\fI(pred_1 value_1 pred_2 value_2 ... 
pred_n value_n else_val)\fR  where \fIpred_1\fR is a predicate, which,
if true, causes the output to be \fIvalue_1\fR.  If false, then
\fIpred_2\fR is tested, and so on.  If none of the predicates evaluate to
true, then the value of the \fBcond\fR expression is the \fIelse_val\fR.
The well-enough algorithm attempts to find predicates that maximize
precision, the point being that if a perfectly precise \fIpred_1\fR
can be found, then it can be left alone ('leave well-enough alone'),
thus simplifying the remainder of the search problem.  Performing this
evaluation is costly, and may lead to a slow-down, without improving
overall accuracy.

.\" ============================================================
.SS "Output control options"
These options control the displayed output.
.TP
.BI \-C1\fR,\ \fB\-\-output\-dominated=1
Print all of the final metapopulation, and not just the highest-scoring
candidates.
.TP
.BI \-c\  count \fR,\ \fB\-\-result\-count= count
The number of non-dominated (best) results to return, ordered according
to score. If negative, then all results are returned, including the
dominated results.
.TP
.BI \-f\  filename \fR,\ \fB\-\-log\-file= filename
Write debug log traces \fIfilename\fR. If not specified, traces
are written to \fBmoses.log\fR.
.TP
.BI \-F\fR,\ \fB\-\-log\-file\-dep\-opt
Write debug log traces to a filename constructed from the passed
option flags and values. The filename will be truncated to a maximum
of 255 characters.
.TP
.BI \-l\  loglevel \fR,\ \fB\-\-log\-level= loglevel
Specify the level of detail for debug logging. Possible
values for \fIloglevel\fR are \fBNONE\fR, \fBERROR\fR, \fBWARN\fR,
\fBINFO\fR, \fBDEBUG\fR, and \fBFINE\fR. Case does not matter.
Caution: excessive logging detail can lead to significant
program slowdown.  The \fBNONE\fR option disables log file creation.
This may make error debugging difficult.
.TP
.BI \-o\  filename \fR,\ \fB\-\-output\-file= filename
Write results to \fIfilename\fR. If not specified, results are written to
\fBstdout\fR.
.TP
.BI \-\-python=1
Output the highest-scoring programs as python snippets, instead of combo.
.TP
.BI \-S0\fR,\ \fB\-\-output\-score=0
Prevent printing of the score.
.TP
.BI \-t1\fR,\ \fB\-\-output\-bscore=1
Print the behavioral score.
.TP
.BI \-V1\fR,\ \fB\-\-output\-eval\-number=1
Print the number of evaluations performed.
.TP
.BI \-W1\fR,\ \fB\-\-output\-with\-labels=1
Use named labels instead of position place-holders when printing
candidates. For example, *("$temperature" "$entropy") instead
of *($3 $4). This option is effective only when the data file
contains labels in its header.
.TP
.BI \-x1\fR,\ \fB\-\-output\-complexity=1
Print the complexity measure of the model, and the scoring penalty.

.PP
.\" ============================================================
.SS "Precision, recall, BEP, F_1 and prerec problem types"
The prerec, recall, bep, f_one and precision problem types are used
to solve binary classification problems:  problems where the goal
is to sort inputs into one of two sets, while maximizing either
the precision, the sensitivity, or some other figure of merit
of the test.
.PP
In \fBmoses\fR, precision and recall (sensitivity) are defined as usual.
Precision is defined as the number of true positives, divided by the number
of true positives plus the number of false positives.  Classifiers 
with a high precision make very few mistakes identifying positives:
they have very few or no false positives.  However, precise classifiers
may completely fail to identify many, if not most positives; they just
don't make mistakes when they do identify them.
.PP
Recall, also known as sensitivity, is defined as the number of true
positives divided by the sum of the number of true positives and false
negatives.  Classifiers with high recall will identify most, or maybe
even all positives; however, they may also identify many negatives,
thus ruining precision.
.PP
A trivial way to maximize precision is to have a very low recall rate,
and conversely, one can very easily have a good recall rate if one
does not mind a poor precision.  Thus a common goal is to maximize
one, while holding the other to a minimum standard.  One common
problem is find a classifier with the highest possible recall, while
holding precision to a fixed minimum level; this may be accomplished
with the \fB\-Hrecall\fR option.  Alternately, one may desire to
maximize precision, while maintaining a minimum sensitivity; this
may be accomplished with the \fB\-Hprerec\fR option.  Note that,
although these two proceedures seem superficially similar, they can
often lead to dramatically different models of the input data.
This is in part because, during early stages, \fBmoses\fP will choose
exemplars that maximize one or the other, thus causing dramatically
different parts of the solution space to be searched.
.PP
A common alternative to maximizing one or the other is to maximize
wither the arithmetic or the harmonic mean of the two.  The arithmetic
mean is sometimes called the "break-even point" or BEP; it is maximized
when the \fB\-Hbep\fR option is specified.  The harmonic mean is known
as the F_1 score, it is maximized when the \fB\-Hf_one\fR option is
specified.
.PP
\fBmoses\fR also provides a second way of maximizing precision, using
the \fB\-Hpre\fR option.  This option searches for the test with the
highest precision, while holding the 'activation' in a bounded range.
The definition of 'activation' is idiosyncratic to moses; it is defined
as the sum of true positives plus false positives: that is, it is 
the fraction of rows for which the trial combo program returned 
a positive answer, regardless of whether this was the right answer.
Activation ranges from 0.0, to 1.0.  It is never desirable to maximize
activation; rather, most commonly, one wants to peg activation at
exactly the fraction of positives in the training set.
.PP
The minimum level to which a fixed component should be held may be
specified with the \fB\-q\fR or \fB\-\-min\-rand\-input\fR option.
Thus, for the \fB\-Hrecall\fR problem, the \fB\-q\fR flag is used
to specify the minimum desired precision.  Similarly, for the 
\fB\-Hprerec\fR problem, the \fB\-q\fR flag is used to specify the
minimum desired recall.
For the \fB\-Hpre\fR problem, the  \fB\-w\fR or 
\fB\-\-max\-rand\-input\fR option should be used to make sure the
activation does not get too high.
.PP
The \fB\-q\fR  and \fB\-w\fR options also set lower and upper
bounds for the BEP problem as well.   When maximizing
BEP, the system attempts to keep the absolute value of the 
difference between precision and recall less than 0.5.  This
maximum difference can be over-ridden with the \fB\-w\fR option.
.PP
Adherence to the bounds is done by means of a scoring penalty;
combo programs that fail to lie within bounds are penalized.
The harshness or hardness of the penalty may be specified by means
of the \fB\-Q\fR or \fB\-\-alpha\fR option.  Values much greater
than one enforce a hard boundary; values much less than one make
for a very soft boundary.  Negative values are invalid.

.PP
.\" ============================================================
.SS "Contin options"
Options that affect the usage of continuously-valued variables.
These options specify values that are used in a variety of different
ways, depending on the chosen problem type.  See appropriate sections
for more details.
.TP
.BI \-Q\  hardness \fR,\ \fB\-\-alpha= hardness
The harshness of hardness of a limit that must be adhered to.
Default 0.0 (limits disabled).
.TP
.BI \-q\  num \fR,\ \fB\-\-min\-rand\-input= num
Minimum value for continuous variables. Default 0.0.
.TP
.BI \-w\  num \fR,\ \fB\-\-max\-rand\-input= num
Maximum value for continuous variables.  Default 1.0.
.TP
.BI \-R\  num \fR,\ \fB\-\-discretize\-threshold= num
Split a continuous domain into two pieces. This option maybe be used
multiple times to split a continuous domain into multiple pieces:
that is, \fIn\fR uses of this option will create \fIn+1\fR domains.

.PP
.\" ============================================================
.SS "Demo options"
These options pertain to the various built-in demo and example problem
modes.  Such demo problems are commonly used to evaluate different
machine learning algorithms, and are thus included here to facilitate
such comparison, as well as to simplify moses regression and performance
testing.
.TP
.BI \-H\  type \fR,\ \fB\-\-problem\-type= type
A number of demonstration problems are supported. In each case, the top
results are printed to stdout, as a score, followed by a combo program.
.I type
may be one of:
.TS
tab (@);
l lx.
\fBcp\fR@T{
Combo program regression. The scoring function is based on the
combo program specified with the \fB-y\fR flag. That is, the goal of
the run is to deduce and learn the specified combo program.

When specifying combo programs with continuous variables in them, be
sure to use the \fB\-q\fR, \fB\-w\fR and \fB\-b\fR flags to specify
a range of input values to be sampled. In order to determine the fitness
of any candidate, it must be compared to the specified combo
program.  The comparison is done at a variety of different input 
values. If the range of sampled input values is inappropriate, or if
there are not enough sampled values, then the fitness function may
select unexpected, undesired candidates.
T}

\fBdj\fR@T{
Disjunction problem. The scoring function awards a result that is a
boolean disjunction (\fIor\fR) of \fIN\fR boolean-valued variables.
The resulting combo program should be \fIor($1 $2 ...)\fR.
The size of the problem may be specified with the \fB\-k\fR option.
T}

\fBmux\fR@T{
Multiplex problem. The scoring function models a boolean digital
multiplexer, that is, an electronic circuit where an "address" of \fIn\fR
bits selects one and only one line, out of \fI2^n\fR possible lines. Thus,
for example, a single address bit can select one of two possible lines:
the first, if its false, and the second, if its true. The \fB\-k\fR
option may be used to specify the value of \fIn\fR.  The actual size
of the problem, measured in bits, is \fIn+2^n\fR and so increases
exponentially fast.
T}

\fBpa\fR@T{
Even parity problem.  The resulting combo program computes the parity of
\fIk\fR bits, evaluating to true if the parity is even, else evaluating
to false.
The size of the problem may be specified with the \fB\-k\fR option.
T}

\fBsr\fR@T{
Polynomial regression problem. Given the polynomial
\fIp(x)=x+x^2+x^3+...x^k\fR, this searches for the shortest program
consisting of nested arithmetic operators to compute \fIp(x)\fR,
given \fIx\fR as a free variable. The arithmetic operators would be
addition, subtraction, multiplication and division; exponentiation
is not allowed in the solution.  So, for example, using the
\fB\-k2\fR option to specify the order\-2 polynomial \fIx+x^2\fR,
then the shortest combo program is \fI*(+(1 $1) $1)\fR (that is,
the solution is \fIp(x)=x(x+1)\fR in the usual arithmetical notation).
T}
.TE

.TP
.BI \-k\  size \fR,\ \fB\-\-problem\-size= size
Specify the size of the problem.  The interpretation of \fIsize\fR
depends on the particular problem type.
.TP
.BI \-y\  prog \fR,\ \fB\-\-combo\-program= prog
Specify the combo program to be learned, when used in combination with
the \fB-H cp\fR option.  Thus, for example, \fB-H cp -y "and(\\$1 \\$2)"\fR
specifies that the two-input conjunction is to be learned.  Keep in mind
that $ is a reserved character in many shells, and thus must be escaped
with a backslash in order to be passed to moses.
.PP
.\" ============================================================
.SH Complexity Penalty
The speed with which the search algorithm can find a reasonable solution
is significantly affected by the complexity ratio specified with the 
\fB\-z\fR or \fB\-p\fR options. This section provides the theoretical
underpinning for the meaning of these flags, and how they affect the
the algorithm.  The complexity penalty has two slightly different
interpretations, depending on whether one is considering learning
a discretely-valued problem (i.e. boolean-valued) or a continuously-valued
problem.  The general structure of the argument is broadly similar
for both cases; they are presented below.  Similar arguments
apply for classification problems (learning to classify data into
one of N categories), and for precision maximization.

.\" ============================================================
.SS "Discrete case"

Let M be the model to be learned (the combo program).  Let D be the
data, assumed to be a table of n inputs i_k and one output o, with
each row in the form:

    i_1 ... i_n o

Here, i_k is the k'th input and o the output.  In the below, we write
o = D(x) where x=(i_1, ..., i_n) is an input data row.

We want to assess the probability P(M|D) of the model M conditioned
on the data D.  In particular, we wish to maximize this, as it
provides the fitness function for the model.  According to Bayes
theorem,

    P(M|D) = P(D|M) * P(M) / P(D)

Consider the log likelihood LL(M) of M knowing D.  Since D is constant,
we can ignore P(D), so:

    LL(M) = log(P(D|M)) + log(P(M))

Assume each output of M on row x has probability p of being wrong.  So,

    P(D|M) = Prod_{x\\in D} [p*(M(x) != D(x)) + (1-p)*(M(x) == D(x))]

where D(x) the observed result given input x.  Then,

    log P(D|M) = Sum_{x\\in D} log[p*(M(x) != D(x)) + (1-p)*(M(x) == D(x))]

Let D = D_eq \\cup D_ne  where D_eq and D_ne are the sets

    D_eq = {x \\in D | M(x) == D(x) }
    D_ne = {x \\in D | M(x) != D(x) }

Then

    log P(D|M) = Sum_{x\\in D_ne} log(p) + Sum_{x\\in D_eq} log(1-p)
               = |D_ne| log(p) + |D_eq| log(1-p)
               = |D_ne| log(p) + |D| log(1-p) - |D_ne| log(1-p)
               = |D_ne| log(p/(1-p)) + |D| log(1-p)

Here, |D| is simply the size of set D, etc.  Assuming that p is
small, i.e. much less than one, then, to second order in p:

   log(1-p) = -p + p^2/2 + O(p^3)

So:

   log P(D|M) = |D_ne| log(p) - p (|D| - |D_ne|) + O(p^2)

Next, assume P(M) is distributed according to Solomonoff's Universal
Distribution, approximated by (for now)

    P(M) = |A|^-|M|
         = exp(-|M|*log(|A|))

where A is the alphabet of the model, |A| is the alphabet size, 
and |M| is the complexity of the model.  Note that this
distribution is identical to the Boltzmann distribution, for an
inverse temperature of log(|A|). Putting it all together,
the log-likelihood of M is:

    LL(M) = -|M|*log(|A|) + |D_ne| log(p/(1-p)) + |D| log(1-p)

To get an expression usable for a scoring function, just bring
out the |D_ne| by dividing by -log(p/(1-p)), to get

    score(M) = - [ LL(M) - |D| log(1-p) ] / log(p/(1-p))
             = -|D_ne| + |M|*log|A| / log(p/(1-p))
             = -|D_ne| - |M| |C_coef|

Note that, since p<1, that log(p) is negative, and so the second
term is negative.  It can be understood as a \fBcomplexity penalty\fR.
That is, we define the complexity penalty as

   complexity_penalty = |M| |C_coef|

The complexity ratio, as set by the \fB\-z\fR option, is given by

   complexity_ratio = 1 / |C_coef|

By contrast, the \fB\-p\fR option may be used to set p directly, as
given in the formulas above.  The value of |A| is computed internally,
depending on the specific problem type (discrete vs. continuous, 
number of included-excluded operators, etc.)  The complexity of each
solution is also computed, using an ad-hoc complexity measure.

.\" ============================================================
.SS "Continuous case"

A similar argument to the above holds for the case of a 
continuously-valued observable.

Let dP(..) be the notation for a probability density (or measure).
As before, start with Bayes theorem:

    dP(M|D) = dP(D|M) * P(M) / P(D)

Since D is constant, one may ignore the prior P(D), and write the
log likelihood of M knowing D as:

    LL(M) = log(dP(D|M)) + log(P(M))

Assume the output of of the model M on input x has a Gaussian 
distributions, of mean M(x) and variance V, so that dP(D|M),
the probability density of the data D given the modem M is:

    dP(D|M) = Prod_{x\\in D} (2*Pi*V)^(-1/2) exp(-(M(x)-D(x))^2/(2*V))

As before, assume a model distribution of

    P(M) = |A|^-|M|

where |A| is the alphabet size and |M| the complexity of the model.
After simplification, and dropping a constant term that does not depend
on either the model complexity or the dataset itself (the dataset size is a
constant), one then can deduce a scoring function:

    score(M) = -|M|*log(|A|)*2*V - Sum_{x\\in D} (M(x)-D(x))^2

As before, |M|*log(|A|)*2*V can be interpreted as a scoring penalty.
Alternately, one may interpret each row x as a feature; then the
penalty term |M|*log(|A|)*2*V can be interpreted as an additional
feature that must be fit.

.\" ============================================================
.SH Distributed processing
.PP
MOSES provides two different styles of distributed processing for
cluster computing systems.  One style is to use MPI (as implemented in
the OpenMPI/MPICH2 systems), the second is to use SSH. The first style
is best suited for local area networks (LANs) and compute clusters. The
second style allows operation over the open Internet, but is more
problematic, as it may require manual cleanup of log files and failed
jobs.  Because MPI is easy to install and manage, it is the recommended
method for distributing moses operation across many machines.
.PP
When \fBmoses\fP is run in distributed fashion, one single node, the
root node, maintains control over a pool of workers that execute on
remote nodes.  The root node maintains the set of candidate solutions,
and assigns these to the workers for additional exploration, as the
workers become free.  The results are automatically collected by the
root, and are automatically merged into the candidate population.  When
termination criteria are met, processing will terminate on all nodes,
and the root node will report the merged, best results.
.PP
.SS "MPI"
Using MPI requires a \fBmoses\fR binary with MPI support compiled in, with
either the OpenMPI or the MPICH2 implementations.  MPI support in MOSES
is enabled with the \fB\-\-mpi=1\fR command-line flag.  When this flag
is specified, \fBmoses\fR may be run as usual in an MPI environment.
Details will vary from one system configuration to another, but a
typical usage might resemble the following:

.\" .TP
.BI mpirun\ \-n \ 15 \ \-\-hostfile \ mpd.hosts \ moses\ \-\-mpi=1\ \-j \ 12\ <other\ moses\ params>

.PP
The above specifies that the run should be distributed over fifteen nodes,
with the node names specified in the \fImpd.hosts\fP file.  The
\fB\-\-mpi=1\fR flag indicates to \fBmoses\fP that it is being run in an
MPI environment.  The \fB\-j12\fP flag tells \fBmoses\fP to use up to
twelve threads on each node, whenever possible; this example assumes
each node has 12 cores per CPU.
.PP
To maximize CPU utilization, it seems best to specify two MPI instances per
node.  This is because not all parts of moses are parallelized, and some
parts are subject to lock contention.  Thus, running multiple instances
per node seems to be an effective way to utilize all available compute
power on that node.  It is almost always the case that moses RAM usage
is relatively small, and so RAM availability is rarely a problem.  The
network utilization by moses is also very modest: the only network
traffic is the reporting of candidate solutions, and so the network
demands are typically in the range of 1 Megabit per second, and are thus
easily supported on an Ethernet connection.  The moses workload
distributes in an 'embarrassingly parallel' fashion, and so there is no
practical limit to scaling on small and medium compute clusters.
.PP
When performing input data regression, be sure that the input data file
is available on all nodes.   This is most easily achieved by placing the
input data file on a shared filesystem.  Each instance of moses will
write a log file.  In order to avoid name collision on the log files,
the process id (PID) will automatically be incorporated into the
log-file name when the \fB\-\-mpi=1\fR option is specified.  Log file
creation can be disabled with the \fB\-lNONE\fR option; however, this is
not recommended, as it makes debugging and progress monitoring
difficult.

.SS "SSH"
The SSH style of distributed processing uses the \fBssh\fP command to
establish communications and to control the pool of workers.  Although
\fBmoses\fP provides job control when the system is running normally, it
does not provide any mechanism for cleaning up after hung or failed
jobs;  this is outside the scope of the ssh implementation.  The use of
a job manager, such as LSF, is recommended.

.PP
Remote machines are specified using the \fB\-j\fP option, using the
notation \fB\-j\fR \fIN:REMOTE_HOST\fR.  Here, \fIN\fR is the number
of threads to use on the machine \fIREMOTE_HOST\fR.  For instance,
one can enter the options \fB\-j\fR4 \fB\-j\fI16:my_server.org\fR
(or \fB\-j\fI16:user@my_server.org\fR if one wishes to run the remote
job under a different user name), meaning that 4 threads are allocated
on the local machine and 16 threads are allocated on \fImy_server.org\fP.
Password prompts will appear unless \fBssh\-agent\fR is being used.
The \fBmoses\fR executable must be on the remote machine(s), and
located in a directory included in the \fBPATH\fR environment variable.
Beware that a lot of log files are going to be generated on the
remote machines.

.\" ============================================================
.SH TODO
Finish documenting these algo flags:  -M
--diversity-pressure
--diversity-exponent
--diversity-normalize
--diversity-dst
--diversity-p-norm
--diversity-dst2dp
.PP
-R discretize target var
.PP
These input flags: -G
.PP
Interesting patterns flags: -J -K -U -X

.SH SEE ALSO
.br
More information is available at
.B http://wiki.opencog.org/w/MOSES
.SH AUTHORS
.nh
\fBmoses\fP was written by Moshe Looks, Nil Geisweiller, and many others.
.PP
This manual page is being written by Linas Vepstas. It is INCOMPLETE.
