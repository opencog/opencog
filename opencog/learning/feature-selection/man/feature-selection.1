.\"                                      Hey, EMACS: -*- nroff -*-
.\" Man page for feature-seection
.\"
.\" Copyright (C) 2012 Linas Vepstas
.\"
.\" First parameter, NAME, should be all caps
.\" Second parameter, SECTION, should be 1-8, maybe w/ subsection
.\" other parameters are allowed: see man(7), man(1)
.pc
.TH FEATURE-SELECTION 1 "May 2, 2012" "3.0.12" "OpenCog Learning"
.LO 1
.\" Please adjust this date whenever revising the manpage.
.\"
.\" Some roff macros, for reference:
.\" .nh        disable hyphenation
.\" .hy        enable hyphenation
.\" .ad l      left justify
.\" .ad b      justify to both left and right margins
.\" .nf        disable filling
.\" .fi        enable filling
.\" .br        insert line break
.\" .sp <n>    insert n+1 empty lines
.\" for manpage-specific macros, see man(7)
.SH NAME
feature-selection \- dimensional reduction of datasets
.SH SYNOPSIS
.\" The help & version command line
.B feature-selection
.RB \-h | \--help
.br
.B feature-selection
.RB \--version
.br
.\" The general command line
.B feature-selection
.RB \-i
.IR filename
.RB [ \-a
.IR algo ]
.RB [ \-C
.IR num_features ]
.RB [ \-T
.IR threshold ]
.RB [ \-u
.IR target_feature ]
.RB [ \-j
.IR jobs ]
.RB [ \-F
.IR log_file ]
.RB [ \-L ]
.RB [ \-l
.IR loglevel ]
.RB [ \-o
.IR output_file ]
.RB [ \-D
.IR fraction ]
.RB [ \-E
.IR tolerance ]
.RB [ \-f
.IR feature ]
.RB [ \-r
.IR random_seed ]
.RB [ \-U
.IR num_terms ]
.SH DESCRIPTION
.PP
.\" TeX users may be more comfortable with the \fB<whatever>\fP and
.\" \fI<whatever>\fP escape sequences to invode bold face and italics,
.\" respectively.
\fBfeature-selection\fP is a command-line utility for performing
dimensional reduction of large datasets, so as to prepare them 
for a later regession analysis.  It is intended to be used to filter
data before it is fit with the \fBmoses\fP machine-learning program,
but it can be used in more general settings.  

.PP
.\" ============================================================
.SH EXAMPLES
.TP
.BI feature-selection\ \-i raw_data.csv \ \-ammi\ \-C 10 \ \-o reduced.csv \ \-F fs.log

Use the input file \fIraw_data.csv\fP as the input dataset.  The input
file must be a tab-separated or comma-separated table of values.  The 
dependent feature is assumed to be in the first column.  The \fBmmi\fP
algorithm will be used to select 10 different features that are the most
strongly correlated with the dependent feature.  The output will be
written to the file \fIreduced.csv\fP, consisting of the dependent
feature, in the first column, followed by the 10 most predictive
columns.  A logfile of algorithm progress will be written to
\fIfs.log\fP.

.TP
.BI feature-selection\ \-i raw_data.csv \ \-ammi\ \-C 10 \ \-o reduced.csv \ \-u Q3

As above, but use the column labelled \fIQ3\fP as the target feature.
This assumes that the input dataset contains columns labels as the first
row in the dataset.  The selected column labels are reproduced in the
output file.

.PP
.\" ============================================================
.SH OVERVIEW

In regression analysis, one has, in general, a table of values or
\'features\', and the goal of predicting one feature, the \'target\', 
(located in one column of the table) based on the values of the
other features (in other columns).  It is common that only some
of the features are useful predictors of the target, while many
others are mostly just noise.  Regression analysis becomes difficult
if there are too many input variables, and so regression can be improved
by simply filtering out the useless features. Doing this is generically
called \'feature selection\' or \'dimensional reduction\'.  This
program, \fBfeature-selection\fP, does this, using one of several
different algorithms.

This program accepts inputs in a fairly generic whitespace or comma
delimited table format: whitespace may consist of repeated tabs or
blanks; values may be comma-separated but padded with blanks.
The input file may contain comment lines; these lines must start with 
a hash-mark (#), exclamation mark (!) or semi-colon (;) as the first
character.

Boolean values may be indicated with any of the characters 0,f,F and 
1,t,T.  Column values must be either boolean or numeric; at this time,
non-numeric data, including dates or columns containing currency symbols
are not supported.

.PP
.\" ============================================================
.SH OPTIONS
.PP
Options fall into three classes: options for controlling input and
output, options for controlling the algorithm behaviour, and general
options.

.SS "General options"
.TP
.B \-h, \-\-help
Print command options and exit.
.TP
.BI \-j\  num \fR,\ \fB\-\-jobs= num
Allocate \fInum\fR threads for feature selection.  Using multiple
threads on multi-core processors will speed the feature selection
search.

.TP
.B -\-version
Print program version, and exit.

.PP
.\" ============================================================
.SS "Input and output control options"
These options control how the input and output is managed by the
program.

.TP
.BI \-F\  filename \fR,\ \fB\-\-log\-file= filename
Write debug log traces to \fIfilename\fR. If not specified, traces
are written to \fBfeature-selection.log\fR.  If the \-L option
is specified, this will be used as the base of the filename.
.TP
.BI \-i\  filename \fR,\ \fB\-\-input\-file= filename
The \fIfilename\fR specifies the input data file. The input table must
be in 'delimiter\-separated value' (DSV) format.  Valid seperators
are comma (CSV, or comma-separated values), blanks and tabs
(whitespace). Columns correspond to features; there is one sample per
(non-blank) row. Comment characters are hash, bang and semicolon (#!;)
lines starting with a comment are ignored.
The \fB\-i\fR flag may be specified multiple times, to indicate multiple
input files. All files must have the same number of columns.
.TP
.BI \-L\fR,\ \fB\-\-log\-file\-dep\-opt
Write debug log traces to a filename constructed from the
\fIlogfile\fP specified in the \fB\-F\fP command, used as a prefix, 
and the other option flags and their values.  The filename will 
be truncated to a maximum of 255 characters.
.TP
.BI \-l\  loglevel \fR,\ \fB\-\-log\-level= loglevel
Specify the level of detail for debug loging. Possible
values for \fIloglevel\fR are \fBNONE\fR, \fBERROR\fR, \fBWARN\fR,
\fBINFO\fR, \fBDEBUG\fR, and \fBFINE\fR. Case does not matter.
Caution: excessive logging detail can lead to program slowdown.
.TP
.BI \-o\  filename \fR,\ \fB\-\-output\-file= filename
Write results to \fIfilename\fR. If not specified, results are written
to \fBstdout\fR.
.TP
.BI \-u\  label \fR,\ \fB\-\-target\-feature= label
The \fIlabel\fR is used as the target feature to fit.  If none is
specified, then the first column is used.  The very first row of the
input file, if it contains non-numeric, non-boolean values, is
interpreted as column labels, as is the common practice for
CSV/DSV file formats.
.PP
.\" ============================================================
.SS "Algorithm control options"
These options provide overall control over the algorithm execution.
The most important of these, for controlling behaviour, are the
\fB\-a\fR, \fB\-C\fR and \fB\-T\fR flags.

.TP
.BI \-a\  algorithm \fR,\ \fB\-\-algo= algorithm
Select the algorithm to use for feature selection.
Available algorithms include:
.TS
tab (@);
l lx.
\fBmmi\fR@T{
Maximal Mutual Information.

This algorithm searches for the featureset with the highest mutual
information (MI) with regard to the target variable.  It does so by
adding one feature at a time to the featureset, computing the MI
between the target variable and this featureset,
ranking the result, and keeping only the highest-ranked results.
It can be thought of as a kind-of hill-climbing in the space
of mutual information.  This process is repeated until the desired
number of features is found, or until the MI score stops improving.

The maximum number of desired features must be specified with the
\fB\-C\fP option.  The \fB-T\fP option can be used to specify the
minimum desired improvement in the MI score.  That is, the algorithm
keeps adding features to the feature set until the improvment in the MI
score does not exceed this threshold.  Features are added in random
order, so that if there are redundant features, only one will be 
added, depending on the random seed given with the \fB\-r\fP option.

Two features are considered redundant if they are highly correlated,
so that adding either one of the two may improve MI a lot, but adding
the second will not.  Thus, only one is really needed; using the 
\fB\-T\fP option helps eliminate redundant features.
T}

\fBinc\fR@T{
Incremental, Non-Redundant Mutual Information.

Builds a featureset by incrementally adding features with the highest
mutual information with regard to the target.  Features are accepted
only if the mutual information is above a specified threshold. Features
are rejected if they appear to be redundant with others: that is,
if, by their presence, they fail to change the total mutual information
by more than a minimum amount.

One may specify either the number of features to be selected, or
one may specify a general "pressure" to automatically modulate the
number of features found.  That is, one must specify either the
\fB\-C\fP or the \fB\-T\fP flag, as otherwise, all features will
be selected.
T}

\fBhc\fR@T{
MOSES Hillclimbing. Currently unsupported.
T}
.TE
.TP
.BI \-C\  num_features \fR,\ \fB\-\-target\-size= num_features
Attempt to select at most \fInum_features\fR out of the input set.

This option is ignored unless the chosen algorithm is \fBmmi\fP or
\fBinc\fP.  When the selected algorithm is \fBinc\fP, then the 
\fB\-T\fP option is ignored.

.TP
.BI \-T\  threshold \fR,\ \fB\-\-threshold= threshold
Apply a floating-point threshold for selecting a feature.
A positive value prevents low-scoring features from being 
selected; a value of zero or less will accept all features.

This option is ignored unless the chosen algorithm is 
\fBmmi\fP or \fBinc\fP.

.TP
.BI \-r\  seed \fR,\ \fB\-\-random\-seed= seed
Use \fIseed\fR as the seed value for the pseudo-random number generator.
The various algorithms use the random number generator in different
ways.  The \fPmmi\fP algorthm explicitly shuffles features, so that 
if the dataset contains multiple redundant features, one will be 
chosen randomly.

.\" ============================================================
.SS "Incremental algorithm options"
These options only apply to the \fB\-ainc\fP algorithm.

.TP
.BI \-D\  fraction \fR,\ \fB\-\-int\-redundant\-intesity= fraction
Threshold fraction used to reject redundant features. If a feature
contributes less than \fIfraction\fP * \fIthreshold\fP to the total
score, it will be rejected from the final feature set.  That is, if
two features are strongly correlated, one should be considered 
redundant; as to which is de-selected will depend on the random-number
generator, i.e. on the random seed specified with the \fB\-r\fP option.

.TP
.BI \-E\  tolerance \fR,\ \fB\-\-inc\-target\-size\-epsilon= tolerance
To be used only with the \fB\-C\fP option.  The incremental algorithm
is not able to directly select a fixed number of features; rather, it
dynamically adjusts the threshold until the desired number of features
results. This option controls the smallest adjustment made.

.TP
.BI \-U\  num_terms \fR,\ \fB\-\-inc\-interaction\-terms= num_terms
The number of variables used in computing the joint entropy.  Normally,
this algorithm never computes the joint entropy of multiple features;
it only considers the effect of a single feature at a time on the
target (that is, it only computes the mutual inforrmation between one
featuree and the target).  Specifying a number greater than one will
consider the mutual information between multiple features and the
target.  Note that using this calculation is combinatorially more 
computatinally expensive, as all possible choices are considered.
.PP
.\" ============================================================
.SH TODO
Document the MOSES-algorithm and the options tht it takes: -A -f -m -O

What's this?: -c ??

.SH SEE ALSO
.br
More information is available at
.B http://wiki.opencog.org/w/XXX???XXX
.SH AUTHORS
.nh
\fBfeature-selection\fP was written by Nil Geisweiller and modified by
Linas Vepstas
.PP
This manual page is being written by Linas Vepstas. It is INCOMPLETE.
