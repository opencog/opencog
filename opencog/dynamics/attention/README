== Introduction ==

The MindAgents in this directory carry out the functions required for attention
allocation among atoms within OpenCog.

== TODO ==

- Move all MindAgents and code to the opencog::ecan namespace.
- Spread LTI
  - Create ECANBank singleton, keep track of various stats, handles that need LTI spread.
  - ImportanceDiffusionAgent flag to spread LTI (implement spread for ALL atoms,
     just for STI > AF boundary, and just for handles in ECANBank that need
     spread)
  - run STI and LTI spread across all atoms in AF every N cycles, run LTI spread
     across handles in ECANBank every M cycles. where N >> M.

== Entities involved in attention allocation ==

- Atoms
  - HebbianLinks - indicate which atoms are important at the same time. 

- MindAgents
  - ImportanceUpdatingAgent - pays "wages" and collects "rent" from atoms in the
    form of Short Term Importance (STI) and Long Term Importance (LTI).
  - ImportanceDiffusionAgent - spreads STI along HebbianLinks.
  - ImportanceSpreadingAgent - alternative way to spread STI along HebbianLinks.
  - ForgettingAgent - "forgets" atoms with low LTI.
  - HebbianUpdatingAgent - updates the weights of HebbianLinks based on what
    is in the Attentional Focus of the OpenCog instance at the time it's run.

- Modules
  - HebbianCreationModule - creates AsymmetricHebbianLinks between
    atoms in the AttentionalFocus for updating by the HebbianUpdatingAgent.

- AttentionValues - Each atom has an AttentionValue to record it's Short
Importance (STI) and Long Term Importance (LTI).

== Overview ==

This sections presents how the flow of attention allocation works:

1. Rewarding "useful" atoms:
 1. Atoms are given stimulus by a MindAgent if they've been useful in
    achieving the MindAgent's goals.
 2. This stimulus is then converted into Short and Long Term Importance, by
    the ImportanceUpdatingAgent. 
2. STI is spread between atoms along HebbianLinks, either by the
   ImportanceDiffusionAgent or the ImportanceSpreadingAgent.
3. The HebbianUpdatingAgent updates the HebbianLink truth values, based
   on whether linked atoms are in the Attentional Focus or not.
4. The ForgettingAgent removes either atoms that are below a threshold LTI,
   or above it.
5. The HebbianCreationModule creates new AsymmetricHebbianLinks as atoms are
   added to the Attentional Focus.

== Hebbian Links ==

A HebbianLink between two atoms indicate that these atoms are likely to be
important to an OpenCog instance at the same time. So if one atom comes into the
attentional focus of the instance, then attention is spread (by the
ImportanceSpreadingAgent) to the other in an attempt to get it into the
attentional focus as well, or at least to prevent it from being forgotten.

HebbianLinks are discovered by the HebbianCreationModule.

HebbianLinks have their weights updated by the HebbianUpdatingAgent.

There are three types of HebbianLinks, Symmetric, Asymmetric, and Inverse:

=== Symmetric Hebbian links ===

Symmetric Hebbian links are unordered, and represent a symmetric correlation
between the atoms connected. Such that, if any of the atoms are in the
attentional focus, then all of the others are also likely to be, or should be,
in the Attentional focus. They can have any Arity.

=== Asymmetric Hebbian links ===

Asymmetric Hebbian links are ordered, and represent an asymmetric correlation
between the connected atoms. If the source atom is in the Attentional focus the
link indicates that the target atom should also be the the the focus, however
the reverse is NOT true.

=== Inverse Hebbian links ===

An Inverse Hebbian links is the opposite of an Asymmetric Hebbian links. The
inverse link is also ordered, but indicates that if the source atom is in the
Attentional focus, then the target atoms should NOT be. This confers the ability
of the target atom to essentially steal importance from the target until it is
no longer in the attentional focus.

This link was introduced to assist in the dynamics of the Hopfield network
emulator but could potentially be useful in other situations too.

== Agents ==

=== ImportanceUpdatingAgent ===

The ImportanceUpdatingAgent handles the exchange of currency between atoms.
Every thought cycle It pays out wages in the form of Short Term Importance (STI)
and Long Term Importance (LTI) to atoms that are currently in use, and also
collects rent from an atoms STI and LTI.

==== Converting Stimulus ====

As atoms are used in mind processes they are endowed with stimulus. MindAgents
should explicit grant atoms stimulus when they make use of them, although it's
left up to the MindAgent to decide how best to do this.

The ImportanceUpdatingAgent takes this stimulus and pays wages to atoms. So in a
sense, the stimulus is a record of how much work the atom has been involved in.
The currency of the wages that the ImportanceUpdatingAgent pays to an atom is in
the form of both STI and LTI. The amount of STI and LTI conferred is based both
on the total stimulus of the atom, and two internal multipliers that indicate
the rate that stimulus is converted into STI and LTI. Atoms also have to pay LTI
rent to exist in the the AtomSpace and STI rent to exist in the Attentional
focus.

For example, say atom A1 has 0 STI and 0 LTI, during some cognitive process it
is given 10 stimulus. Now it comes time for the ImportanceUpdatingAgent to run.
It's internal conversion rate for stimulus into STI is 10 and for LTI the
conversion rate is 3. At this stage a lower conversion rate for LTI is thought
to be of more use because we want LTI to respond slower than the short term
importance (rent for LTI is also lower, see below), for example, things flit in
and out of your attention as you go through the day, but the things you remember
long term are the things that you've spent more time thinking about. When the
ImportanceUpdatingAgent pays wages, it gives the atom A1 100 STI and 30 LTI.

==== Charging rent ====

STI rent is charged from atoms that are above the Attentional focus boundary and
LTI rent for all atoms in the system (conceptually at least, once atoms start
being swapped to disk, LTI rent for these atoms might only be charged
periodically due to the expense involved in updating them). So, if the
attentional focus boundary was set at zero, i.e. all atoms with positive STI are
in the attention span of the OpenCog instance, then for our atom A1, it would be
charged both STI and LTI rent. If STI was < 0 then atom A1 would not be charged
STI rent, but would still be charged LTI rent regardless of the STI or LTI of
A1.

The simplest rent scheme is a flat rent for all atoms in the Attentional Focus.
More dynamic schemes involve increasing the rent the higher STI of the atom
gets. 

==== Updating rent to maintain AtomSpace funds ====

Now, since both STI and LTI currency are conserved, the funds that the
ImportanceUpdatingAgent uses to pay wages has to come from somewhere. In an
ideal situation, this would be balanced by the rent charged to atoms. However,
the number of atoms in the AtomSpace and in the attentional focus will be
constantly changing. In fact, an OpenCog instance may even have some
method of controlling the attentional focus boundary depending on how
focussed, or quick, thought and reaction needs to be. More atoms in the
attentional focus mean more atoms to consider during reasoning, although
reasoning methods can and do use atoms outside of the attentional focus if
those in attention are insufficient to come up with suitable results.

To manage the variable number of atoms in attention and in the Atom Space,
the ImportanceUpdatingAgent draws STI and LTI from a pool of funds managed
by the Atom Space. These pools have a homeostatic range that the
ImportanceUpdatingAgent tries to keep the Atom Space funds within. If, at
the end of an update cycle, the pools are outside of this range, then the
agent taxes all atoms to bring the pools back to the middle of the range
(Note that although it's called a tax, it can actually result in a refund
if the agent has been charging too much tax... just like real life ;-) ).
The agent then recalculates the optimal rent based on decaying measures
of the AtomSpace size and number of atoms in the attentional focus.

=== ImportanceSpreadingAgent ===

The ImportanceSpreadingAgent takes the amount of excess importance that an atom
has and spreads it along the HebbianLinks, with more importance being spread
along HebbianLinks with higher TruthValues. This excess importance is defined as
the amount of STI above the importance spreading threshold and this is usually
greater than the attentional focus boundary to ensure that importance spreading
doesnâ€™t remove atoms from the attentional focus. This transfer of importance can
then result in these destination atoms being elevated into the attentional
focus, increasing their chances of being used in inference and other cognitive
processes. 

(Note: currently, the use of the diffusion agent is favoured).

=== ImportanceDiffusionAgent ===

The ImportanceDiffusionAgent treats the spread of importance as a diffusive process.

Let the variable p_i equal the "probability that Atom A_i is selected", which
should be proportional to its STI.

Then the HebbianLinks and InverseHebbianLinks should determine the transition
probabilities p_ij (The probability that A_i is selected, given that A_j was selected)

Then, we have a Markov matrix M.

Then we multiply q=Mp, where p=(p_1,...,p_n)

... and set the STI levels relative to the entries of q.

If the interaction of the Atoms with the world don't change the HebbianLink
strengths or systematically perturb the Atom STI levels, the network will
eventually relax to the fixed point of the Markov matrix.

This initial implementation uses the gsl::matrix class from
[http://gslwrap.sourceforge.net GSLWrap]. This can only work for a small
AtomSpace, since gsl::matrix isn't sparse.

==== In future ====

[http://www.boost.org/libs/numeric/ Boost] provides a series of sparse matrices
and simple operation on these.

Or work out how to carry out the same procedure without using a sparse matrix
(doable... but might be slower).

=== ForgettingAgent ===

The ForgettingAgent, carries out the forgetting process in OpenCog Prime. It
does based on the LTI of Atoms. Low LTI indicates that an atom has not been of
any use for a long time, and additionally, isn't near any other important atoms.
The latter condition is because the ImportanceSpreadingAgent would otherwise
increase the STI of the atom, by moving STI from nearby important atoms, and
increase the likelihood of the atom in question being:

   1. used in mind processes, and thus
   2. rewarded with stimulus which later gets exchanged for STI. 

The ForgettingAgent also takes into account the VLTI of an atom. This is a
boolean value that indicates whether the atom in fact is allowed to be
forgotten. This allows a mechanism for ensuring that highly important
information will never be forgotten, even if it's used only very very rarely.

Forgetting can be tuned via two parameters:

   1. the maximum LTI value that can be forgotten, and
   2. the percentage of the AtomSpace to forget (typically this would be very low!) 

These work in concert to limit how much and what atoms are forgotten. If only
one parameter is set, then the other has free reign. I.e. a certain percentage
of the AtomSpace will always be forgotten regardless of their LTI, or, any atom
that drops below the maximum forgetting LTI will be forgotten.

=== HebbianCreationModule ===

The HebbianCreationModule creates AsymmetricHebbianLinks between atoms in the
AttentionalFocus for updating by the HebbianUpdatingAgent. Rather than being a
MindAgent that runs every cognitive cycle, it is implemented as a Module that is
notified of atoms that are added to the AttentionalFocus by registering a slot
with the Boost Signals2 events exposed by the AtomSpace. When this slot is
called, a separate handler thread is created to do the work while allowing
execution to continue.
