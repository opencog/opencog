                                Persist
                                -------
                   Linas Vepstas <linasvepstas@gmail.com>
                    Basic implementation Feb-June 2008
                         Status update May 2009
                         Status update Dec 2013

A simple implementation of atom persistence into SQL.

Status
======
Functional, but somewhat incomplete.  It works and has been used with
databases containing millions of atoms, accessed by cogservers that ran
for months to perform computations.

Features
--------
-- Save and restore of atoms, and several kinds of truth values.
-- Bulk save-and-restore of entire AtomSpace contents.
-- Incremental save/restore (i.e. updates the SQL contents as AtomSpace
   changes).  This is *not* fully automatic; see below.
-- Generic API, useful for inter-server communications

Missing features/ToDo items
---------------------------
-- Add full support for CompositeTruthValue.
-- Add full support for attention values. (??)
-- Add full support for Atom deletion.
-- Add full support for 64-bit systems, including >4Gigs of atoms.
-- Provide optimized table layout for EvaluationLinks.
-- Add support for Space/TimeServer data.

Performance status
------------------
AtomTable load time is more than 3x faster than loading from NMXml;
Also, loading from SQL reduces cogServer memory usage by 3x.


Design Goals
============
The goal of this implementation is to:

1) Provide OpenCog with a working memory, so that the Cogsever could
   be stopped and restarted without having to dump/reload all data
   using some slow save/restore interfaces.  By using a database,
   we don't have to worry about disk layout.  By using incremental
   access, we don't have to fill the atomspace with atoms that are not
   immediately needed.

2) Provide an API for inter-server communications and atom exchange.
   Saving and fetching atoms from storage is conceptually similar, if
   not identical, to sending and receiving atoms from another opencog
   instance.  This opens the door to distributed OpenCog.

3) Make sure that the opencog core design is amenable to incremental,
   just-in-time data persistence; that is, the fetching of data
   as it is needed, and saving it away when its not needed. This
   requires an infrastructure for attention allocation (atoms
   that don't get attention can be saved away to disk, while those
   that are needed are fetched on demand.) This will also require
   infrastructure for locks, use counts, etc. that are typical
   of multi-threaded programming.  This prototype should lay the
   foundations for more sophisticated schemes (non-SQL-based)
   to take its place.  This architectural work is inherently difficult,
   and is ongoing.

4) Provide a baseline/reference implementation by which other
   persistence designs can be measured. It is hoped that other systems
   would be at least as fast and as scalable as this one is: this is
   meant to provide a minimal function and performance level. The
   strength of the current design is supposed to be simplicity, not
   scalability or raw performance.

5) A non-design-goal (at this time) is to build a system that can scale
   to more than 100 cogserver instances.  The current design might be
   able to scale to this many, but almost surely not more.  Scaling
   larger than this would probably require a fundamental redesign of
   all of opencog, starting with the atomspace.


Current Design
==============
The core design defines only a few very simple SQL tables, and some
simple readers and writers to save and restore atoms from an SQL
database.

Note that the core design does *not* make use of object reflection,
nor can it store arbitrary kinds of objects. It is very definitely
hard-wired. Yes, this can be considered to be a short-coming.
A more general, persistent object framework (for C) can be found
at http://estron.alioth.debian.org/  However, simplicity, at the
cost of missing flexibility, seems more important.

The current design can save/restore individual atoms, and it can
bulk-save/bulk-restore the entire contents of an AtomTable.
A semi-realized goal of the prototype is to implement incremental save
and restore -- that is, to fetch atoms in a "just in time" fashion, and
to save away atoms that are not needed in RAM (e.g. atoms with
low/non-existent attention values). The AtomSpace BackingStore provides
the current, minimalistic, low-function API for this.

There are architectural issues that remain to be resolved before fully
automatic incremental save/restore can be implemented.  For example,
the current AtomSpace design assumes that the incoming set of an atom is
fully populated; this makes it hard to leave some atoms on disk while
others are in RAM.  These issues need to be resolved for *any* persistent
back-end, and are not specific to this one.

It is not entirely clear that a fully-automatic incremental save/restore
is a good idea. Forcing the server to guess when an atom should be saved
can lead to inefficiendies. If an atom is going to have a short life, it
would be pointless to save it to disk. Fetching atoms only when they are
needed, instead of fetching them earlier, can lead to bottlenecks and
stalls.  All this remains an open problem.

The current design avoids UUID collisions, and automatically thunks
UUID's as needed if an accidental collision with the database occurs.
In order to avoid excess thunking, it is strongly recommended that
the SQL database should be opened immediately on cogserver start, before
any atoms are created.  This will automatically reserve the range of
UUID's that are stored in the DB, and thus avoid collisions as new atoms
are created.


Install, Setup and Usage HOWTO
==============================
There are many steps needed to install and use this. Sorry!

Compiling
---------
Download and install UnixODBC devel packages. Run cmake; make.
Do NOT use IODBC, it fails to support UTF-8.


Database Setup
--------------
There are four basic database setup: installation, device driver setup,
user and password setup, and table initialization.  Each step discussed
below.


Database Install
----------------
Download and install Postgres.  The current design does not support
MySQL, because it uses SQL features that MySQL is missing.  Same
holds true for SQLite.

Be sure to install the postgres server, the postgress client, and the
odbc-postgresql device driver.


Device Driver Setup
-------------------
Configure the ODBC driver. After install, edit (as root) /etc/odbcinst.ini
and put the stanza below into it. Notice that it uses the Unicode drivers,
and NOT the ANSI (ASCII) drivers.  Opencog uses unicode!.

    [PostgreSQL]
    Description = PostgreSQL ODBC driver (Unicode version)
    Driver      = psqlodbcw.so
    Setup       = libodbcpsqlS.so
    Debug       = 0
    CommLog     = 0

The above stanza associates the name 'PostgreSQL' with a particular
driver; this name is needed below.  MySQL users need the stanza below;
a file can safely contain may stanzas defining many different drivers.

WARNING: MySQL is currently not supported. Anyway, if you wanted to mess
with it, then add the below:

    [MySQL]
    Description = MySQL driver
    Driver      = libmyodbc.so
    Setup       = libodbcmyS.so
    CPTimeout   =
    CPReuse     =


Performance tweaks
------------------
The Postgres default configuration needs to be tweaked for performance.
Failure to do this will result in disastrous load and store times.
(The notes below date to 2008; perhaps things have changed?)

Edit postgresql.conf (a typical location is
/etc/postgresql/8.4/main/postgresql.conf) and make the following changes:

   shared_buffers = default was 32MB, change to 25% of install RAM
   effective_cache_size = default was 128MB, change to 50% of installed RAM
   work_mem = default was 1MB change to 32MB
   fsync = default on  change to off
   synchronous_commit = default on change to off
   wal_buffers = default 64kB change to 512kB
   commit_delay = default 0 change to 10000 (10K) microseconds
   ssl = default true change to false

Restarting the server might lead to errors stating that max shared mem
usage has been exceeded. This can be fixed by:

   vi /etc/sysctl.conf
   kernel.shmmax = 440100100

save file contents, then:

   sysctl -p /etc/sysctl.conf


User and Database Setup
-----------------------
A database user needs to be created; the database tables need to be
created. The database user is NOT the same thing as a unix user:
the user login is for the database, not the OS. Do NOT use the same
login and password!

Multiple databases can be created.  In this example, the daatabase
name will be "mycogdata".  Change this as desired.

So, at the Unix command line:

   $ createdb mycogdata

If you get an error message "FATAL:  role "<user>" does not exist", then
try doing this:

   $ su - postgres; createuser <your-unix-username>

Answer the question (yes, you want to be superuser) and exit.
Under rare circumstances, you may need to edit pg_hba.conf. Google for
additional help.

Next, create a database user named opencog_user with password 'cheese'.
You can pick a different username and password, but it must be consistent
with the ~/.odbc.ini file. Do NOT use your login password!  Pick something
else! Create the user at the shell prompt:

   $ psql -c "CREATE USER opencog_user WITH PASSWORD 'cheese'" -d mycogdata

Check that the above worked, by manually logging in:

   $  psql mycogdata -U opencog_user -W -h localhost

If you can't login, something up above failed.

Next, create the database tables:

   $ cat opencog/persist/sql/atom.sql | psql mycogdata -U opencog_user -W -h localhost

Verify that the tables were created. Login as before, then enter \d
at the postgres prompt.  You should see this:

    mycogdata=> \d
                  List of relations
     Schema |   Name    | Type  |     Owner
    --------+-----------+-------+----------------
     public | atoms     | table | opencog_user
     public | edges     | table | opencog_user
     public | global    | table | opencog_user
     public | typecodes | table | opencog_user
    (4 rows)

If the above doesn't work, go back, and try again.

Verify that opencog_user has write permissions to the tables. Do this
entering the below.

    mycogdata=> INSERT INTO TypeCodes (type, typename) VALUES (97, 'SemanticRelationNode');

You should see the appropriate respone:

    INSERT 0 1

If the above doesn't work, go back, and try again.


ODBC Setup, Part the Second
---------------------------
Edit ~/.odbc.ini in your home directory to add a stanza of the form
below. You MUST create one of these for EACH repository you plan to
use! The name of the stanza, and of the database, can be whatever you
wish.  Pay special attention to the username, the password, and the
name of the database to connect to.

   [mycogdata]
   Description       = My Favorite Database
   Driver            = PostgreSQL
   Trace             = No
   TraceFile         =
   Database          = mycogdata
   Servername        = localhost
   Port              = 5432
   Username          = opencog_user
   Password          = cheese
   ReadOnly          = No
   RowVersioning     = No
   ShowSystemTables  = Yes
   ShowOidColumn     = Yes
   FakeOidIndex      = Yes
   ConnSettings      =

Be sure to adjust the username and passwd appropriately.


Opencog Setup
-------------
Edit lib/opencog.conf and set the DB username and password there to the
same values as in ~/.odbc.ini.  Better yet, copy lib/opencog.conf to
your build directory, edit the copy, and start the opencog server as:

   $ ./opencog/server/cogserver -c my.conf

Verify that everything works. Start the cogserver, and bulk-save:

   $ telnet localhost 17001
   opencog> sql-open mycogdata opencog_user cheese
   Opened "mycogdata" as user "opencog_user"

   opencog> sql-store
   
Some output should be printed in the cogserver window: 
   Max UUID is 278
   Finished storing 277 atoms total.

The typical nuber of atoms stored will differ from this.


How To Pass the Unit Test
=========================
There is a unit test for this API.  It is called BasicSaveUTest, and
is disabled by default, because it requires the above complicated setup.
Please run this test.  To do this:

 -- edit tests/persist/sql/CMakeLists.txt and uncomment:
    ADD_CXXTEST(BasicSaveUTest)
    at the bottom.
 -- Read tests/persist/sql/README and follow the instructions there.
    They are almost identical to the instructions above, except that
    they call for a test user to be set up. You can do that, or you
    can use yor current DB user.
 -- To compile and run:
    $ make test

So here's a super-short recap:

 -- Create a new database: e.g.
        $ createdb opencog-test
 -- Create the tables:
        $ cat atom.sql | psql opencog-test
 -- Create an entry in ~/.odbc.ini, as explained above.
 -- Edit lib/opencog-test.conf, and change the database name,
    username, and password as appropriate.

After the above steps, the BasicSaveUTest should pass.


Unit Test Status
----------------
As of 2011-04-29 bzr revision 5314, BasicSaveUTest (still) passes.
As of 2013-11-26 BasicSaveUTest works and passes, again.


Using the System
================
Some example walkthroughs of some typical usage scenarios.

Bulk Save and Restore
---------------------
At last! bulk save of atoms that were previous created is done by
getting to the opencog prompt (telnet localhost 17001) and issuing the
commands:

    opencog> ?
    Available commands:
      exit help list scm shutdown sql-open sql-close sql-store sql-load
    opencog> sql-open
    sql-open: invalid command syntax
    Usage: sql-open <dbname> <username> <auth>

    opencog> sql-open mycogdata opencog_user cheese
    Opened "mycogdata" as user "opencog_user"

    opencog> sql-store
    SQL data store thread started

At this point, a progress indicator will be printed by the opencog
server, on the OpenCog server's stdout. It  will say something like:
Stored 236000 atoms. When finished, its nice to say:

    opencog> sql-close

At this point, the cogserver can be stopped and restarted.  After a
restart, load the data with:

    opencog> sql-open mycogdata opencog_user cheese
    opencog> sql-load
    SQL loader thread started

The completion message will be on the server output, for example:

    Finished loading 973300 atoms in total


Individual-atom save and restore
--------------------------------
There is an interface to save and restore individual atoms. It may be
used as follows:

Start the server:
    $ opencog/server/cogserver -c ../lib/opencog.conf

Open a shell:
    $ telnet localhost 17001
    Trying 127.0.0.1...
    opencog> sql-open mycogdata opencog_user cheese
    Database opened
    opencog> scm
    Entering scheme shell; use ^D or a single . on a line by itself to exit.
    guile> (define x (ConceptNode "asdfasdf" (stv 0.123 0.789)))
    guile> (store-atom x)
    (ConceptNode "asdfasdf" (stv 0.123 0.78899997))

The above will have caused a single atom to be stored in the database.
It's presence can be verified by examining the database directly:

    $ psql mycogdata
    Welcome to psql 8.3.6, the PostgreSQL interactive terminal.
    mycogdata=# select * from atoms;
     uuid | type | tv_type | stv_mean | stv_confidence | stv_count | height |   name   | outgoing
    ------+------+---------+----------+----------------+-----------+--------+----------+----------
        2 |    3 |       1 |    0.123 |     0.78899997 | 2991.4688 |      0 | asdfasdf |
    (1 row)

The backing-store mechanism can now automatically retrieve this atom at
a later time.  Thus, for example, shut down the server, restart it,
re-open the database, and enter the scheme shell again. Then,

    guile> (define y (ConceptNode "asdfasdf"))
    guile> y
    (ConceptNode "asdfasdf" (stv 0.123 0.78899997))

Note that, this time, when the node was created, the database was
searched for a ConceptNode with the same string name. If it was found
in the database, then it is automatically loaded into the AtomSpace,
with the appropriate truth value taken from the database.

If the truth value is modified, the atom is *not* automatically saved
to the database (not at this time; this may change someday).  The
modified atom would need to be explicitly saved again.

Once stored, the atom may be deleted from the AtomSpace; it will
remain in storage, and can be recreated at will:

    guile> (cog-delete y)
    #t
    guile> y
    #<Invalid handle>
    guile> (define y (ConceptNode "asdfasdf"))
    guile> y
    (ConceptNode "asdfasdf" (stv 0.123 0.78899997))

The UUID associated with the atom will NOT change between deletions
and server restarts. This can be verified with the cog-handle command,
which returns the UUID:

    guile> (cog-handle y)
    2

Using SQL Persistence from Scheme
---------------------------------
SQL fetch from storage is not fully automatic at this time. This is to
allay performance concerns: we don't want to be constantly querying the
database every time some atom is needed. In particular: NOTE THIS IS 
IMPORTANT: the incoming set of an atom is not automatically fecthed from
storage.  This must be done manually, using either C++:

   AtomSpace::getImpl()->fetchIncomingSet(h, true);

or the scheme call

  (fetch-incoming-set atom)

If the numeric uuid for an atom is known, but the atom itsself is not,
then the scheme call

   (fetch-atom handle)

can be used to pull an atom into the AtomSpace from the database server.


Copying Databases
-----------------
Sooner or later you will want to make a copy of your database, for
backup purposes, or sharing. Here's the current 'best practices' for
that:

   $ pg_dump mycogdata

That's all!


Experimental Diary & Results
============================
Diary entries from June 2008

Store performance
-----------------
This section reviews the performance for storage of data from opencog
to the SQL server (and thence to disk).  Performed in 2008, on a typical
Intel desktop that was new in 2004. Viz. under two GhZ, and 4GB RAM.

First run with a large data set (save of 1564K atoms to the database)
was a disaster.  Huge CPU usage, with 75% of CPU usage occurring in the
kernel block i/o layer, and 12% each for the opencog and postgres times:
   112:00 [md4_raid1] or 4.3 millisecs per atom
   17 minutes for postgres, and opencog, each. or 0.66 millisecs per atom
   1937576 - 1088032 kB = 850MBytes disk use

Experiment: is this due to the bad design for trying to figure whether
"INSERT" or "UPDATE" should be used? A local client-side cache of the
keys in the DB seems to change little:

   CPU usage for postgres 11:04  and opencog 10:40 and 112:30 for md

So this change drops postgres server and opencog CPU usage
significantly, but the insane kernel CPU usage remains.

The above disaster can be attributed to bad defaults for the postgres
server. In particular, sync to disk, while critical for commercial
database use, is pointless for current use. Also, buffer sizes are much
too small. Edit postgresql.conf and make the following changes:

   shared_buffers = default was 24MB, change to 384MB
   work_mem = default was 1MB change to 32MB
   fsync = default on  change to off
   synchronous_commit = default on change to off
   wal_buffers = default 64kB change to 512kB
   commit_delay = default 0 change to 10000 (10K) microseconds
   ssl = default true change to false

Restarting the server might lead to errors stating that max shared mem
usage has been exceeded. This can be fixed by:

   vi /etc/sysctl.conf
   kernel.shmmax = 440100100
(save file contents, then)
   sysctl -p /etc/sysctl.conf

After tuning, save of data to empty DB gives result:
   cogserver = 10:45 mins = 0.41  millisecs/atom (2.42K atoms/sec)
   postgres  =  7:32 mins = 0.29  millisecs/atom (2.65K atoms/sec)
   md        =  0:42 mins = 0.026 millisecs/atom (37K atoms/sec)

Try again, dropping the indexes on the atom and edge tables. Then,
after loading all atoms, rebuild the index. This time, get
   cogserver = 5:49 mins = 0.227 millisecs/atom (4.40K atoms/sec)
   postgres  = 4:50 mins = 0.189 millisecs/atom (5.30K atoms/sec)

Try again, this time with in-line outgoing sets. This improves
performance even further:

   cogserver = 2:54 mm:ss = 0.113 millisecs/atom (8.83K atoms/sec)
   postgres  = 2:22 mm:ss = 0.092 millisecs/atom (10.82K atoms/sec)

Try again, compiled with -O3, storing to an empty table, with
no indexes on it (and with in-line outgoing sets):
   cogserver = 2:40 mm:ss
   postgres  = 2:16 mm:ss

Try again, compiled with -O3, storing to empty table, while holding
the index on tables (and with in-line outgoing sets).

   cogserver = 2:51 mm:ss
   postgres  = 2:06 mm:ss

Apparently, the problem with the indexes has to do with holding them
for the edge table; when there's no edge table, then there's no index
issue!?

Try again, compiled with -O3, saving to (updating) a *full* database.
(i.e. database already has the data, so we are doing UPDATE not INSERT)
   cogserver = 2:19 mm:ss
   postgres  = 4:35 mm:ss

Try again, using UnixODBC instead of iODBC, to empty table, withOUT
index tables (and -O3 and in-lined outgoing):
   cogserver = 2:36 mm:ss
   postgres  = 2:13 mm:ss

It appears that UnixODBC is essentially identical to iODBC performance

begin; commit;
use analyze;
use prepare;

XML loading performance
-----------------------
Loading the dataset from XML files takes:

   cogserver 2:34 mm:ss when compiled without optimization
   cogserver 1:19 mm:ss when compiled with -O3 optimization

Loading performance
-------------------
Loading performance. Database contains 1564K Atoms, and 2413K edges.
CPU usage:
2:08 postgres = 82 microsecs/atom (12.2K atoms/sec)
similar to for opencog, but then AtomTable needs to reconcile, which
takes an additional 8:30 minutes to attach incoming handles!!

Conclude: database loading would be much faster if we loaded all of
the atoms first, then all of the lowest-height links, etc.  This assumes
that opencog is strictly hierarchically structured. (no "crazy loops")

After implementing height-structured restore, get following, loading
from a "hot" postgres instance (had not been stopped since previous
use, i.e. data should have been hot in RAM):
  cogserver = 2:36 mm:ss = 0.101 millisecs/atom (9.85K atoms/sec)
  postgres  = 1:59 mm:ss = 0.077 millisecs/atom (12.91K atoms/sec)

The dataset had 357162 Nodes, 1206544 Links at height 1

After a cold start, have

  cogserver = 2:32 mm:ss
  postgres  = 1:55 mm:ss

Appears that there is no performance degradation for cold-starts.

Note also: cogServer CPU usage is *identical* to its CPU usage when
loading XML! Hurrah! Also, see below: RAM usage is significantly
reduced; apparently, the reading of XML results in very bad memory
fragmentation.

Implement in-line edges, instead of storing edges in an outboard table.

  cogserver = 41 seconds = 26.7 microsecs/atom (37.5K atoms/sec)
  postgres  =  7 seconds =  4.56 microsecs/atom (219K atoms/sec)

Turn on -O3 optimization during compile ... all previous figures
were without *any* optimization. Now get

  cogserver = 24 seconds = 15.6 microsecs/atom (64.0K atoms/sec)
  postgres  = 11 seconds

Much much better!

10.78
23.15


Evidence of severe memory fragmentation
---------------------------------------
loading 1536K Atoms from XML results in cogserver using 1210 MBytes ram.
loading 1536K Atoms from SQL results in cogserver using 633 MBytes ram.
loading 1536K Atoms from SQL (with new record management) results in
cogserver using 442 MBytes ram.

Almost one-third the ram !!

The fragmentation occurs when the XML is piped to cogserver much faster
than the cogserver can process it; the fragmentation is between data
buffered from the sockets, and the cogserver itself.  The fragmentation
does *not* occur when the data is piped in more slowly,i.e. at the same
rate that opencog processes it. Alternately, the problem may be a memory
leak in CSockets; I think this is the true culprit, but that's not yet
clear.


TODO
====
-- Fix storage of CompositeTruthValues by storing the equivalent
   atoms, i.e. creating a ContextLink for each Composite context.
   This does NOT require modifying the SQL table! Yay!

-- Store attention values. (??) maybe ??

-- Create custom table, tailored for EvaluationLink triples.
   Since majority of nodes/links in the DB will be in the form of
   EvaluationLink triples, a significant performance gain can be gotten
   by creating a custom table, and shunting queries to that.  This would
   decrease the SQL table sizes significantly, and decrease server I/O
   by factors of 2x-3x.  Another table, designed just for simple pairs,
   might help a lot, too.
