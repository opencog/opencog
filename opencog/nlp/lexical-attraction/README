
                     Lexical Attraction, Redux
                     -------------------------
                     Linas Vepstas November 2009


Proof of concept:
-- Deniz Yuret has shown that high-mutual-information word pairs 
   capture semantic content. This has been verified in the LexAt 
   code, which has a bunch of interesting pairs.
-- Many of these pairs can and should be considered to be "semes"
   (see "nlp/seme/README")
-- Dekang Lin's DIRT shows how mutual information can be used to 
   discover synonymous phrases.
-- Hoifung Poon + Pedro Domingos have shown how a very small number
   of relatively simple logical assertions can be used to extract
   important information, including synonymous phrases (USP parsing)
   and coreference resolution.


Core idea:
-- Synthesize the above four points. That is, the Yuret work gives 
   a very basic definition of mutual information (MI) for word pairs.
   Lin shows how a more general notion of MI can be put to effective
   use.  Domingos points out that thinking about logical formulas
   (ungrounded patterns) in general is the correct approach.

-- A key point from Domingos is the focus on "ungrounded formulas". 
   Recast in terms of hypergraphs, the idea is to work with "pattern
   templates" -- smaller hypergraphs containing variable nodes. These
   are the hypergraph analogs of (ungrounded) logical formulas. One
   then looks for pattern matches to these templates -- i.e. for 
   "groundings" of the templates. Domingos says that the resulting 
   matches are used to define a partition function, and a probability.
   The idea here is to instead use mutual information to find frequent,
   semantically meaningful patterns.

-- A key difficulty with the Domingos approach is it's wedding to 
   Markov networks, which have difficulty being tractable (i.e. a
   seeming need to compute the partition function).  The work of Poon
   makes me wonder whether this is strictly necessary: rather, Poon 
   (indirectly, between the lines) suggests that maybe what is really 
   needed are the ability to recognize patterns, and to perform 
   clustering when these are found.  Poon uses the machinery of
   Markov nets to provide a measure of when the clustering should be 
   performed.  The hypothesis here is that perhaps, instead, a simpler
   measure, such as MI, could serve just as well. 

-- To summarize: look for patterns that have high mutual information.
   Do so hierarchically. Perform clustering to find patterns with
   synonymous meanings.  


Implementation:
-- Look for certain kinds of patterns in the dependency parses as passed
   into OpenCog. Make note of those that have high MI.

-- Key stumbling blocks: 
   In a dream world, we'd look for any and all possible patterns.
   In practice, we'd probably only look for a select few. 
   Step 1: do word-pairs, except this time, using the existing 
   hypergraph representations.
   Step 2: do synonymous phrases

-- Stumbling block:
   How to correctly define mutual-info for an arbitrary pattern?

-- Key point:
   Computing the mutual information for a pattern is a time-consuming
   "global" process. The probability of observing a pattern has to 
   divided by the conditional probabilities of observing portions of
   the pattern. Maintaining a large collection of conditional 
   probabilities takes a lot of space; computing these when finally 
   needed requires an (expensive) scan over the entire atomspace. 
   Thus once an MI is computed once, it should be learned "forever".
   That is, once may stop collecting new evidence for the pattern,
   and just take it as a given, known fact.  (Other unrelated mechanisms
   could lead to the active destruction/forgetting of learned pattern.)

-- Stumbling block:
   Once a high-MI pattern has been identified, it needs to be tagged 
   in some way, such that the tag can be a stand-in for the pattern.
   Why? So that hierarchical grouping can be (more easily) discovered.
   This seems to somehow be key.  So: how should this grouping be best
   represented as a hypergraph?

   Examples: "X borders Y" and "X is next to Y" are synonymous phrases
   that should be discoverable by applying appropriate patter detectors.
   however, the underlying dependency parses are quite different, and so
   a key part of the seme-abstraction process is to normalize across 
   these.

Worked Example -- Word Pairs
----------------------------
Here, we work out the details for a given example: high mutual-information
word-pairs -- that is, word-pairs frequently observed together.  There
are three steps involved: counting the frequency of word-pairs, computing
the mutual information, and then, applying the results.


Worked Example: Counting
------------------------
The first step for computing the mutual information for word-pairs is 
counting -- we want to count how often certain words are observed
together.  The primary complication is that the OpenCog representation
of input text is in terms of word-instances whereas we are interested in
lemma-pairs.  

Consider the word pair "bicycle helmet", which we might expect to have
a high MI. This word pair will typically be observed via the _nn
(noun-modifier) relation:

      ; _nn (<<helmet>>, <<bicycle>>) 
      EvaluationLink (stv 1.0 1.0)
         DefinedLinguisticRelationshipNode "_nn"
         ListLink
            WordInstanceNode "helmet@9a282977-c5b5-452f-b2e4-b4f6f08e39a7"
            WordInstanceNode "bicycle@25211c6e-8d78-40d5-ae2e-92ebbf181ab2"

But we are interested in the lemma forms:

      LemmaLink (stv 1.0 1.0)
         WordInstanceNode "bicycle@25211c6e-8d78-40d5-ae2e-92ebbf181ab2"
         WordNode "bicycle"
      LemmaLink (stv 1.0 1.0)
         WordInstanceNode "helmet@9a282977-c5b5-452f-b2e4-b4f6f08e39a7"
         WordNode "helmet"

The template pattern to recognize the above word-pair is shown below. 
The actual pattern recognition will be done by the query pattern matcher.
To distinguish this from other uses, we invent a new TemplateLink, which
derives from VariableScopeLink (http://opencog.org/wiki/VariableScopeLink)

Note the strong typing used here. The strong typing is probably not 
strictly necessary, but is useful for making sure the pattern template
doesn't match any accidentally similar hypergraphs.

      TemplateLink 
         ListLink
            TypedVariableLink
               VariableNode "$rel-var"
               VariableTypeNode "DefinedLinguisticRelationshipNode"
            TypedVariableLink
               VariableNode "$wi-a"
               VariableTypeNode "WordInstanceNode"
            TypedVariableLink
               VariableNode "$wi-b"
               VariableTypeNode "WordInstanceNode"
            TypedVariableLink
               VariableNode "$word-a"
               VariableTypeNode "WordNode"
            TypedVariableLink
               VariableNode "$word-b"
               VariableTypeNode "WordNode"
         ImplicationLink
            AndLink
               EvaluationLink
                  VariableNode "$rel-var"
                  ListLink
                     VariableNode "$wi-a"
                     VariableNode "$wi-b"
               LemmaLink
                  VariableNode "$wi-a"
                  VariableNode "$word-a"
               LemmaLink
                  VariableNode "$wi-b"
                  VariableNode "$word-b"
            CountLink    ;; should be EvaluationLink, actually ... 
               DefinedPatternNode "word-pairs"  ; Maybe a DefinedRelationshipNode ??
               ListLink
                  VariableNode "$word-a"
                  VariableNode "$word-b"

Once the pattern has been recognized, we need to maintain a count of
the number of times that it's been seen.  For this, another new link
type is introduced: the CountLink.  Above, it is used as the implicand
of the ImplicationLink.  Upon running the the implication, the count
on the implicand should be incremented. In this example, CountLink
inherits from EvaluationLink.

(TODO: it might be better to not have a new count-link type, and 
instead, have the pattern matcher always perform counting, by using
a Composite truth value on the implicand, and having a CountTruthValue
appearing in the composite.  This would allow for defacto counting for
all uses of the pattern matcher, which is probably a good idea...)

To summarize: for each new sentence that is input, we run the pattern
matcher to find the above pattern, and increment pattern counts.

Example: Computing the mutual information
-----------------------------------------
Once some fairly large number of sentences have been input, the 
mutual information must be computed. See the section called "Lexical
Attraction" at the bottom for a definition.  The conditional 
probabilities can be obtained by running three different patterns,
to obtain n(x,*), n(*,y) and n(*,*)

For example, the count n(x,*) for x==bicycle is obtained by matching
the following:

      TemplateLink 
         TypedVariableLink
            VariableNode "$word-b"
            VariableTypeNode "WordNode"
         ImplicationLink
            EvaluationLink
               DefinedPatternNode "word-pairs"
               ListLink
                  WordNode "bicycle"
                  VariableNode "$word-b"
            EvaluationLink   ;; CountLink, actually ...
               DefinedPatternNode "word-pairs"
               ListLink
                  WordNode "bicycle"
                  StarNode "match-any"

The StarNode is introduced here to act as a place-holder. It prevents
an accidental pattern-match to WordNode during the pattern search.

Example: Applying the results
-----------------------------
A basic premise of the above is that high MI word pairs correspond to
semantic concepts. Thus, for example, a "bicycle helmet" is a specific
kind of thing.  From the linguistic construction _nn, we know that it
is a kind-of helmet.  We might eventual learn other things about it.
In order to make other manipulations simpler, and to be able to more
easily find related patterns, we want to represent this as a single
node. Provisionally, it seems like the idea of a SemeNode (see 
nlp/seme/README for details) would be the best representation.
Thus, one might construct something like:

      SemeLink
         SemeNode "bicycle helmet"
         EvaluationLink
            DefinedLinguisticRelationshipNode "_nn"
            ListLink
               WordNode "bicycle"
               WordNode "helmet"

... or something like that. This is an incompletely-worked example, 
because that's not the current focus, right .. ??  Need something 
more motivated, e.g. synonymous sentences ... XXX TODO finish me.

Representation points -- 
-- need to have some way of driving output, so have the _nn rel
in place to do this.
-- The semes are usually nouns or noun phrases.
 

Worked example: DIRT, USP, synonymous phrases
----------------------------------------------
In this example, redo Lin's DIRT and/or Domingos USP.

In DIRT, Lin focuses only on binary relations (triples)
From Meaning-Text theory, we know that there are semantic relations 
with other arities as well.

Both Lin and Domingos decompose sentences into parts --
Lin does this casually, with "paths", Domingos formally,
with lambda-partitions. So to keep example simple, do paths first,
then the general case.



=======================================================================

                      Count Truth Value
                      -----------------
                   Linas Vepstas February 2009

Until now, statistical data gathering in the OpenCog NLP pipeline has
required the creation of SQL databases to the collected information.
These databases have been in purely custom-built to hold information
on the specific items of interest; they are not general in any way, and
require considerable effort for each new relation. The goal of the work
here is to provide a uniform infrastructure for statistical data
gathering and processing within OpenCog.

Statistics gathered for NLP input can be used to discern patterns and
relationships in the input.  An example of such are minimum-spanning-tree
dependency parsers, such as the Yuret algorithm, described below. More
generally, the concept of hierarchical mutual information can be used
to identify data clusters, thus extracting semantic content in a
statistical manner.

Statistics gathering are to be done using the CountTruthValue class,
which includes a counter, together with atom persistence, allowing for
a much larger hypergraph than what can be kept in RAM.  As text is
processed, the relevant atoms are pulled in from persistent store,
their counts updated, and then stored back.  It is eventually planned
to tie this to the short-term importance mechanism for RAM management.

At this time, all data mining, including the computation of conditional
probabilities, mutual information, and hierarchical clustering, is
envisioned to be done off-line.

Hypergraphs specific to NLP
---------------------------

Below follows some of the relations of current interest. Statistical
information will be kept by maintaining counts on all of the links and
nodes.  This should be sufficient for computing conditional
probabilities, mutual information and entropy.

Consider first word-pairs:

	WordPairLink  ; CountTruthValue stores count
		WordNode "red"
		WordNode "balloon"

To determine the probability P(left,right), one needs only the count
on the WordPairLink, since P(left,right) = N(left,right) / N(*,*)
That is, the counts on the individual WordNodes are not needed to
obtain the word-pair probabilities, mutual information, etc.

The reason use a WordPairLink, as opposed to recycling some existing
link, is to avoid garbaging the data by having some other counting
process accidentally make use of this link, and improperly increment
its count.

One minimal way of refining the "sense" or "meaning" of a word is to
identify it's part-of-speech. Thus, one is interested in word-pairs
tagged with part-of-speech. To generalize this, one considers word
lemmas (word roots or word stems), tagged with part-of-speech, and
maybe tagged with tense, number, etc.

	FeatureLink                      ; should this be PartOfSpeechLink?
		WordNode "refer"
		DefinedLinguisticConceptNode "verb"   ; part of speech tag

	FeatureLink
		WordNode "refer"
		DefinedLinguisticConceptNode "infinitive"  ; verb tense tag

	FeatureLink
		WordNode "refer"
		DefinedLinguisticConceptNode ".v"  ; the link-grammar "inflection"


The above should be gathered together into a SetLink, as the relative
order is not important.

Thus, pairs of words might be gathered together as:

	CountLink  ; holds CountTruthValue for counting
		SetLink
			FeatureLink
				WordNode "refer"
				DefinedLinguisticConceptNode "verb"   ; part of speech tag
			FeatureLink
				WordNode "refer"
				DefinedLinguisticConceptNode "infinitive"  ; verb tense tag
			FeatureLink
				WordNode "refer"
				DefinedLinguisticConceptNode ".v"  ; the link-grammar "inflection"
		SetLink
			FeatureLink
				WordNode "to"
				DefinedLinguisticConceptNode "prep"   ; part of speech tag


Of some interest is the connection between grammatical usage/construction,
and word sense.  That is, the occurrence of triples of the form
(inflected-word, word-sense, disjunct). These are used to narrow down
the likelihood of a given word sense, given that it was used with a
certain disjunct.  That is, we are interested in the conditional
probability P(sense |word, disjunct)

CountLink                                ; kind-of-like an evaluation link
    DisjunctNode "S- O+ "                ; link-grammar disjunct
    ListLink
       WordNode "blah.n"                 ; inflected word
       WordSenseNode "blah::1:32:0:0.0"  ; wordnet sense

Also of interest is not only how syntax affixes meaning to a single
word, but also how syntax is used to narrow down meanings of pairs
of words. In particular, we expect to see a lot of collocations here,
and in particular, collocations involving noun-modifiers. Link-grammar
uses "AN+" links to denote noun-modifier phrases, e.g. "hand grip",
"lead glass", "soup bowl", where the modifier is a noun.

Thus, we are lead to consider pairs of the above:

CountLink
    LinkGrammarRelationshipNode "MX"
    CountLink
        DisjunctNode "S- O+ MX+ "
        ListLink
           WordNode "blah.n"
           WordSense "blah::1:32:0:0.0"
    CountLink
        DisjunctNode "S- MX- O+ "
        ListLink
           WordNode "glub.n"
           WordSense "glub::1:47:0:0.0"


Gathering Statistics
--------------------
The statistics are meant to be gathered by running the "standard" NLP
processing pipeline, which currently culminates with word-sense
identification. After word-sense assignments, CountLinks as defined
above are to be generated.  The easiest way to assemble these
particular hypergraphs from the mish-mash of input hypergraphs is to
use the query/forward-chainer to perform the assembly.  The result
of assembly will probably be a hypergraph that is sequestered in the
persistence (SQL) database. This hypergraph will be automatically
retrieved by the atomspace, using the "BackingStore" mechanism. After
identification and retrieval, the count on the count link needs to be
updated, and then saved back to the backing store.  At this point, its
acceptable to delete the count link structures, so as to save on RAM
usage.




=======================================================================


                        Lexical Attraction
                        ------------------
                     Linas Vepstas June 2008

This directory contains a prototype implementation of Yuret's "lexical
attraction" algorithm for the discovery of syntactic links between
words in a sentence [Yuret 1998]. The goal of this algorithm is to
provide a score for ranking the output of the link-grammar parser.

The stuff discussed below has been implemented in Java, as a part of
the LexAt package, aka the statistical-relex package, available on
the launchpad site.  It has been used to fish out a large number of
MI values for word pairs.  Many of these are quite interesting.


Data Structures
---------------
The Yuret algorithm requires, as input, the mutual information between
pairs of words. Computing the mutual information requires performing a
statistical analysis on a large corpus of sentences. The statistical
analysis needs to maintain four counts:
-- n(x,y) = the number of times a word pair was observed
-- n(x,*) = the number of times a word occurred as the left element of a
            word pair
-- n(*,y) = the number of times a word occurred as the right element of
            a word pair
-- n(*,*) = N = the number of word pairs observed.

The joint probability of seeing a word pair (x,y) is then

  p(x,y) = n(x,y) / n(*,*)

The marginal probabilities of seeing a word on the left, or right,
respectively, are

  p(x,*) = n(x,*) / n(*,*)
  p(*,y) = n(*,y) / n(*,*)

The "lexical attraction" is defined by Yuret as [chapter 4, page 40]:

   LA(x,y) = log_2 p(x,y) / (p(x,*) p(*,y))

Yuret calls this the "mutual information" for the word pair; however,
this usage, while in a certain sense is correct, is also in conflict
with the common definition of mutual information as the expectation
value of LA; that is, as:

  MI(x,y) = E[LA(x,y)] = p(x,y) LA(x,y)

See for example, Wikipedia "mutual information" or [Ash, Chapter 1.5]




Misc comments
-------------

A classic parse-ranking example is given in the following sentence.
Both parses are more or less legitimate, and have valid interpretations.

(S (NP I) (VP saw (NP the man) (PP with (NP the telescope))) .)

(S (NP I) (VP saw (NP (NP the man) (PP with (NP the telescope)))) .)



References
----------
See also probabilistic context-free grammars.
E. Charniak, 1997. Statistical parsing with a context-free grammar
and word statistics. AAAI 1997.

* Robert B. Ash, ''Information Theory'' (1965) Dover publications.
* Deniz Yuret, ''Discovery of Linguistic Relations Using Lexical
  Attraction'' (1998) PhD Thesis
  http://citeseer.ist.psu.edu/yuret98discovery.html
* [Lin2001] Dekang Lin and Patrick Pantel, ''DIRT â€“ Discovery of 
  Inference Rules from Text'', KDD01

