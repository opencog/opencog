
                     Lexical Attraction, Redux
                     -------------------------
                     Linas Vepstas November 2009


Proof of concept:
-- Deniz Yuret has shown that high-mutual-information word pairs
   capture semantic content. This has been verified in the LexAt
   code, which has a bunch of interesting pairs.
-- Many of these pairs can and should be considered to be "semes"
   (see "nlp/seme/README")
-- Dekang Lin's DIRT shows how mutual information can be used to
   discover synonymous phrases.
-- Hoifung Poon + Pedro Domingos have shown how a very small number
   of relatively simple logical assertions can be used to extract
   important information, including synonymous phrases (USP parsing)
   and coreference resolution.


Core idea:
-- Synthesize the above four points. That is, the Yuret work gives
   a very basic definition of mutual information (MI) for word pairs.
   Lin shows how a more general notion of MI can be put to effective
   use.  Domingos points out that thinking about logical formulas
   (ungrounded patterns) in general is the correct approach.

-- A key point from Domingos is the focus on "ungrounded formulas".
   Recast in terms of hypergraphs, the idea is to work with "pattern
   templates" -- smaller hypergraphs containing variable nodes. These
   are the hypergraph analogs of (ungrounded) logical formulas. One
   then looks for pattern matches to these templates -- i.e. for
   "groundings" of the templates. Domingos says that the resulting
   matches are used to define a partition function, and a probability.
   The idea here is to instead use mutual information to find frequent,
   semantically meaningful patterns.

-- A key difficulty with the Domingos approach is it's wedding to
   Markov networks, which have difficulty being tractable (i.e. a
   seeming need to compute the partition function).  The work of Poon
   makes me wonder whether this is strictly necessary: rather, Poon
   (indirectly, between the lines) suggests that maybe what is really
   needed are the ability to recognize patterns, and to perform
   clustering when these are found.  Poon uses the machinery of
   Markov nets to provide a measure of when the clustering should be
   performed.  The hypothesis here is that perhaps, instead, a simpler
   measure, such as MI, could serve just as well.

-- To summarize: look for patterns that have high mutual information.
   Do so hierarchically. Perform clustering to find patterns with
   synonymous meanings.


Implementation:
-- Look for certain kinds of patterns in the dependency parses as passed
   into OpenCog. Make note of those that have high MI.

-- Key stumbling blocks:
   In a dream world, we'd look for any and all possible patterns.
   In practice, we'd probably only look for a select few.
   Step 1: do word-pairs, except this time, using the existing
   hypergraph representations.
   Step 2: do synonymous phrases

-- Stumbling block:
   How to correctly define mutual-info for an arbitrary pattern?

-- Key point:
   Computing the mutual information for a pattern is a time-consuming
   "global" process. The probability of observing a pattern has to
   divided by the conditional probabilities of observing portions of
   the pattern. Maintaining a large collection of conditional
   probabilities takes a lot of space; computing these when finally
   needed requires an (expensive) scan over the entire atomspace.
   Thus once an MI is computed once, it should be learned "forever".
   That is, once may stop collecting new evidence for the pattern,
   and just take it as a given, known fact.  (Other unrelated mechanisms
   could lead to the active destruction/forgetting of learned pattern.)

-- Stumbling block:
   Once a high-MI pattern has been identified, it needs to be tagged
   in some way, such that the tag can be a stand-in for the pattern.
   Why? So that hierarchical grouping can be (more easily) discovered.
   This seems to somehow be key.  So: how should this grouping be best
   represented as a hypergraph?

   Examples: "X borders Y" and "X is next to Y" are synonymous phrases
   that should be discoverable by applying appropriate patter detectors.
   however, the underlying dependency parses are quite different, and so
   a key part of the seme-abstraction process is to normalize across
   these.

Worked Example -- Word Pairs
----------------------------
Here, we work out the details for a given example: high mutual-information
word-pairs -- that is, word-pairs frequently observed together.  There
are three steps involved: counting the frequency of word-pairs, computing
the mutual information, and then, applying the results.


Worked Example: Counting
------------------------
The first step for computing the mutual information for word-pairs is
counting -- we want to count how often certain words are observed
together.  The primary complication is that the OpenCog representation
of input text is in terms of word-instances whereas we are interested in
lemma-pairs.

Consider the word pair "bicycle helmet", which we might expect to have
a high MI. This word pair will typically be observed via the _nn
(noun-modifier) relation:

      ; _nn (<<helmet>>, <<bicycle>>)
      EvaluationLink (stv 1.0 1.0)
         DefinedLinguisticRelationshipNode "_nn"
         ListLink
            WordInstanceNode "helmet@9a282977-c5b5-452f-b2e4-b4f6f08e39a7"
            WordInstanceNode "bicycle@25211c6e-8d78-40d5-ae2e-92ebbf181ab2"

But we are interested in the lemma forms:

      LemmaLink (stv 1.0 1.0)
         WordInstanceNode "bicycle@25211c6e-8d78-40d5-ae2e-92ebbf181ab2"
         WordNode "bicycle"
      LemmaLink (stv 1.0 1.0)
         WordInstanceNode "helmet@9a282977-c5b5-452f-b2e4-b4f6f08e39a7"
         WordNode "helmet"

The template pattern to recognize the above word-pair is shown below.
The actual pattern recognition will be done by the query pattern matcher.
To distinguish this from other uses, we invent a new TemplateLink, which
derives from BindLink (http://opencog.org/wiki/BindLink)

Note the strong typing used here. The strong typing is probably not
strictly necessary, but is useful for making sure the pattern template
doesn't match any accidentally similar hypergraphs.

      TemplateLink
         ListLink
            TypedVariableLink
               VariableNode "$rel-var"
               VariableTypeNode "DefinedLinguisticRelationshipNode"
            TypedVariableLink
               VariableNode "$wi-a"
               VariableTypeNode "WordInstanceNode"
            TypedVariableLink
               VariableNode "$wi-b"
               VariableTypeNode "WordInstanceNode"
            TypedVariableLink
               VariableNode "$word-a"
               VariableTypeNode "WordNode"
            TypedVariableLink
               VariableNode "$word-b"
               VariableTypeNode "WordNode"
         ImplicationLink
            AndLink
               EvaluationLink
                  VariableNode "$rel-var"
                  ListLink
                     VariableNode "$wi-a"
                     VariableNode "$wi-b"
               LemmaLink
                  VariableNode "$wi-a"
                  VariableNode "$word-a"
               LemmaLink
                  VariableNode "$wi-b"
                  VariableNode "$word-b"

            ;; The implicand, if the above pattern is matched.
            CountLink    ;; should be EvaluationLink, actually ...
               DefinedPatternNode "word-pairs"  ; Maybe a DefinedRelationshipNode ??
               ListLink
                  VariableNode "$word-a"
                  VariableNode "$word-b"

Once the pattern has been recognized, we need to maintain a count of
the number of times that it's been seen.  For this, another new link
type is introduced: the CountLink.  Above, it is used as the implicand
of the ImplicationLink.  Upon running the the implication, the count
on the implicand should be incremented. In this example, CountLink
inherits from EvaluationLink.

(TODO: it might be better to not have a new count-link type, and
instead, have the pattern matcher always perform counting, by using
a Composite truth value on the implicand, and having a CountTruthValue
appearing in the composite.  This would allow for defacto counting for
all uses of the pattern matcher, which is probably a good idea...)

To summarize: for each new sentence that is input, we run the pattern
matcher to find the above pattern, and increment pattern counts.

Example: Computing the mutual information
-----------------------------------------
Once some fairly large number of sentences have been input, the
mutual information must be computed. See the section called "Lexical
Attraction" at the bottom for a definition.  The conditional
probabilities can be obtained by running three different patterns,
to obtain n(x,*), n(*,y) and n(*,*)

For example, the count n(x,*) for x==bicycle is obtained by matching
the following:

      TemplateLink
         TypedVariableLink
            VariableNode "$word-b"
            VariableTypeNode "WordNode"
         ImplicationLink
            EvaluationLink
               DefinedPatternNode "word-pairs"
               ListLink
                  WordNode "bicycle"
                  VariableNode "$word-b"
            EvaluationLink   ;; CountLink, actually ...
               DefinedPatternNode "word-pairs"
               ListLink
                  WordNode "bicycle"
                  StarNode "match-any"

The StarNode is introduced here to act as a place-holder. It prevents
an accidental pattern-match to WordNode during the pattern search.

Example: Applying the results
-----------------------------
A basic premise of the above is that high MI word pairs correspond to
semantic concepts. Thus, for example, a "bicycle helmet" is a specific
kind of thing.  From the linguistic construction _nn, we know that it
is a kind-of helmet.  We might eventual learn other things about it.
In order to make other manipulations simpler, and to be able to more
easily find related patterns, we want to represent this as a single
node. Provisionally, it seems like the idea of a SemeNode (see
nlp/seme/README for details) would be the best representation.
Thus, one might construct something like:

      SemeLink
         SemeNode "bicycle helmet"
         EvaluationLink
            DefinedLinguisticRelationshipNode "_nn"
            ListLink
               WordNode "bicycle"
               WordNode "helmet"

... or something like that. This is an incompletely-worked example,
because that's not the current focus, right .. ??  Need something
more motivated, e.g. synonymous sentences ... XXX TODO finish me.

Representation points --
-- need to have some way of driving output, so have the _nn rel
in place to do this.
-- The semes are usually nouns or noun phrases.


Worked example: DIRT, USP, synonymous phrases
---------------------------------------------
Below follows a worked example of synonymous phrase extraction,
roughly equivalent to Dekang Lin's DIRT [Lin2001] or Poon & Domingos
USP [Poon2009]. The DIRT system focuses on triples of the form
(X VP Y) where X and Y are nouns, and VP is a verb phrase. DIRT then
uses mutual information to find pairings between VP and X, and also
between  VP and Y.  Synonymous X's, Y's and VP's are then discovered
by using a similarity measure btween such pairings.

DIRT proceeds by starting with a dependency parse of a sentence, and
then collecting statistics on all possible "paths" in the parse. A
"path" is a directed graph, starting at the head-word (almost always
a verb) and moving to its dependents, via chaining of head-to-dependent.
DIRT's "paths" always have arity 2, i.e. they always connect some X to
some Y, in the form of (X VP Y).  As Lin demonstrates, this works
reasonably well.  However, it is "well known" that not all semantic
relationships have arity 2.

One of the more convincing analysis of the arity of semantic concepts
is that of Mel'cuk's "Meaning-Test Theory" (MTT), which provides many
examples of semantic relationship with various different arities. A
computational framework for representing expressions with different
arities is provided by [Poon 2009], which defines "lambda forms" --
a notation based on lambda calculus that enables something akin to
Lin's "paths", but of arbitrary arity.  (See the section "USP Notes"
below for a quick tour).  The worked example below focuses first on the
arity-2 "path" case, just to keep things simple.


Example: Counting Paths
-----------------------
Consider the example sentence "John threw a ball". The dependency parse
is "_subj(throw, John) _obj(throw, ball)". The opencog reprsentation
for this parse is

      ; _subj (<<throw>>, <<John>>)
      EvaluationLink (stv 1.0 1.0)
         DefinedLinguisticRelationshipNode "_subj"
         ListLink
            WordInstanceNode "threw@cb670df2-2837-4f18-828b-1c06e8c62bc5"
            WordInstanceNode "John@0cde1b19-6bc3-4939-ad10-6c028beca3b1"

      ; _obj (<<throw>>, <<ball>>)
      EvaluationLink (stv 1.0 1.0)
         DefinedLinguisticRelationshipNode "_obj"
         ListLink
            WordInstanceNode "threw@cb670df2-2837-4f18-828b-1c06e8c62bc5"
            WordInstanceNode "ball@44f8de2d-c089-4620-b09c-fc0947a96df8"

      LemmaLink
         WordInstanceNode "John@0cde1b19-6bc3-4939-ad10-6c028beca3b1"
         WordNode "John"
      LemmaLink
         WordInstanceNode "threw@cb670df2-2837-4f18-828b-1c06e8c62bc5"
         WordNode "throw"
      LemmaLink
         WordInstanceNode "ball@44f8de2d-c089-4620-b09c-fc0947a96df8"
         WordNode "ball"

      WordInstanceLink
         WordInstanceNode "John@0cde1b19-6bc3-4939-ad10-6c028beca3b1"
         ParseNode "sentence@c9d2e919-0682-4d39-9878-39408475f248_parse_0"
      WordInstanceLink
         WordInstanceNode "threw@cb670df2-2837-4f18-828b-1c06e8c62bc5"
         ParseNode "sentence@c9d2e919-0682-4d39-9878-39408475f248_parse_0"
      WordInstanceLink
         WordInstanceNode "ball@44f8de2d-c089-4620-b09c-fc0947a96df8"
         ParseNode "sentence@c9d2e919-0682-4d39-9878-39408475f248_parse_0"

Notable in the above are the WordInstanceLinks, which are used to
confirm that that each word instance belongs to the same parse of the
same sentence. This will be important below, as otherwise, we risk
joining together relations sentences that have nothing to do with each
other.

A pattern template to recognize the above construct, and create a "path",
is shown below.

      TemplateLink
         ListLink
            TypedVariableLink
               VariableNode "$rel-l"
               VariableTypeNode "DefinedLinguisticRelationshipNode"
            TypedVariableLink
               VariableNode "$rel-r"
               VariableTypeNode "DefinedLinguisticRelationshipNode"
            TypedVariableLink
               VariableNode "$wi-head"
               VariableTypeNode "WordInstanceNode"
            TypedVariableLink
               VariableNode "$wi-a"
               VariableTypeNode "WordInstanceNode"
            TypedVariableLink
               VariableNode "$wi-b"
               VariableTypeNode "WordInstanceNode"
            TypedVariableLink
               VariableNode "$word-a"
               VariableTypeNode "WordNode"
            TypedVariableLink
               VariableNode "$word-b"
               VariableTypeNode "WordNode"
            TypedVariableLink
               VariableNode "$parse"
               VariableTypeNode "ParseNode"
         ImplicationLink
            AndLink
               ;; The pattern matcher operates more efficiently if
               ;; we first ask for three words belonging to the same
               ;; sentence, and then do the rest of the pattern.
               WordInstanceLink
                  VariableNode "$wi-head"
                  VariableNode "$parse"
               WordInstanceLink
                  VariableNode "$wi-a"
                  VariableNode "$parse"
               WordInstanceLink
                  VariableNode "$wi-b"
                  VariableNode "$parse"

               ;; The two relations, subj and obj (this matches *any*
               ;; two relations.
               EvaluationLink
                  VariableNode "$rel-l"
                  ListLink
                     VariableNode "$wi-head"
                     VariableNode "$wi-a"
               EvaluationLink
                  VariableNode "$rel-r"
                  ListLink
                     VariableNode "$wi-head"
                     VariableNode "$wi-b"
               LemmaLink
                  VariableNode "$wi-head"
                  VariableNode "$word-head"
               LemmaLink
                  VariableNode "$wi-a"
                  VariableNode "$word-a"
               LemmaLink
                  VariableNode "$wi-b"
                  VariableNode "$word-b"

            ;; The implicand, if the above pattern is matched.
            Implicand ;; See discussion below.

The part preceeding the "Implicand" is the part that needs to be matched
in order for the implicand to apply. Yes, it is very verbose, for what
should be a relatively straight-forward pattern specification.

The form of the implicand requires some dicsussion. Ideally, the implicand,
for the example of "John threw a ball", should be the following:

      EvaluationLink
         PredicateNode "subj(throw,X) obj(throw,Y)"
         ListLink
            WordNode "John"
            WordNode "ball"

The name of the predicate node above is the string "subj(throw,X)
obj(throw,Y)". This string looks suggestive, but doesn't actually have
any meaning -- rather, it is just a unique identifier for this
predicate. Its constructed in this way simply so that when other
sentences are input, if they happen to have the same structure, then
they will use the same name for the predicate node. This string could
be hash -- all that is important is that this hash be unique, and
reproducible for the given relations.

Why do we want the implicand to take this simple form? Because when the
form is simple, then other pattern matching templates can easily find it,
and manipulate it.  This allows some of the techniques being generally
discussed here to be applied hierarchically/recursively.

The problem is that OpenCog does not provide any simple facilities for
creating this hash.  Thus, there are several alternative possibilitites.
One is to use a callback into scheme code to create this link. so, for
example, the Implicand would be:

      ExecutionLink
         GroundedSchemaNode "scm:make-rel-path"
         ListLink
            VariableNode "$rel-l"
            VariableNode "$rel-r"
            VariableNode "$word-head"
            VariableNode "$word-a"
            VariableNode "$word-b"

and the scheme callback would create the string and the final
EvaluationLink:

      (define (make-rel-path rel-l rel-r word-head word-a word-b)
         (define pred-name-str
            (string-concat   ;; create a string by concatenation
               (cog-name rel-l) "(" (cog-name word-head) ", X)"
               (cog-name rel-r) "(" (cog-name word-head) ", Y)"
            )
         )

         ;; Create an evaluation link, and predicate noee with
         ;; the computed name string
         (EvaluationLink
            (PredicateNode pred-name-str)
            (ListLink word-a word-b)
         )
      )

This seems workable, but kind of hacky -- calling out to a special
purpose scheme function just seems hacky/unclean. However, this appears
to be the best solution at this time, after a discussion on the mailing
list.

The above counts paths only of length 1+1, starting from the head-word.
Pattern templates for paths of other lengths would need to be written.

Clustering
----------
Lin [Lin2001] reports parsing a 1GByte text, and obtaining 7M "paths"
of which 231K are unique.

Clustering requires some sort of similarity measure, to determine how 
"similar" two things are, or how similar a thing is to an existing 
cluster. Poon, etal [Poon 2009] consider measures that maximize entropy,
Lin [Lin2001] considers a measure based on set overlap.

For example, consider first whether two different right-hand words of a
collection of word-pairs are similar or not. Define S1 to be the set of
all words w for which mi(w, w1) > 0.  Define S2 analogously as the set
of all words w for which mi(w, w2) > 0.  Finally, define I12 to be the
intersection of S1 and S2.  Then similarity might be defined as a measure
of set overlap:

                  sum_w_in_I12 [ mi (w, w1) + mi(w, w2) ]
  sim(w1, w2) = --------------------------------------------
                [sum_w_in_S1 mi(w, w1)] + [sum_w_in_S2 mi(w, w2)]

Criticism: this would make more sense mathematically if mi was a true
measure-theoretic measure. Suggest exploring actual measures that might
be derived from mi. 

The above similarity measure generalizes in a straight-forward way to 
any graph having shared/common subgraphs.

Computing the above, in a general way, within opencog, needs to make use
of pattern reco, again.

Suppose, for example, word-pairs were linked as follows:

   ;; mi stored in strength of truth value, in the form of the "MI Metric"
   PredicateLink 
       RelationshipNode "word-pair"
       ListLink
           WordNode "bicycle"
           WordNode "tire"

Then for example, sum_w_in_S1 mi(w, w1) with w1='tire' could be computed
by running the pattern matcher on the following:

   BindLink
       ListLink
           TypedVariableLink
               VariableNode "$left-word"
               VariableTypeNode "WordNode"
       ImplicationLink
           ;; The IF clause
           PredicateLink
               RelationshipNode "word-pair"
               ListLink
                   VariableNode "$left-word"
                   WordNode "tire"

           ;; the THEN clause
           ExecutionLink
               GroundedSchemaNode "scm:incr-count"
               PredicateLink
                   RelationshipNode "word-pair"
                   ListLink
                       DefinedCountNode "*"
                       WordNode "tire"

where the scheme function "scm:incr-count" increments the count on the
CountTruthValue of the argument passed in --  in this case, the 
PredicateLink.  Notice the wild-card '*' to indicate that a sum was
taken.

Computing the intersected sum is slightly harder. So, to obtain
sum_w_in_I12 [ mi (w, w1) + mi(w, w2) ] for w1=tire, w2=wheel, one 
should run the following pattern:

   BindLink
       ListLink
           TypedVariableLink
               VariableNode "$left-word"
               VariableTypeNode "WordNode"
       ImplicationLink
           ;; The IF clause
           AndLink
               PredicateLink
                   RelationshipNode "word-pair"
                   ListLink
                       VariableNode "$left-word"
                       WordNode "tire"
               PredicateLink
                   RelationshipNode "word-pair"
                   ListLink
                       VariableNode "$left-word"
                       WordNode "wheel"

           ;; the THEN clause
           ExecutionLink
               GroundedSchemaNode "scm:incr-count"
               PredicateLink
                   RelationshipNode "left-word-pair-inter"
                   ListLink
                       DefinedCountNode "*"
                       WordNode "tire"
                       WordNode "wheel" xxxxxxxxxxxwrong  how to rep this ... 




XXX problem: bad MI scores because "tire" has multiple meanings ... 




=======================================================================

Big picture:

Four features of language: [Sin2004]
-- ambituity -- many senses per word
-- variation -- many ways of saying the same thing
-- terminology -- labelling features e.g. tense, etc. or lack of 
   labels: e.g "What kind of things can be rife? And where are they 
   likely to occur?" this clas has no label.
-- Incompletelness -- not just a lack of terms for lexical categories
   (e.g. things that are rife) but even larer structures: "Its about
   as useful as a ..." there's no namee for "..." and you can't even
   hope to list the membership of "..."

Solutions:
-- ambiguity disappears when context is examined. (WSD)
-- The number of meanings is explosively large when collocations are 
   considered  (i.e. distinct collocations have narrow meanings)
   (example: "saved out skin")
-- Terminology problems are due to separation between grammar and lexis.
   (focus on structure, without sense, focus on sense, w/o structure.)

-- P1 -- "The lexical item is best described maximially, not minimally"
   i.e. provide enough context for full disambiguation.
-- P2 -- (a strong hypothesis) -- each word sense is *unique* once 
   enough context has been provided

-- P3 "Lexical meaning is created at two levels: the general meaing 
   of the lexical item, and the modulation of the meaning by selections
   of individual words".


[Sin2004] "New evidence, new priorities, new attitudes" in J. Sinclair, (ed)
(2004) How to use corpora in language teaching, Amsterdam: John Benjamins


=======================================================================

                      Count Truth Value
                      -----------------
                   Linas Vepstas February 2009

Until now, statistical data gathering in the OpenCog NLP pipeline has
required the creation of SQL databases to the collected information.
These databases have been in purely custom-built to hold information
on the specific items of interest; they are not general in any way, and
require considerable effort for each new relation. The goal of the work
here is to provide a uniform infrastructure for statistical data
gathering and processing within OpenCog.

Statistics gathered for NLP input can be used to discern patterns and
relationships in the input.  An example of such are minimum-spanning-tree
dependency parsers, such as the Yuret algorithm, described below. More
generally, the concept of hierarchical mutual information can be used
to identify data clusters, thus extracting semantic content in a
statistical manner.

Statistics gathering are to be done using the CountTruthValue class,
which includes a counter, together with atom persistence, allowing for
a much larger hypergraph than what can be kept in RAM.  As text is
processed, the relevant atoms are pulled in from persistent store,
their counts updated, and then stored back.  It is eventually planned
to tie this to the short-term importance mechanism for RAM management.

At this time, all data mining, including the computation of conditional
probabilities, mutual information, and hierarchical clustering, is
envisioned to be done off-line.

Hypergraphs specific to NLP
---------------------------

Below follows some of the relations of current interest. Statistical
information will be kept by maintaining counts on all of the links and
nodes.  This should be sufficient for computing conditional
probabilities, mutual information and entropy.

Consider first word-pairs:

   WordPairLink  ; CountTruthValue stores count
      WordNode "red"
      WordNode "balloon"

To determine the probability P(left,right), one needs only the count
on the WordPairLink, since P(left,right) = N(left,right) / N(*,*)
That is, the counts on the individual WordNodes are not needed to
obtain the word-pair probabilities, mutual information, etc.

The reason use a WordPairLink, as opposed to recycling some existing
link, is to avoid garbaging the data by having some other counting
process accidentally make use of this link, and improperly increment
its count.

One minimal way of refining the "sense" or "meaning" of a word is to
identify it's part-of-speech. Thus, one is interested in word-pairs
tagged with part-of-speech. To generalize this, one considers word
lemmas (word roots or word stems), tagged with part-of-speech, and
maybe tagged with tense, number, etc.

   FeatureLink                      ; should this be PartOfSpeechLink?
      WordNode "refer"
      DefinedLinguisticConceptNode "verb"   ; part of speech tag

   FeatureLink
      WordNode "refer"
      DefinedLinguisticConceptNode "infinitive"  ; verb tense tag

   FeatureLink
      WordNode "refer"
      DefinedLinguisticConceptNode ".v"  ; the link-grammar subscript


The above should be gathered together into a SetLink, as the relative
order is not important.

Thus, pairs of words might be gathered together as:

   CountLink  ; holds CountTruthValue for counting
      SetLink
         FeatureLink
            WordNode "refer"
            DefinedLinguisticConceptNode "verb"   ; part of speech tag
         FeatureLink
            WordNode "refer"
            DefinedLinguisticConceptNode "infinitive"  ; verb tense tag
         FeatureLink
            WordNode "refer"
            DefinedLinguisticConceptNode ".v"  ; the link-grammar subscript
      SetLink
         FeatureLink
            WordNode "to"
            DefinedLinguisticConceptNode "prep"   ; part of speech tag


Of some interest is the connection between grammatical usage/construction,
and word sense.  That is, the occurrence of triples of the form
(subscripted-word, word-sense, disjunct). These are used to narrow down
the likelihood of a given word sense, given that it was used with a
certain disjunct.  That is, we are interested in the conditional
probability P(sense |word, disjunct)

CountLink                                ; kind-of-like an evaluation link
    DisjunctNode "S- O+ "                ; link-grammar disjunct
    ListLink
       WordNode "blah.n"                 ; subscripted word
       WordSenseNode "blah::1:32:0:0.0"  ; wordnet sense

Also of interest is not only how syntax affixes meaning to a single
word, but also how syntax is used to narrow down meanings of pairs
of words. In particular, we expect to see a lot of collocations here,
and in particular, collocations involving noun-modifiers. Link-grammar
uses "AN+" links to denote noun-modifier phrases, e.g. "hand grip",
"lead glass", "soup bowl", where the modifier is a noun.

Thus, we are lead to consider pairs of the above:

CountLink
    LinkGrammarRelationshipNode "MX"
    CountLink
        DisjunctNode "S- O+ MX+ "
        ListLink
           WordNode "blah.n"
           WordSense "blah::1:32:0:0.0"
    CountLink
        DisjunctNode "S- MX- O+ "
        ListLink
           WordNode "glub.n"
           WordSense "glub::1:47:0:0.0"


Gathering Statistics
--------------------
The statistics are meant to be gathered by running the "standard" NLP
processing pipeline, which currently culminates with word-sense
identification. After word-sense assignments, CountLinks as defined
above are to be generated.  The easiest way to assemble these
particular hypergraphs from the mish-mash of input hypergraphs is to
use the query/forward-chainer to perform the assembly.  The result
of assembly will probably be a hypergraph that is sequestered in the
persistence (SQL) database. This hypergraph will be automatically
retrieved by the atomspace, using the "BackingStore" mechanism. After
identification and retrieval, the count on the count link needs to be
updated, and then saved back to the backing store.  At this point, its
acceptable to delete the count link structures, so as to save on RAM
usage.




=======================================================================


                        Lexical Attraction
                        ------------------
                     Linas Vepstas June 2008

This section provides a quick review of Yuret's defininition of 
"lexical attraction" [Yur1998]. 

An implementation of the Yuret "lexical attraction" algorithm has been
implemented in Java, as a part of the LexAt (aka "statistical relex")
package on the LaunchPad (canonical/ubuntu) website.  It is used for 
providing a parse-ranking score for link-grammar parses.  It has also 
been used to fish out a large number of MI values for word pairs.  Many
of these are quite interesting.


Data Structures
---------------
The Yuret algorithm requires, as input, the mutual information between
pairs of words. Computing the mutual information requires performing a
statistical analysis on a large corpus of sentences. The statistical
analysis needs to maintain four counts:
-- n(x,y) = the number of times a word pair was observed
-- n(x,*) = the number of times a word occurred as the left element of a
            word pair
-- n(*,y) = the number of times a word occurred as the right element of
            a word pair
-- n(*,*) = N = the number of word pairs observed.

The joint probability of seeing a word pair (x,y) is then

  p(x,y) = n(x,y) / n(*,*)

The marginal probabilities of seeing a word on the left, or right,
respectively, are

  p(x,*) = n(x,*) / n(*,*)
  p(*,y) = n(*,y) / n(*,*)

The "lexical attraction" is defined by Yuret as [chapter 4, page 40]:

   LA(x,y) = log_2 p(x,y) / (p(x,*) p(*,y))

Yuret calls this the "mutual information" for the word pair; however,
this usage, while in a certain sense is correct, is also in conflict
with the common definition of mutual information as the expectation
value of LA; that is, as:

  MI(X,Y) = E[LA(x,y)] = sum_{x \in X, y \in Y}  p(x,y) LA(x,y)

See for example, Wikipedia "mutual information" or [Ash, Chapter 1.5]

Note in particular that MI is non-negative, but LA can be negative.

Large positive LA corresponds to words that are seen together more often
than seen apart -- e.g. "Northern Ireland".  Large negative LA corresponds
to words that are infrequently, or never seen together: e.g. "a books"
the mismatch of the determiner "a" with the plural "books" means this
will be seen rarely, only as a typo -- so that p(x,y) is extremely small.  


                    Mutual Information Metric
                    -------------------------

Define (per [WP-MI], [Kra2003]) a mutual-information metric as

    d(X,Y) = H(X,Y) - I(X,Y)

and 
    D(X,Y) = d(X,Y) / H(X,Y)

Then D(X,Y) is a "universal metric", and is 0 <= D(X,Y) <= 1

Here, H(X,Y) = H(X) + H(Y) - I(X,Y) is the mutual entropy.

The "obvious" redefinition of this to the context of LA on word pairs 
is:
  I(X,Y) --> LA(x,y) = log p(x,y) - log p(x,*) - log p(*,y)
  H(X,Y) -->  h(x,y) = -log p(x,y)

  D(X,Y) --> 2 - log (p(x,*) p(*,y)) / log p(x,y)

but this redefinition/extension does not have the deisred properties.

However, S(x,y) = sqrt(log (p(x,*) p(*,y))) / log p(x,y)
does seem to have S=1 for hi LA, and S=0 for low LA ... ??

[WP-MI] Wikipedia- Mutual Information
        http://en.wikipedia.org/wiki/Mutual_information

[Kra2003] Alexander Kraskov, Harald Stögbauer, Ralph G. Andrzejak, and 
        Peter Grassberger, "Hierarchical Clustering Based on Mutual 
        Information", (2003) ArXiv q-bio/0311039

=======================================================================

                     USP Notes
                     ---------
              Linas Vepstas November 2009

Notes about the paper "Unsupervised Semantic Learning", by Hoifung Poon
and Pedro Domingos.  Notes are intended to lead to a possible
implementation within OpenCog, maybe.


Definition of Terms
===================

Example Sentence
----------------
Utah borders Idaho

Dependency parse:
-----------------
_subj(border, Utah) & _obj(border, Idaho)


QLF "Quasi-Logical Form":
-------------------------
reify the above into:
border(n1) & Utah(n2) & Idaho(n3) & subj(n1, n2) & obj(n1, n3)
(for feature tags, maybe also & definite(n2) & etc)

The ni are called "Skolem constants" (i.e. there exists n1 s.t....)

Lambda form:
------------
Partition graph by cutting, replace cut by lambda-bound variable.
L == lambda

Lx2.Lx3.border(n1) & subj(n1, x2) & obj(n1, x3)

Decompose parts using Davidsonian semantics:
--------------------------------------------
Core form:
----------
borders (n1)  contains no variables

Argument forms:
---------------
Lx2.subj(n1, x2)  each argument from contains only one variable.
Lx3.obj(n1, x3)

Argument type:
--------------
A cluster of argument forms. e.g containing dobj and nsubjpass,
which normalizes for semantic content of distinct syntactic
presentations. (just like in MTT!)

Semantic cluster:
-----------------
Collection of semantically equivalent lambda forms. All lambda
forms have same arity.  Cluster contains core forms and argument
types.

Abstract Lambda Form:
---------------------
Representationn of the semantic cluster as follows:
Let T(n) stand for core forms, let arity of cluster be k, let
Ai(n,xi) be the Argument type, then the ALF is

Lx1.Lx2... Lxk.T(n)A1(n,x1)A2(n,x2) ...A(n,xk)

Yes, of course binding order is important, that's the point of the
argument types.

MLN:
----
Partition QLF Q into parts. Assign each part to a semantic cluster.
Let L be a partition of Q. Then model joint probability distribution
given Q and L i.e.  probability Pr(Q,L) of pair (Q,L)

MLN predicates:
---------------
Distributions over lambda forms uses two MLN predicates:
Form (p, f!), ArgForm (p,i,f!)

p a part, i index, f a "QLF subformula" (actually, a "Davidsonian form")
i.e. f is a core form or arg form.

Form(p,f) is true iff p has core form f
ArgForm(p,i,f) is true iff ith arg of p has arg form f

Distribution over arguments uses three predicates:
ArgType(p,i,a!)
Arg(p,i,p')
Number(p,a,n)

Formulas:
---------
In(p,c) & Form(p,f)
ArgType(p,i,a) & ArgForm(p,i,f)
ArgType(p,i,a) & Arg(p,i,p') & In(p', c')
Number(p,a,n)

(p is member of some cluster c) and (p has some core form f)
(arg has some arg type) and (arg has some arg form)
(arg has some arg type) and (arg is some part) and (part is in a cluster)
(p has n args of arg type a)

Each formula has a distinct weight for each parameter f,c,a,p',c'
Have exponential prior weight on number of parameters to avoid
overfitting.

MAP Semantic parse
------------------
MAP=="Maximum a posteriori"
Most likely parse according to the weighting.

=======================================================================

                   A BindLink Proposal
                   ---------------------
                       November 2009

Various algorithms within OpenCog might be simplified if there was a 
lambda, in the sense of lambda calculus. Here's a proposal for what this
might look like:

      BindLink
         ListLink
            VariableNode "$word-a"
            VariableNode "$word-b"
         AndLink
            EvaluationLink
               VariableNode "$rel-l"
               ListLink
                  VariableNode "$word-head"
                  VariableNode "$word-a"
            EvaluationLink
               VariableNode "$rel-r"
               ListLink
                  VariableNode "$word-head"
                  VariableNode "$word-b"

Syntactically, the above is nothing more nor less than an extension of
the BindLink -- see http://opencog.org/wiki/BindLink

Another way to think of a BindLink is as a generalization of
EvaluationLink, but with the first and second args reversed -- i.e. the
ListLink appearing first, not last.


=======================================================================

Misc comments
-------------

A classic parse-ranking example is given in the following sentence.
Both parses are more or less legitimate, and have valid interpretations.

(S (NP I) (VP saw (NP the man) (PP with (NP the telescope))) .)

(S (NP I) (VP saw (NP (NP the man) (PP with (NP the telescope)))) .)



References
----------
See also probabilistic context-free grammars.
E. Charniak, 1997. Statistical parsing with a context-free grammar
and word statistics. AAAI 1997.

* Robert B. Ash, ''Information Theory'' (1965) Dover publications.
* [Yur1998] Deniz Yuret, ''Discovery of Linguistic Relations Using
  Lexical Attraction'' (1998) PhD Thesis
  http://citeseer.ist.psu.edu/yuret98discovery.html
* [Lin2001] Dekang Lin and Patrick Pantel, ''DIRT – Discovery of
  Inference Rules from Text'', KDD01

* [Poon2009] Hoifung Poon, Pedro Domingos, ''Unsupervised Semantic
  Parsing''

* MCL - a cluster algorithm for graphs http://www.micans.org/mcl/

* O'Donnell, Timothy J.; Tenenbaum, Joshua B.; Goodman, Noah D.
  Fragment Grammars: Exploring Computation and Reuse in Language
  http://dspace.mit.edu/handle/1721.1/44963
  http://hdl.handle.net/1721.1/44963

* http://chasen.org/~taku/software/freqt/
  FREQT: An implementation of FREQT (FREQuent Tree miner)
  The algorithm FREQT (FREQuent Tree miner), independently introduced by
  Asai[1] and Zaki[2], efficiently extracts frequent ordered-sub-trees
  from a set of ordered-trees (forest database). Frequent means that a
  sub-tree occurs in no less than N trees, where N is a user given
  threshold usually called minimum support. FREQT efficiently enumerates
  frequent sub-trees using right-most expansion.

* http://penglab.janelia.org/proj/mRMR/
  minimum redundancy/maximum relevance

* http://kzi.polsl.pl/~jbiesiada/Infosel/index.html
  Information Based Feature Selection C++ Library

