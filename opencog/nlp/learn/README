
                   Language Learning
                   -----------------
             Linas Vepstas December 2013

Current project, under construction.

Summary
-------
The goal of the project is to build a system that can learn parse
dictionaries for different languages, and possibly do some rudimentary
semantic extraction.  The primary design point is that the learning is
to be done in an unsupervised fashion.  A sketch of the theory that
enables this can be found in the paper "Language Learning", B. Goertzel
and L, Vepstas (2014) on ArXiv abs/1401.3372  An even shorter sketch is
given below.  Most of this README concerns the practical details of
operating the system, and some diary-like notes about partial and
preliminary results.

The basic algorithmic steps, as implemented so far, are as follows:
A) Ingest a lot of raw text, such as Wikipedia articles, and count the
   occurance of nearby word-pairs.
B) Compute the mutual information (mutual entropy) between the word-pairs.
C) Use a Minimum-Spanning-Tree algorithm to obtain provisional parses
   of sentences.  This requires ingesting a lot of raw text, again.
   (Independently of step A)
D) Extract linkage disjuncts from the parses, and count their frequency.
E) Use maximum-entropy principles to merge similar linkage disjuncts.
   This will also result in sets of similar words. Presumably, the 
   sets will roughly correspond to nouns, verbs, adjectives, and so-on.

Currently, the software implements steps A & B, and part of step C.
The automation of step C for large-scale parsing is not yet done. 
Steps D & E are planned. The remainder of step C and all of step D
are technically straight-forward.  The theoretical details step E are
incomplete; its not entirely clear what the best merging algorithm might
be, or how it will work.  Steps D and E are the new parts. Steps A-C have
been done before, and are "well known" in the literature.


System Processing Steps
-----------------------
Perform these steps to make the system go.

Processing is done with the cogserver; the large amount of data
generated requires storing it. (Millions of atoms will be generated.
Although these could fit into RAM, the data would be lost if the
cogserver crashes or is rebooted. Parsing will require weeks or 
months of CPU effort. Thus, SQL persistance is used to store the atoms.
So, set that up:

1) Set up and configure postgres, as described in 
   opencog/persist/sql/README
2) createdb learn-pairs
   cat atom.sql | psql learn-pairs
   Edit and setup ~/.odbc.ini
3) Start the cogserver, verify that connections to the database work.

Obtaining word-pair counts requires digesting a lot of text, and
counting the word-pairs that occur in the text.  The easiest way of
doing this, at the moment, is to parse the text with link-grammar and
RelEx, using the "any" language.  This pseudo-language will ink any
word to any other, and is simply a convenient way of extracting word
pairs from text.  Although this might seem to be a very concoluted way
of extracting word pairs, it actually "make sense", for two reasons:
a) it already works; little or no new code required. b) later processing
steps will require passing text through the link-grammar parser anyway,
so we may as well start using it right away. So, set that up:

4) Install link-grammar-4.8.4 or newer.
5) Copy the RelEx opencog-server.sh shell script to some other name.
   Edit it, and replace the final line with this:
   java $VM_OPTS $RELEX_OPTS $CLASSPATH relex.Server --lang any -n 99 --link
   Run the new shell script.

   The above tells relex to use the 'any' language, and return up to 99
   different parses. This should generate plenty of word-pairs.

6) Copy one of the opencog-??.conf files from the misc-scripts directory
   to the cogserver directory. Modify as desired.  (current experiments
   are being run on French, Polish and Lithuanian. Try other languages,
   if desired.

7) telnet localhost 17001
   opencog> sql-open learn-pairs linas asdf
   opencog> scm
   guile> (observe-text "this is a test")

   Better yet:
   echo -e "scm\n (observe-text \"this is a another test\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Bernstein () (1876\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Lietuvos žydų kilmės žurnalistas\")" |nc localhost 17001

   This should result in activity on the RelEx server, on the cogserver,
   and on the database: the "observe text" scheme code sends the text to
   RelEx for parsing, counts the returned word-pairs, and stores them in
   the database.

8) Verify that the above resulted in data sent to the SQL database. So:
   psql learn-pairs
   learn-pairs=# SELECT * from atoms;

   The above shows that the database now contains word-counts for
   pair-wise linkages for the above sentences.

9) Get ready to feed it text.  For example, parse wikipedia.
   a) Download lang-wiki-pages-articles for some lanugage lang.
   b) See 'Wikipedia processing' section below. Do the stuff there.
   c) See the section 'Sentence Splitting'. Set that up.
   d) See Wikipedia 'Processing, part II' below.

10) After doing at least several days worth of parsing  (i.e. at least
   hundreds of articles, tens of thousands of sentences) the word-pair
   mutual information can be extracted. Do the stuff in section "Mutual
   Information" section below.

11) MST parsing can now be started. See below.

That's all for now, folks!


Misc TODO
---------
* Remove the NLP_HACK from persist/sql/PersistModule.cc
* Optimize the scm scripts to not pound the database so hard.
  (How? Lots of word pushes, but really, there's not much repeat
  traffic ...) Not sure this is much of a problem, any more.

DONE
----
* Make sure that link-parser fully randomizes linkage choices for
  long sentences. Done. See the rand_state and predictable_rand
  flags in version 4.8.3.
* Need a mutli-language sentence splitter; the maxent splitter only
  works for English sentences.  Probably something simple will do ...
  Done: copied one from moses-smt and put it into a relex subdirectory
* Raw psql seems not to be utf8.  Were the tables created wrong,
  or is this a client bug? 
  Fixed: turned out to be a guile-related bug.
* Handle database write asynchronously: have a thread collect
  up the atoms, and write them. Maybe even multiple threads.
  This is OK due to the atomptr design.
  Done.  Not just one, but multiple writers.
* Can we start a guile thread for each incoming sentence?
  Threading will require more subtle sentence cleanup.
  No. The main guile bug, opened 5 years ago, is still not fixed.
* Table counts need to be more than 32-bit. Looks like the 
  any-language goes hog-wild and creates huge counts for just
  one sentence ... is this desired?  Anyway, they'll overflow...
  Done. atoms.sql now uses bigint 64-bit uuids.
* Investigate crash: while writing truuth value, stv_confidence 
  was pure virtual (circa line 920 of AtomStorage.cc) There's some
  kind of smart-pointer race. See "Crash" section below.
  Done. Ran for a month without issues.


Misc Notes about batch processing
----------------------------------
The relex parse is extreely fast, the cogserver part is extremely slow ... 
Non-linear, even.
4-word sentence: 53 parses, 3-4 seconds elapsed

5-word sentence, 373 parses,
32 second elapsed,
postgres runs at 36% cpu
cogserver runs at 56% cpu for a while then drops to 26% cpu

8-word sentence, max of 999 parses,
2 minutes elapsed.  Same CPU usage as above...

Solution: 999 parses are not needed.  Asorted tuning was done since
the above was written. 


Wikipedia processing
--------------------
Notes below are for four languages: French, Lithuanian, Polish, and
"Simple English".  Adjust as desired.

Set up distinct databases, one for each language:
    createdb fr_pairs lt_pairs pl_pairs simple_pairs
    cat opencog/persist/sql/atom.sql | psql
    vi .odbc.ini  learn-fr learn-lt learn-pl learn-simple

The wikipedia articles need to be scrubbed of stray wiki markup, so
that we send (mostly) plain text into the system.  There's a set of
cleanup files in the RelEx source distribution, in the src/perl
directory.  Use these as follows:

    cd /storage/wikipedia

    mkdir wiki-stripped
    time cat blahwiki.xml.bz2 |bunzip2 | /home/linas/src/relex/src/perl/wiki-scrub.pl
    cd wiki-stripped
    find |wc
    /home/linas/src/relex/src/perl/wiki-clean.sh
    find |wc
    cd ..
    mkdir alpha-pages
    cd alpha-pages
    /home/linas/src/relex/src/perl/wiki-alpha.sh

simple:
find |wc     gives 131434 total files
find |wc     gives 98825 files after cat/template removal.

lt:
7 mins to unpack
find |wc gives 190364 total files
find |wc gives 161463 after cat/template removal

pl:
1 hour to unpack (15 minutes each part)
find | wc gives 1097612 articles
52K are categories
35K are templates
find |wc gives 1007670 files after cat/template removal

fr:
3 hours to unpack (25-60 minutes per glob)
find |wc gives 1764813 articles
214K are categories
55K are templates
find |wc gives 1452739 files after cat/template removal

Wikipedia processing continues in Part II below.  But first, an interruption.


Sentence Splitting
------------------
Raw text needs to be split up into sentences.  Some distant future day,
opencog will do this automatically. For now, we hack it.

The sentence splitter needs to work for any language.
Maybe use NLTK ? ... OK, but not today, ... you can experiment.

Here's a simple one, easy-to-use:
https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes
Has french, polish, latvian
Its LGPL.  Copy this over into RelEx, for now. (Its now in the RelEx sources)

Verify that it works: e.g.

   cat /storage/wikipedia/ltwiki-20131216-pages-articles/alpha-pages/A/./Akiduobė | /home/linas/src/relex/src/split-sentences/split-sentences.pl -l lt > x

Some typical sentence-splitting concerns:
A question mark or exclamation mark always ends a sentence.  A period followed by an
upper-case letter generally ends a sentence, but there are a number of exceptions.  For
example, if the period is part of an abbreviated title ("Mr.", "Gen.", ...), it does not end a
sentence.  A period following a single capitalized letter is assumed to be a person's initial,
and is not considered the end of a sentence.


Wikipedia Parsing, part II
--------------------------
Now, build some working directories (so that we don't mess up
alpha-pages after all that work)

    cp -pr alpha-pages beta-pages

* Copy scripts from opencog/nlp/learn/misc-scripts into working dir.
* Modify as needed for chosen language. 
* Make sure the scripts point at the RelEx parse server.
  (The RelEx location is given in config-*.scm Make sure that its right,
  and that its loaded... i.e. is listed in opencog-*.conf)
* Copy opencog-lang.conf to opencog build dir.
  Start like this:    cogserver -c opencog-lang.conf 
* Go back to the wikipedia dir, and run ./wiki-ss-lang.sh
* Wait a few days.
* This requires postgres 8.4 or newer, for multiple reasons. One reason
  is that older postgres don't automatically VACUUM (see "performance"
  below) The other is the list membership functions are needed.
* Be sure to perform the postgres tuning recommendations found in 
  various online postgres peformance wikis, or in the 
  opencog/persist/sql/README file.  See also 'Performance' section
  below.

Some handy SQL commands:
SELECT count(uuid) FROM Atoms;
select count(uuid) from  atoms where type = 77;

type 77 is WordNode for me; verify with
SELECT * FROM Typecodes;


Mutual Information
------------------
After accumulating a few million word pairs, we're ready to compute
the mutual entropy between them.  This is done manually, by runnning
these scripts:

load:
   compute-mi.scm  Look at it. Go to the bottom, and run (do-em-all)

If tracing is turned on, a trace file is written to /tmp/progress.

Example stats and performance:
current fr_pairs db has 16785 words and 177960 pairs.

This takes 17K + 2x 178K = 370K total atoms loaded.
These load up in 10-20 seconds-ish or so.

New fr_pairs has 225K words, 5M pairs (10.3M atoms):
Load 10.3M atoms, which takes about 10 minutes cpu time to load
20-30 minutes wall-clock time (500K atoms per minute, 9K/second
on an overloaded server).

RSS for cogserver: 436MB, holding approx 370K atoms
So this is about 1.2KB per atom, all included. Atoms are a bit fat...
... loading all pairs is very manageable even for modest-sized machines.

RSS for cogserver: 10GB, holding 10.3M atoms
So this is just under 1KB per atom.

(By comparison, direct measurement of atom size i.e. class Atom:
typical atom size: 4820384 / 35444 = 136 Bytes/atom
this is NOT counting indexes, etc.)

Wild-card pair-count was taking about 30 per minute.  After extensive
bug fixing and tuning in the cogserver, its now running at hundreds
per minute (maybe 10x faster than before).  An additional round of
fixes, avoiding the pattern matcher, has added another 50x per
improvement.  The rate is strongly dependent on the size of the
dataset.

For dataset (fr_pairs) with 225K words, 5M pairs: 
Current rate is 150 words/sec or 9K words/min.

XXX !??? After wild-card count finshes, there is a 5563 second 
pause before batch counting starts. cogserver runs at 100%.
WTF is this? Garbage collection? Flushing of SQL queues?

After the single-word counts complete, and all-pair count is done.
This is fast, takes a couple of minutes.

Next: batch-logli takes 540 seconds for 225K words

Finally, an MI compute stage.
We are now up to about 250 per minute on wild-card, that's an 8x speedup!
That's on the small dataset, on the large dataset, that 8 per minute.
Ouch.  Rewrite w/o pattern matcher, get: 60 words/sec = 3.6K per minute
Thats another 500x speedup... 500x wow.  This rate is per-word, not
per word-pair.

... at first. Then there are long pauses, which seem to be related to
SQL queue flushing.  The rate then drops to about half at 30 words/sec
or 1.8K/min. And worse, dropping another 2x or 3x for a while.


Update Feb 2014: fr_pairs now contains 10.3M atoms
SELECT count(uuid) FROM Atoms;  gives  10324863 (10.3M atoms)
select count(uuid) from atoms where type = 77; gives  226030 (226K words)
select count(uuid) from atoms where type = 8;  gives 5050835 (5M pairs ListLink)
select count(uuid) from atoms where type = 27; gives 5050847 (5M pairs EvaluationLink)


XXX
single-word probs seem to be un-computed?



Next step: Minimum Spanning Tree parsing
----------------------------------------
Minimum spanning tree code is in mst-parser.scm

The MST parser discovers the minimum spanning tree that connects the
words together in a sentence.  The link-cost used is (minus) the mutual
information between word-pairs (so we are miximizing MI).

The algorithm implemented in mst-parser.scm works. 

Here's what's not yet implemented: The next step is to pick apart the
parse, back into link-grammar style disjuncts, and store these in the
atom space.  This is to be followed by a second parse phase, so that
we now MST-parse gobs and gobs of text.

One a bunch of statistics are accumulated for these super-fine-grained
disjuncts, the next step is to coarse-grain them, by clustering together
similar ones.  The goal is that clusters will auto-rediscover link-grammar
links. 


Some typical entropies for word-pairs
-------------------------------------
Three experiments:
1) Get H(de,*) H(*,de)  H(en,*) H(*,en) and compare to
   H(de+en,*) H(*, de+en)

2) H(vieux,*) H(* vieux) H(nouveaux, *) H(*, nouveaux)

3) H(vieille, *) etc + vieux

4) H(le, *) H(la,*)  vs. H(le+la)

5) H(le,*) H(famille,*) which should fail ...!?



Some typical entropies for word-pairs
-------------------------------------
The below is arithmeticaly correct, but theoretically garbage.

(WordNode "famille") entropy H=11.185

H(*, famille) = 11.195548 
H(famille, *) = 11.174561

MI(et, famille) = -5.2777815
H(et, *) = 5.5696678
P(et, *) = 0.021055372363972875

thus: 
H(et, famille) = -MI(et, famille) + H(famille, *) + H(et, *) = 22.0220103
P(et, famille) = 2.348087815164205e-7

MI(de, famille) = 2.1422486
H(de, *) = 4.3749881
P(de, *) = 0.04819448582223504

H(de, famille) = -2.1422486 + 4.3749881 + 11.195548 = 13.4282875
P(de, famille) = 9.071574511509601e-5

P(de+et. *) = 0.06924985818620791
H(de+et, *) = 3.8520450730427047

P(de+et, famille) = 9.095055389661243e-5
H(de+et, famille) = 13.424558050397735
MI(de+et, famille) = 1.6230350226449701

So  MI(et, famille) < MI(de+et, famille) < MI(de, famille)
       -5.2777815   <   1.6230350226     <   2.1422486

By contrast, the arithmetic average is:
(MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille)) /
    (P(de, famille) + P(et, famille))
  = 2.1230921666199825

Change in entropy:
MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille) =  0.0012169

MI(de+et, famille) * P(de+et, famille) = 1.476159343e-4

Oh, wait ... 
H(de, famille) * P(de, famille) + H(et, famille) * P(et, famille) =  0.001223328

H(de+et, famille) * P(de+et, famille) = 0.00122097099

Change in entropy = 0.00122097099 - 0.001223328 = -2.35701e-6

-------
H(de) = 4.3808608
H(et) = 5.5862331

P(de) = 0.04799870191172842
P(et) = 0.02081499323761464
P(de+et) = 0.06881369514934306
H(de+et) = 3.8611604742976153  = -log_2 (P(de)+P(et))
By contrast, the weighted average is

(P(de)*H(de) + P(et)*H(et)) /(P(de) + P(et)) = 4.745465784790553

Combinations:
  P(de+et)*H(de+et) = 0.2657007
  P(de)*H(de) + P(et)*H(et) = 0.32655303

The change in entropy, from forming a union, is:
  P(de+et)*H(de+et) - P(de)*H(de) - P(et)*H(et) = -0.060852316

Recap: Delta(de+et) = -0.060852316
       Delta(de+et, famille) = -2.35701e-6

Entropy increases (strongly) if word-pair merged, words are separated,

-------

MI(d'une, famille) = 5.230504
H(d'une, *) = 9.792551

H(la) = 5.6536283
H(la, *) = 5.5858526

sa  
est
de 
H(d'une) = 9.7960119
H(un) = 7.1578913
H(et) = 5.5862331

-----
repeat, for vielle+nouveaux
H(nouveaux) = 14.28815
P(nouveaux) = 4.998483553100357e-5

H(vieille) = 16.16037
P(vieille) = 1.365349e-5

P(nouveaux+vieille) = 6.363833e-5
H(nouveaux+vieille) = 13.93974
P(nouveaux+vieille)*H(nouveaux+vieille) = 8.87102088-4

P(nouveaux)*H(nouveaux) + P(vieille)*H(vieille) = 9.3483638e-4

Change in entropy is diff of the two: -4.7734297e-5


-----
repeat, for vielle+nouveaux
H(*, famille) = 11.195548 

H(nouveaux, *) = 13.974219
P(nouveaux, *) = 6.213565989765264e-5
MI(nouveaux, famille) = 5.2966957
H(nouveaux, famille) = 19.8730713
P(nouveaux, famille) = 1.0413804797188067e-6

H(vieille, *) = 15.998603
P(vieille, *) = 1.5273571710064995e-5
MI(vieille, famille) = 10.195547
H(vieille, famille) = 16.998604
P(vieille, famille) = 7.636780561617735e-6

P(vieille+nouveaux, famille) = 8.678161041336542e-6
H(vieille+nouveaux, famille) = 16.814179210712517

P(vieille+nouveaux, *) = 7.740923160771763e-5
H(vieille+nouveaux, *) = 13.657134846045357

MI(vieille+nouveaux, famille) = 8.038503635332841

so MI(nouveaux, famille) < MI(vieille+nouveaux, famille) < MI(vieille, famille)
         5.2966957       <         8.038503635332841     <       10.195547

Change in entropy:
P(nouveaux, famille)*H(nouveaux, famille)  + P(vieille, famille)*H(vieille, famille)
    = 1.505100371e-4

P(vieille+nouveaux, famille) * H(vieille+nouveaux, famille) = 1.459161549e-4

Change = 1.459161549e-4 - 1.505100371e-4 = -4.5938821e-6

To recap: Delta(vieille+nouveaux) = -4.7734297e-5
               reduces the entropy more than
          Delta(vieille+nouveaux, famille) = -4.5938821e-6

i.e. entropy increses if the word-pairs are merged, the words are separated.


---------------

Total entropy:
(fold + 0 (map (lambda (atom)
	(let ((ent (tv-conf (cog-tv atom))))
	(* ent (exp (* (- ent) (log 2))))))
	(cog-get-atoms 'WordNode)))

gives: 7.2199274514956



======================================================================
Minimal morphology output


;; Lets say that there was one word in the sentence, it was 'foobar'
;; and the splitter split it into foo and bar
;; then the following should be generated:

;; for each sentence, create one of these, each with a distinct uuid:
(ParseLink (stv 1 1)
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
   (SentenceNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9")
)

;; For each pair of morphemes, cereate the below:
(EvaluationLink (stv 1.0 1.0)
   (LinkGrammarRelationshipNode "MOR")
   (ListLink
      (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
      (WordInstanceNode "bar@cb2443bb-fbec-472c-baee-36b822579861")
   )
)

;; For each "word" aka morpheme, create these two clauses:
;; note that the UUID's match up exactly with the above.
;; the below shows only "foo", another pair is needed for "bar".
(ReferenceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (WordNode "foo")
)
(WordInstanceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
)


;; finally, at the very end:
;; again, the UUID must match with what was given above.

(ListLink (stv 1 1)
   (AnchorNode "# New Parsed Sentence")
   (SentenceNode "sentence@68e51cae-98bc-4102-b19c-78649c5f6cfb")
)

======================================================================
Tagalog status:

31 july 2014
4519631 = 4.5M morpehem pairs
204K morpehemes

   62 | LinkGrammarRelationshipNode
select * from atoms where type=62;

select * from atoms order by stv_confidence;

{60,5592759}
{60,4941961}
{60,1710609}
17720161
7337858
2586082
16810147
9548076
3844852

select * from atoms where stv_count > 100 order by stv_confidence;

1217445

1043142


select * from atoms where uuid=5592759;

{223,223}
236
541






======================================================================
Crashes and bugs


Crash 2
-------
All new crash. Note:
1) This is with the old scheme singleton-instance.
2) This one received when defining a function with an unbound variable
   in it.  It should have just been a standard stack trace, but wasn't.
3) Sometimes, when doing this, and parser is running, I get 

[2013-12-26 22:50:57:680] [ERROR] Caught signal 11 (Segmentation fault) on thread 47486186035520
        Stack Trace:
        2: CogServerMain.cc:92  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ??:0   std::string::append(std::string const&)
        5: basic_string.h:290     std::string::_M_data() const
        6: SchemeEval.cc:381      opencog::SchemeEval::c_wrap_eval(void*)
line 381 is  self->answer = self->do_eval(*(self->pexpr));
So maybe p is null, or self->pexpr is null..
...
        7: continuations.c:511  c_body()
        8: vm-i-system.c:855    vm_regular_engine()
        9: eval.c:508   scm_call_4()
        10: continuations.c:456 scm_i_with_continuation_barrier()
        11: continuations.c:550 scm_c_with_continuation_barrier()
        12: pthread_support.c:1272      GC_call_with_gc_active()
        13: threads.c:937       with_guile_and_parent()
        14: misc.c:1835 GC_call_with_stack_base()
        15: threads.c:959       scm_with_guile()
        16: SchemeEval.cc:372     opencog::SchemeEval::eval(std::string const&)
        17: basic_string.h:536  ~basic_string()
        18: GenericShell.cc:157   opencog::GenericShell::eval(std::string const&, opencog::ConsoleSocket*)
        19: ConsoleSocket.cc:232          opencog::ConsoleSocket::OnLine(std::string const&)
        20: basic_string.h:536  ~basic_string()
        21: thread.cpp:0        thread_proxy()
        22: pthread_create.c:0  start_thread()
        23: ??:0        __clone()

again 7 Jan 2014 but this time its different:

[2014-01-07 00:58:32:234] [ERROR] Caught signal 11 (Segmentation fault) on threa
d 47389012408640
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: vm-i-system.c:887    vm_regular_engine()
        5: eval.c:508   scm_call_4()
        6: backtrace.c:163      scm_display_error()
        7: inline.h:131 scm_puts()
        8: vm-i-system.c:855    vm_regular_engine()
        9: eval.c:508   scm_call_4()
        10: SchemeEval.cc:433     opencog::SchemeEval::do_eval(std::string const&)
        11: SchemeEval.cc:392     opencog::SchemeEval::c_wrap_eval(void*)
        12: continuations.c:511 c_body()
        13: vm-i-system.c:855   vm_regular_engine()
        14: eval.c:508  scm_call_4()

19 Jan 2014: again: same but different:

[2014-01-19 06:06:32:335] [ERROR] Caught signal 11 (Segmentation fault)
on threa
d 47820857940288
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ??:0 strlen()  
        5: strings.c:1518       scm_from_stringn()
        6: strports.c:517       scm_c_eval_string()
        7: vm-i-system.c:855    vm_regular_engine()
        8: eval.c:508   scm_call_4()
        9: SchemeEval.cc:433    opencog::SchemeEval::do_eval(std::string const&)
        10: SchemeEval.cc:392   opencog::SchemeEval::c_wrap_eval(void*)

Got exactly above, again 21 Jan. 

Preceeded a few minutes earlier by below.

opencog-fr> Backtrace:
In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 25dd04e0> ...]
In unknown file:
   ?: 5 [apply-smob/1 #<catch-closure 25dd04e0>]
In ice-9/boot-9.scm:
 157: 4 [catch #t #<catch-closure 25d2ff20> ...]
In unknown file:
   ?: 3 [apply-smob/1 #<catch-closure 25d2ff20>]
   ?: 2 [call-with-input-string "(observe-text \"Hvozdnica est un village de Slovaquie situé dans la région de \u017dilina.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure cfebf00 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error ...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure 25d2fee0> encoding-error ...]
ERROR: In procedure apply-smob/1:
ERROR: In procedure scm_to_stringn: Error while printing exception.
ABORT: encoding-error
Backtrace:
In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 174c8ac0> ...]
In unknown file:
   ?: 5 [apply-smob/1 #<catch-closure 174c8ac0>]
In ice-9/boot-9.scm:
 157: 4 [catch #t #<catch-closure ac0b5e0> ...]
In unknown file:
   ?: 3 [apply-smob/1 #<catch-closure ac0b5e0>]
   ?: 2 [call-with-input-string "(observe-text \"Hvozdnica est un village de Slovaquie situé dans la région de \u017dilina.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure a82f4c0 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error ...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure ac0b5a0> encoding-error ...]

ERROR: In procedure apply-smob/1:
ERROR: In procedure scm_to_stringn: Error while printing exception.
ABORT: encoding-error


Crash 6
-------
All new 21 jan 2014

[2014-01-21 20:30:03:348] [ERROR] Caught signal 11 (Segmentation fault) on thread 47570270030144
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ??:0   std::back_insert_iterator<std::vector<opencog::Handle, std::allocator<opencog::Handle> > > opencog::Atom::getIncomingSet<std::back_insert_iterator<std::vector<opencog::Handle, std::allocator<opencog::Handle> > > >(std::back_insert_iterator<std::vector<opencog::Handle, std::allocator<opencog::Handle> > >)
        5: AtomSpaceImpl.cc:402 opencog::AtomSpaceImpl::getIncoming(opencog::H
andle)
        6: ??:0   opencog::AtomSpace::getIncoming(opencog::Handle)
        7: SchemeSmobNew.cc:538   opencog::SchemeSmob::ss_delete(scm_u

Again, same exact trace:
[2014-01-22 07:42:21:675] [ERROR] Caught signal 11
* Again, same exact trace .. again, only for french. Why is french different???
* Again, same exact trace ..  22 Jan
* Again, same exact trace ..  23 Jan  Only in french ... wtf ... 
* Again, same exact trace ..  23 Jan  Only in french ... wtf ... 
* Again, same exact trace ..  24 Jan  Only in french ... wtf ... 
* Again, handle=480036842 ptr is null .. why?


But this time without the weirdo message:
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'
  what():  boost shared_lock has no mutex: Operation not permitted

WTF ... where is the above shared_lock coming from????



Crash 7
-------
[2014-01-22 10:18:24:103] [ERROR] Caught signal 11 (Segmentation fault)
on thread 47155674880320
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ConsoleSocket.cc:211 opencog::ConsoleSocket::OnLine(std::string const&)





Irritation 4
------------
Occasionally see this: 

In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 294a63c0> ...]

   ?: 3 [apply-smob/1 #<catch-closure 294a62a0>]
   ?: 2 [call-with-input-string "(observe-text \"Cheopso piramid\u0117, arba Did
\u017eioji Gizos piramid\u0117 \u2013 Egipto piramid\u0117, faraono Cheopso kapa
s.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure 193e5c80 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encodi
ng-error ...]

-----------
another:
 "(observe-text \"Andrzej G\u0105siorowski i Krzysztof Steyer, Tajna Organizacja Wojskowa Gryf Pomorski, Polnord Wydawnictwo Oskar, Gda\u0144sk 2010 r.\")\n" .

Andrzej Gąsiorowski i Krzysztof Steyer, Tajna Organizacja Wojskowa Gryf Pomorski, Polnord Wydawnictwo Oskar, Gdańsk 2010 r.

In ice-9/boot-9.scm:
 102: 1 [#<procedure 3db5240 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error ...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure 3be53c40> encoding-error ...]

ABORT: encoding-error

ERROR: In procedure scm_to_stringn: Error while printing exception.



Performance
-----------
Performance seems to suck: 
-- two parsers, each takes maybe 4% cpu time total. Load avg of about 0.03
-- each parser runs 4 async write threads pushing atoms to postgres. 
   each one complains about it taking too long to flush the write queues.
-- postmaster is running 10 threads, load-avg of about 2.00  so about
   2 cpu's at 100%
-- vmstat shows 500 blks per second written. This is low...
-- top shows maybe 0.2% wait state. So its not disk-bound.
-- what is taking so long?

So, take a tcpdump:
-- a typical tcpdump packet:
   UPDATE Atoms SET tv_type = 2, stv_mean = 0 , stv_confidence = 0, stv_count = 54036 WHERE uuid = 367785;
   its maybe 226 bytes long.
-- this gets one response from server, about 96 bytes long.
-- then one more req, one more repsonse, seems to be a 'were'done' mesg
   or something ...  which I guess is due to SQLFreeHandle(SQL_HANDLE_STMT ???
-- time delta in seconds, of tcpdump of traffic packets, between update, and 
   response from server:
   0.0006  0.0002 0.0002 0.0002 0.028 (yow!!) 0.001 0.0002

-- so it looks like about every 8-10 packets are replied to failry quick,
   then there's one that takes 0.025 seconds to reply.... stairsteps in
   response time like this all the way through the capture.

Wild guess:
-- Hmm ... this seems to be related to the commit delay in postgresql.conf
   Change commit_delay to 1 second
   change wal_bufers to 32MB since its mostly update traffic.
   change checkpoint_segments to 32 (each one takes up 16MB of disk space.)

-- Making these changes has no obvious effect ... bummer.

I don't get it; performance sucks and I don't see why.  Or rather: postmaster
is chewing up vast amounts of cpu time for no apparent reason...


select * from pg_stat_user_tables;
select * from pg_stat_all_tables;
select * from pg_statio_user_tables;
select * from pg_database;

pg_stat_user_indexes
pg_stat_all_indexes

select * from pg_catalog.pg_stat_activity;
select * from pg_catalog.pg_locks;


-- WOW!!!   VACUUM ANALYZE; had a huge effect!!

-- vacuum tells em to do following:
   change max_fsm_pages to 600K
   chage max_fsm_relations to 10K

Anyway ... performance measured as of 27 Dec 2013:

Takes about 105 millisecs to clear 90 eval-links from the write-back
queues. This each eval-link is 5 atoms (eval, defind, list, word, word)
so this works out to 5*90 atoms /0.105 seconds = 4.3KAtoms/sec 
which is still pretty pathetic... 



Misc 
----
Change load-atoms to return a count?

gdb:
---
handle SIGPWR nostop noprint
handle SIGXCPU nostop noprint


How about using a reader-writer lock?
----------------------------------

boost::shared_lock  for reading,
unique_lock for writing ... 

upgrade_lock<shared_mutex> lock(workerAccess); 
        upgrade_to_unique_lock<shared_mutex> uniqueLock(lock);



shared_mutex 
write uses:  unique_lock<shared_mutex>
readers use shared_lock<shared_mutex>

writer does:

  // get upgradable access
  boost::upgrade_lock<boost::shared_mutex> lock(_access);

  // get exclusive access
  boost::upgrade_to_unique_lock<boost::shared_mutex> uniqueLock(lock);
  // now we have exclusive access
}

am using boost-1.49 on cray

------------------------------------------

