
                   Language Learning
                   -----------------
             Linas Vepstas December 2013
                 Updated January 2017

Current project, under construction.  See the [language learning wiki]
(http://wiki.opencog.org/w/Language_learning)
for an alternate overview.

Summary
-------
The goal of the project is to build a system that can learn parse
dictionaries for different languages, and possibly do some rudimentary
semantic extraction.  The primary design point is that the learning is
to be done in an unsupervised fashion.  A sketch of the theory that
enables this can be found in the paper "Language Learning", B. Goertzel
and L. Vepstas (2014) on [ArXiv abs/1401.3372](https://arxiv.org/abs/1401.3372)
A shorter sketch is given below.  Most of this README concerns the
practical details of configuring and operating the system, and some
diary-like notes about system configurtion and operation. A diary of
scientific notes and results is in the `learn-lang-diary` directory.

The basic algorithmic steps, as implemented so far, are as follows:
A) Ingest a lot of raw text, such as Wikipedia articles, and count the
   occurance of nearby word-pairs.
B) Compute the mutual information (mutual entropy) between the word-pairs.
C) Use a Minimum-Spanning-Tree algorithm to obtain provisional parses
   of sentences.  This requires ingesting a lot of raw text, again.
   (Independently of step A)
D) Extract linkage disjuncts from the parses, and count their frequency.
E) Use maximum-entropy principles to merge similar linkage disjuncts.
   This will also result in sets of similar words. Presumably, the 
   sets will roughly correspond to nouns, verbs, adjectives, and so-on.

Currently, the software implements steps A & B, and part of step C.
The automation of step C for large-scale parsing is not yet done. 
Steps D & E are planned. The remainder of step C and all of step D
are technically straight-forward.  The theoretical details step E are
incomplete; its not entirely clear what the best merging algorithm might
be, or how it will work.  Steps D and E are the new parts. Steps A-C have
been done before, and are "well known" in the literature.

All of the statistics gathering is done within the OpenCog AtomSpace,
where counts and other statisical quantites are associated with various
different hypergraphs.  The contents of the atomspace are saved to an
SQL (Postgres) server for storage.  The system is fed with raw text
using assorted ad-hoc scripts, which include both link-grammar and
RelEx as central components of the processing pipeline. Most of the
data analysis is performed with an assortment of scheme scripts.

Thus, operating the sytem requires three basic steps:
* Setting up the atomspace with the SQL backing store,
* Setting up the misc scripts to feed in raw text, and
* Processing the data after it has been collected.

Each of these is described in greater detail in separate sections below.


Setting up the AtomSpace
------------------------
This section describes how to set up the atomspace to collect
statistics.  Most of this revolves around setting up postgres.

1) Set up and configure postgres, as described in 
   `opencog/persist/sql/README.md`

2) Create and initialize the database
```
   createdb learn-pairs
   cat atom.sql | psql learn-pairs
```

3) Start the cogserver, verify that connections to the database work.
   (see steps 6,7,8 below, for more on how to do this.)

Obtaining word-pair counts requires digesting a lot of text, and
counting the word-pairs that occur in the text.  The easiest way of
doing this, at the moment, is to parse the text with link-grammar and
RelEx, using the "any" language.  This pseudo-language will link any
word to any other, and is simply a convenient way of extracting word
pairs from text.  Although this might seem to be a very convoluted way
of extracting word pairs, it actually "make sense", for two reasons:
a) it already works; little or no new code required. b) later processing
steps will require passing text through the link-grammar parser anyway,
so we may as well start using it right away. So, set that up:

4) Install `link-grammar-5.3.15` or newer.
5) Copy the RelEx opencog-server.sh shell script to some other name.
   Edit it, and replace the final line with this:
```
   java $VM_OPTS $RELEX_OPTS $CLASSPATH relex.Server --lang any -n 16 --link
```
   Run the new shell script.

   The above tells relex to use the 'any' language, and return up to 16
   different parses. This should generate plenty of word-pairs.

6) Create/edit the `~/.guile` file and add the following content:
```
   (use-modules (ice-9 readline))
   (activate-readline)
   (debug-enable 'backtrace)
   (read-enable 'positions)
   (add-to-load-path "/usr/local/share/opencog/scm")
   (add-to-load-path ".")
```

7) Copy one of the `opencog-??.conf` files from the `misc-scripts`
   directory to your working directory. Modify as needed. This file
   specifies the cogserver port number, and the prompt to use; this
   can be changed so that it does not conflict with other cogservers
   that might be running at the same time.

8) Chose the corresponding `learn-pairs-??.scm` file, and review and
   edit the configuration as necessary. It contains the database 
   credentials.

9) Start the cogserver, as follows:
```
   guile -l learn-pairs-??.scm
```
   There are other ways of starting the cogserver; the above is handy.

10) Verify that the language processing pipeline works:
```
   telnet localhost 17002  # or wherever your cogserver is configured.
   guile> (observe-text "this is a test")
```

   Better yet:
```
   echo -e "scm\n (observe-text \"this is a another test\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Bernstein () (1876\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Lietuvos žydų kilmės žurnalistas\")" |nc localhost 17001
```

   This should result in activity on the RelEx server, on the cogserver,
   and on the database: the "observe text" scheme code sends the text to
   RelEx for parsing, counts the returned word-pairs, and stores them in
   the database.

8) Verify that the above resulted in data sent to the SQL database. So:
```
   psql learn-pairs
   learn-pairs=# SELECT * FROM atoms;
   learn-pairs=# SELECT COUNT(*) FROM atoms;
```
   The above shows that the database now contains word-counts for
   pair-wise linkages for the above sentences.

9) Get ready to feed it text.  For example, parse wikipedia.
   a) Download lang-wiki-pages-articles for some lanugage lang.
   b) See 'Wikipedia processing' section below. Do the stuff there.
   c) See the section 'Sentence Splitting'. Set that up.
   d) See Wikipedia 'Processing, part II' below.

10) After doing at least several days worth of parsing  (i.e. at least
   hundreds of articles, tens of thousands of sentences) the word-pair
   mutual information can be extracted. Do the stuff in section "Mutual
   Information" section below.

11) MST parsing can now be started. See below.

That's all for now, folks!


Misc TODO
---------
* Remove the NLP_HACK from persist/sql/PersistModule.cc
* Optimize the scm scripts to not pound the database so hard.
  (How? Lots of word pushes, but really, there's not much repeat
  traffic ...) Not sure this is much of a problem, any more.

DONE
----
* Make sure that link-parser fully randomizes linkage choices for
  long sentences. Done. See the rand_state and predictable_rand
  flags in version 4.8.3.
* Need a mutli-language sentence splitter; the maxent splitter only
  works for English sentences.  Probably something simple will do ...
  Done: copied one from moses-smt and put it into a relex subdirectory
* Raw psql seems not to be utf8.  Were the tables created wrong,
  or is this a client bug? 
  Fixed: turned out to be a guile-related bug.
* Handle database write asynchronously: have a thread collect
  up the atoms, and write them. Maybe even multiple threads.
  This is OK due to the atomptr design.
  Done.  Not just one, but multiple writers.
* Can we start a guile thread for each incoming sentence?
  Threading will require more subtle sentence cleanup.
  No. The main guile bug, opened 5 years ago, is still not fixed.
* Table counts need to be more than 32-bit. Looks like the 
  any-language goes hog-wild and creates huge counts for just
  one sentence ... is this desired?  Anyway, they'll overflow...
  Done. atoms.sql now uses bigint 64-bit uuids.
* Investigate crash: while writing truuth value, stv_confidence 
  was pure virtual (circa line 920 of AtomStorage.cc) There's some
  kind of smart-pointer race. See "Crash" section below.
  Done. Ran for a month without issues.


Misc Notes about batch processing
----------------------------------
The relex parse is extreely fast, the cogserver part is extremely slow ... 
Non-linear, even.
4-word sentence: 53 parses, 3-4 seconds elapsed

5-word sentence, 373 parses,
32 second elapsed,
postgres runs at 36% cpu
cogserver runs at 56% cpu for a while then drops to 26% cpu

8-word sentence, max of 999 parses,
2 minutes elapsed.  Same CPU usage as above...

Solution: 999 parses are not needed.  Asorted tuning was done since
the above was written. 


Wikipedia processing
--------------------
Notes below are for four languages: French, Lithuanian, Polish, and
"Simple English".  Adjust as desired.

Download wikipedia database dumps:
https://dumps.wikimedia.org/ltwiki/
https://dumps.wikimedia.org/frwiki/
https://dumps.wikimedia.org/zhwiki/ (Mandarin)
https://dumps.wikimedia.org/zh_yuewiki/ (Cantonese)

The wikipedia articles need to be scrubbed of stray wiki markup, so
that we send (mostly) plain text into the system.  There's a set of
cleanup files in the RelEx source distribution, in the src/perl
directory.  Use these as follows:
```
    cd /storage/wikipedia

    mkdir wiki-stripped
    time cat blahwiki.xml.bz2 |bunzip2 | /home/linas/src/relex/src/perl/wiki-scrub.pl
    cd wiki-stripped
    find |wc
    /home/ubuntu/src/relex/src/perl/wiki-clean.sh
    find |wc
    cd ..
    mkdir alpha-pages
    cd alpha-pages
    /home/ubuntu/src/relex/src/perl/wiki-alpha.sh
```

simple:
find |wc     gives 131434 total files
find |wc     gives 98825 files after cat/template removal.

lt:
7 mins to unpack
find |wc gives 190364 total files
find |wc gives 161463 after cat/template removal

pl:
1 hour to unpack (15 minutes each part)
find | wc gives 1097612 articles
52K are categories
35K are templates
find |wc gives 1007670 files after cat/template removal

fr:
3 hours to unpack (25-60 minutes per glob)
find |wc gives 1764813 articles
214K are categories
55K are templates
find |wc gives 1452739 files after cat/template removal

Wikipedia processing continues in Part II below.  But first, an interruption.


Sentence Splitting
------------------
Raw text needs to be split up into sentences.  Some distant future day,
opencog will do this automatically. For now, we hack it.

The sentence splitter needs to work for any language.
Maybe use NLTK ? ... OK, but not today, ... you can experiment.

Here's a simple one, easy-to-use:
https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes
Has french, polish, latvian, more.  Its LGPL. It has been copied over
into RelEx, for now. (Its should be here, I guess, not in relex!?)

Verify that it works: e.g.
```
   cat /storage/wikipedia/ltwiki-20131216-pages-articles/alpha-pages/A/./Akiduobė | /home/linas/src/relex/src/split-sentences/split-sentences.pl -l lt > x
```
Some typical sentence-splitting concerns:
A question mark or exclamation mark always ends a sentence.  A period
followed by an upper-case letter generally ends a sentence, but there
are a number of exceptions.  For example, if the period is part of an
abbreviated title ("Mr.", "Gen.", ...), it does not end a sentence.
A period following a single capitalized letter is assumed to be a
person's initial, and is not considered the end of a sentence.


Wikipedia Parsing, part II
--------------------------
Set up distinct databases, one for each language:
    createdb fr_pairs lt_pairs pl_pairs simple_pairs
    cat opencog/persist/sql/odbc/atom.sql | psql foobar_pairs
    vi learn-fr learn-lt learn-pl learn-simple

Now, build some working directories (so that we don't mess up
alpha-pages after all that work)
```
    cp -pr alpha-pages beta-pages
```
* Review the README file in the `opencog/nlp/learn/misc-scripts`
  directory, and then copy the scripts into your working dir.
* Modify as needed for chosen language. Most of these don't need
  modification:
  -- `run-all-servers.sh` runs (almost) all of the servers, at once,
     in a multi-paneled byobu window. Edit this, and manually configure
     the relex port-number to use. After starting, the batch script
     `./wiki-ss-en.sh` needs to be started by hand.

  -- `relex-server-any.sh` performs random-syntax parsing for languages
     that do not need morphological analysis. Edit this, and verify that
     the relex server is running on the desired port number.

  -- `wiki-ss-en.sh` performs batch processing of wikipedia articles.
     Edit this, and set the correct language and cogserver port number.
     This calls `ss-one.sh` to process one wikipedia article.

  -- `ss-one.sh` calls `split-sentences.pl` to split the article into
     sentences, and then calls `submit-one.pl` to send the sentences
     to relex for parsing. None of these should need any modification.

* Make sure the scripts point at the RelEx parse server.
  (The RelEx location is given in config-*.scm Make sure that its right,
  and that its loaded... i.e. is listed in opencog-*.conf)
* Copy `opencog-lang.conf` and `pair-count-lang.scm` to opencog build dir.
  Start like this:    `guile -l pair-count-lang.scm`
* Go back to the wikipedia dir, and run `./wiki-ss-lang.sh`
* Wait a few days.
* This requires postgres 8.4 or newer, for multiple reasons. One reason
  is that older postgres don't automatically VACUUM (see "performance"
  below) The other is the list membership functions are needed.
* Be sure to perform the postgres tuning recommendations found in 
  various online postgres peformance wikis, or in the 
  `opencog/persist/sql/README.md` file.  See also 'Performance' section
  below.

Some handy SQL commands:
```
SELECT count(uuid) FROM Atoms;
select count(uuid) from  atoms where type = 77;
```

type 77 is WordNode for me; verify with
```
SELECT * FROM Typecodes;
```


Mutual Information
------------------
After accumulating a few million word pairs, we're ready to compute
the mutual entropy between them.  This is done manually, by runnning
these scripts:

load:
   compute-mi.scm  Look at it. Go to the bottom, and run (do-em-all)

If tracing is turned on, a trace file is written to /tmp/progress.

Example stats and performance:
current fr_pairs db has 16785 words and 177960 pairs.

This takes 17K + 2x 178K = 370K total atoms loaded.
These load up in 10-20 seconds-ish or so.

New fr_pairs has 225K words, 5M pairs (10.3M atoms):
Load 10.3M atoms, which takes about 10 minutes cpu time to load
20-30 minutes wall-clock time (500K atoms per minute, 9K/second
on an overloaded server).

RSS for cogserver: 436MB, holding approx 370K atoms
So this is about 1.2KB per atom, all included. Atoms are a bit fat...
... loading all pairs is very manageable even for modest-sized machines.

RSS for cogserver: 10GB, holding 10.3M atoms
So this is just under 1KB per atom.

(By comparison, direct measurement of atom size i.e. class Atom:
typical atom size: 4820384 / 35444 = 136 Bytes/atom
this is NOT counting indexes, etc.)

Wild-card pair-count was taking about 30 per minute.  After extensive
bug fixing and tuning in the cogserver, its now running at hundreds
per minute (maybe 10x faster than before).  An additional round of
fixes, avoiding the pattern matcher, has added another 50x per
improvement.  The rate is strongly dependent on the size of the
dataset.

For dataset (fr_pairs) with 225K words, 5M pairs: 
Current rate is 150 words/sec or 9K words/min.

XXX !??? After wild-card count finshes, there is a 5563 second 
pause before batch counting starts. cogserver runs at 100%.
WTF is this? Garbage collection? Flushing of SQL queues?

After the single-word counts complete, and all-pair count is done.
This is fast, takes a couple of minutes.

Next: batch-logli takes 540 seconds for 225K words

Finally, an MI compute stage.
We are now up to about 250 per minute on wild-card, that's an 8x speedup!
That's on the small dataset, on the large dataset, that 8 per minute.
Ouch.  Rewrite w/o pattern matcher, get: 60 words/sec = 3.6K per minute
Thats another 500x speedup... 500x wow.  This rate is per-word, not
per word-pair.

... at first. Then there are long pauses, which seem to be related to
SQL queue flushing.  The rate then drops to about half at 30 words/sec
or 1.8K/min. And worse, dropping another 2x or 3x for a while.


Update Feb 2014: fr_pairs now contains 10.3M atoms
SELECT count(uuid) FROM Atoms;  gives  10324863 (10.3M atoms)
select count(uuid) from atoms where type = 77; gives  226030 (226K words)
select count(uuid) from atoms where type = 8;  gives 5050835 (5M pairs ListLink)
select count(uuid) from atoms where type = 27; gives 5050847 (5M pairs EvaluationLink)


XXX
single-word probs seem to be un-computed?



Next step: Minimum Spanning Tree parsing
----------------------------------------
Minimum spanning tree code is in `mst-parser.scm`

The MST parser discovers the minimum spanning tree that connects the
words together in a sentence.  The link-cost used is (minus) the mutual
information between word-pairs (so we are miximizing MI).

The algorithm implemented in mst-parser.scm works. 

Here's what's not yet implemented: The next step is to pick apart the
parse, back into link-grammar style disjuncts, and store these in the
atom space.  This is to be followed by a second parse phase, so that
we now MST-parse gobs and gobs of text.

One a bunch of statistics are accumulated for these super-fine-grained
disjuncts, the next step is to coarse-grain them, by clustering together
similar ones.  The goal is that clusters will auto-rediscover link-grammar
links. 


Some typical entropies for word-pairs
-------------------------------------
Three experiments:
1) Get H(de,*) H(*,de)  H(en,*) H(*,en) and compare to
   H(de+en,*) H(*, de+en)

2) H(vieux,*) H(* vieux) H(nouveaux, *) H(*, nouveaux)

3) H(vieille, *) etc + vieux

4) H(le, *) H(la,*)  vs. H(le+la)

5) H(le,*) H(famille,*) which should fail ...!?



Some typical entropies for word-pairs
-------------------------------------
The below is arithmeticaly correct, but theoretically garbage.

(WordNode "famille") entropy H=11.185

H(*, famille) = 11.195548 
H(famille, *) = 11.174561

MI(et, famille) = -5.2777815
H(et, *) = 5.5696678
P(et, *) = 0.021055372363972875

thus: 
H(et, famille) = -MI(et, famille) + H(famille, *) + H(et, *) = 22.0220103
P(et, famille) = 2.348087815164205e-7

MI(de, famille) = 2.1422486
H(de, *) = 4.3749881
P(de, *) = 0.04819448582223504

H(de, famille) = -2.1422486 + 4.3749881 + 11.195548 = 13.4282875
P(de, famille) = 9.071574511509601e-5

P(de+et. *) = 0.06924985818620791
H(de+et, *) = 3.8520450730427047

P(de+et, famille) = 9.095055389661243e-5
H(de+et, famille) = 13.424558050397735
MI(de+et, famille) = 1.6230350226449701

So  MI(et, famille) < MI(de+et, famille) < MI(de, famille)
       -5.2777815   <   1.6230350226     <   2.1422486

By contrast, the arithmetic average is:
(MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille)) /
    (P(de, famille) + P(et, famille))
  = 2.1230921666199825

Change in entropy:
MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille) =  0.0012169

MI(de+et, famille) * P(de+et, famille) = 1.476159343e-4

Oh, wait ... 
H(de, famille) * P(de, famille) + H(et, famille) * P(et, famille) =  0.001223328

H(de+et, famille) * P(de+et, famille) = 0.00122097099

Change in entropy = 0.00122097099 - 0.001223328 = -2.35701e-6

-------
H(de) = 4.3808608
H(et) = 5.5862331

P(de) = 0.04799870191172842
P(et) = 0.02081499323761464
P(de+et) = 0.06881369514934306
H(de+et) = 3.8611604742976153  = -log_2 (P(de)+P(et))
By contrast, the weighted average is

(P(de)*H(de) + P(et)*H(et)) /(P(de) + P(et)) = 4.745465784790553

Combinations:
  P(de+et)*H(de+et) = 0.2657007
  P(de)*H(de) + P(et)*H(et) = 0.32655303

The change in entropy, from forming a union, is:
  P(de+et)*H(de+et) - P(de)*H(de) - P(et)*H(et) = -0.060852316

Recap: Delta(de+et) = -0.060852316
       Delta(de+et, famille) = -2.35701e-6

Entropy increases (strongly) if word-pair merged, words are separated,

-------

MI(d'une, famille) = 5.230504
H(d'une, *) = 9.792551

H(la) = 5.6536283
H(la, *) = 5.5858526

sa  
est
de 
H(d'une) = 9.7960119
H(un) = 7.1578913
H(et) = 5.5862331

-----
repeat, for vielle+nouveaux
H(nouveaux) = 14.28815
P(nouveaux) = 4.998483553100357e-5

H(vieille) = 16.16037
P(vieille) = 1.365349e-5

P(nouveaux+vieille) = 6.363833e-5
H(nouveaux+vieille) = 13.93974
P(nouveaux+vieille)*H(nouveaux+vieille) = 8.87102088-4

P(nouveaux)*H(nouveaux) + P(vieille)*H(vieille) = 9.3483638e-4

Change in entropy is diff of the two: -4.7734297e-5


-----
repeat, for vielle+nouveaux
H(*, famille) = 11.195548 

H(nouveaux, *) = 13.974219
P(nouveaux, *) = 6.213565989765264e-5
MI(nouveaux, famille) = 5.2966957
H(nouveaux, famille) = 19.8730713
P(nouveaux, famille) = 1.0413804797188067e-6

H(vieille, *) = 15.998603
P(vieille, *) = 1.5273571710064995e-5
MI(vieille, famille) = 10.195547
H(vieille, famille) = 16.998604
P(vieille, famille) = 7.636780561617735e-6

P(vieille+nouveaux, famille) = 8.678161041336542e-6
H(vieille+nouveaux, famille) = 16.814179210712517

P(vieille+nouveaux, *) = 7.740923160771763e-5
H(vieille+nouveaux, *) = 13.657134846045357

MI(vieille+nouveaux, famille) = 8.038503635332841

so MI(nouveaux, famille) < MI(vieille+nouveaux, famille) < MI(vieille, famille)
         5.2966957       <         8.038503635332841     <       10.195547

Change in entropy:
P(nouveaux, famille)*H(nouveaux, famille)  + P(vieille, famille)*H(vieille, famille)
    = 1.505100371e-4

P(vieille+nouveaux, famille) * H(vieille+nouveaux, famille) = 1.459161549e-4

Change = 1.459161549e-4 - 1.505100371e-4 = -4.5938821e-6

To recap: Delta(vieille+nouveaux) = -4.7734297e-5
               reduces the entropy more than
          Delta(vieille+nouveaux, famille) = -4.5938821e-6

i.e. entropy increses if the word-pairs are merged, the words are separated.


---------------

Total entropy:
(fold + 0 (map (lambda (atom)
	(let ((ent (tv-conf (cog-tv atom))))
	(* ent (exp (* (- ent) (log 2))))))
	(cog-get-atoms 'WordNode)))

gives: 7.2199274514956



======================================================================
Minimal morphology output


;; Lets say that there was one word in the sentence, it was 'foobar'
;; and the splitter split it into foo and bar
;; then the following should be generated:

;; for each sentence, create one of these, each with a distinct uuid:
(ParseLink (stv 1 1)
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
   (SentenceNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9")
)

;; For each pair of morphemes, cereate the below:
(EvaluationLink (stv 1.0 1.0)
   (LinkGrammarRelationshipNode "MOR")
   (ListLink
      (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
      (WordInstanceNode "bar@cb2443bb-fbec-472c-baee-36b822579861")
   )
)

;; For each "word" aka morpheme, create these two clauses:
;; note that the UUID's match up exactly with the above.
;; the below shows only "foo", another pair is needed for "bar".
(ReferenceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (WordNode "foo")
)
(WordInstanceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
)


;; finally, at the very end:
;; again, the UUID must match with what was given above.

(ListLink (stv 1 1)
   (AnchorNode "# New Parsed Sentence")
   (SentenceNode "sentence@68e51cae-98bc-4102-b19c-78649c5f6cfb")
)

======================================================================
Tagalog status:

31 july 2014
4519631 = 4.5M morpehem pairs
204K morpehemes

   62 | LinkGrammarRelationshipNode
select * from atoms where type=62;

select * from atoms order by stv_confidence;

{60,5592759}
{60,4941961}
{60,1710609}
17720161
7337858
2586082
16810147
9548076
3844852

select * from atoms where stv_count > 100 order by stv_confidence;

1217445

1043142


select * from atoms where uuid=5592759;

{223,223}
236
541






======================================================================
Crashes and bugs


Crash 2
-------
All new crash. Note:
1) This is with the old scheme singleton-instance.
2) This one received when defining a function with an unbound variable
   in it.  It should have just been a standard stack trace, but wasn't.
3) Sometimes, when doing this, and parser is running, I get 

[2013-12-26 22:50:57:680] [ERROR] Caught signal 11 (Segmentation fault) on thread 47486186035520
        Stack Trace:
        2: CogServerMain.cc:92  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ??:0   std::string::append(std::string const&)
        5: basic_string.h:290     std::string::_M_data() const
        6: SchemeEval.cc:381      opencog::SchemeEval::c_wrap_eval(void*)
line 381 is  self->answer = self->do_eval(*(self->pexpr));
So maybe p is null, or self->pexpr is null..
...
        7: continuations.c:511  c_body()
        8: vm-i-system.c:855    vm_regular_engine()
        9: eval.c:508   scm_call_4()
        10: continuations.c:456 scm_i_with_continuation_barrier()
        11: continuations.c:550 scm_c_with_continuation_barrier()
        12: pthread_support.c:1272      GC_call_with_gc_active()
        13: threads.c:937       with_guile_and_parent()
        14: misc.c:1835 GC_call_with_stack_base()
        15: threads.c:959       scm_with_guile()
        16: SchemeEval.cc:372     opencog::SchemeEval::eval(std::string const&)
        17: basic_string.h:536  ~basic_string()
        18: GenericShell.cc:157   opencog::GenericShell::eval(std::string const&, opencog::ConsoleSocket*)
        19: ConsoleSocket.cc:232          opencog::ConsoleSocket::OnLine(std::string const&)
        20: basic_string.h:536  ~basic_string()
        21: thread.cpp:0        thread_proxy()
        22: pthread_create.c:0  start_thread()
        23: ??:0        __clone()

again 7 Jan 2014 but this time its different:

[2014-01-07 00:58:32:234] [ERROR] Caught signal 11 (Segmentation fault) on threa
d 47389012408640
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: vm-i-system.c:887    vm_regular_engine()
        5: eval.c:508   scm_call_4()
        6: backtrace.c:163      scm_display_error()
        7: inline.h:131 scm_puts()
        8: vm-i-system.c:855    vm_regular_engine()
        9: eval.c:508   scm_call_4()
        10: SchemeEval.cc:433     opencog::SchemeEval::do_eval(std::string const&)
        11: SchemeEval.cc:392     opencog::SchemeEval::c_wrap_eval(void*)
        12: continuations.c:511 c_body()
        13: vm-i-system.c:855   vm_regular_engine()
        14: eval.c:508  scm_call_4()

19 Jan 2014: again: same but different:

[2014-01-19 06:06:32:335] [ERROR] Caught signal 11 (Segmentation fault)
on threa
d 47820857940288
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ??:0 strlen()  
        5: strings.c:1518       scm_from_stringn()
        6: strports.c:517       scm_c_eval_string()
        7: vm-i-system.c:855    vm_regular_engine()
        8: eval.c:508   scm_call_4()
        9: SchemeEval.cc:433    opencog::SchemeEval::do_eval(std::string const&)
        10: SchemeEval.cc:392   opencog::SchemeEval::c_wrap_eval(void*)

Got exactly above, again 21 Jan. 

Preceeded a few minutes earlier by below.

opencog-fr> Backtrace:
In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 25dd04e0> ...]
In unknown file:
   ?: 5 [apply-smob/1 #<catch-closure 25dd04e0>]
In ice-9/boot-9.scm:
 157: 4 [catch #t #<catch-closure 25d2ff20> ...]
In unknown file:
   ?: 3 [apply-smob/1 #<catch-closure 25d2ff20>]
   ?: 2 [call-with-input-string "(observe-text \"Hvozdnica est un village de Slovaquie situé dans la région de \u017dilina.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure cfebf00 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error ...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure 25d2fee0> encoding-error ...]
ERROR: In procedure apply-smob/1:
ERROR: In procedure scm_to_stringn: Error while printing exception.
ABORT: encoding-error
Backtrace:
In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 174c8ac0> ...]
In unknown file:
   ?: 5 [apply-smob/1 #<catch-closure 174c8ac0>]
In ice-9/boot-9.scm:
 157: 4 [catch #t #<catch-closure ac0b5e0> ...]
In unknown file:
   ?: 3 [apply-smob/1 #<catch-closure ac0b5e0>]
   ?: 2 [call-with-input-string "(observe-text \"Hvozdnica est un village de Slovaquie situé dans la région de \u017dilina.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure a82f4c0 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error ...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure ac0b5a0> encoding-error ...]

ERROR: In procedure apply-smob/1:
ERROR: In procedure scm_to_stringn: Error while printing exception.
ABORT: encoding-error


Crash 6
-------
All new 21 jan 2014

[2014-01-21 20:30:03:348] [ERROR] Caught signal 11 (Segmentation fault) on thread 47570270030144
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ??:0   std::back_insert_iterator<std::vector<opencog::Handle, std::allocator<opencog::Handle> > > opencog::Atom::getIncomingSet<std::back_insert_iterator<std::vector<opencog::Handle, std::allocator<opencog::Handle> > > >(std::back_insert_iterator<std::vector<opencog::Handle, std::allocator<opencog::Handle> > >)
        5: AtomSpaceImpl.cc:402 opencog::AtomSpaceImpl::getIncoming(opencog::H
andle)
        6: ??:0   opencog::AtomSpace::getIncoming(opencog::Handle)
        7: SchemeSmobNew.cc:538   opencog::SchemeSmob::ss_delete(scm_u

Again, same exact trace:
[2014-01-22 07:42:21:675] [ERROR] Caught signal 11
* Again, same exact trace .. again, only for french. Why is french different???
* Again, same exact trace ..  22 Jan
* Again, same exact trace ..  23 Jan  Only in french ... wtf ... 
* Again, same exact trace ..  23 Jan  Only in french ... wtf ... 
* Again, same exact trace ..  24 Jan  Only in french ... wtf ... 
* Again, handle=480036842 ptr is null .. why?


But this time without the weirdo message:
terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<boost::lock_error> >'
  what():  boost shared_lock has no mutex: Operation not permitted

WTF ... where is the above shared_lock coming from????



Crash 7
-------
[2014-01-22 10:18:24:103] [ERROR] Caught signal 11 (Segmentation fault)
on thread 47155674880320
        Stack Trace:
        2: CogServerMain.cc:91  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ConsoleSocket.cc:211 opencog::ConsoleSocket::OnLine(std::string const&)





Irritation 4
------------
Occasionally see this:

In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 294a63c0> ...]

   ?: 3 [apply-smob/1 #<catch-closure 294a62a0>]
   ?: 2 [call-with-input-string "(observe-text \"Cheopso piramid\u0117, arba Did
\u017eioji Gizos piramid\u0117 \u2013 Egipto piramid\u0117, faraono Cheopso kapa
s.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure 193e5c80 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encodi
ng-error ...]

-----------
another:
 "(observe-text \"Andrzej G\u0105siorowski i Krzysztof Steyer, Tajna Organizacja Wojskowa Gryf Pomorski, Polnord Wydawnictwo Oskar, Gda\u0144sk 2010 r.\")\n" .

Andrzej Gąsiorowski i Krzysztof Steyer, Tajna Organizacja Wojskowa Gryf Pomorski, Polnord Wydawnictwo Oskar, Gdańsk 2010 r.

In ice-9/boot-9.scm:
 102: 1 [#<procedure 3db5240 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error ...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure 3be53c40> encoding-error ...]

ABORT: encoding-error

ERROR: In procedure scm_to_stringn: Error while printing exception.


Crash 8
-------
[2015-07-17 12:37:57:428] [ERROR] Caught signal 11 (Segmentation fault) on
thread 140537051608832
        Stack Trace:
        2: basic_string.h:539   ~basic_string()
        3: CogServerMain.cc:79  _Z7sighandi()
        4: ??:0 killpg()
        5: ??:0 scm_make_vm()
        6: ??:0 GC_mark_from()
        7: ??:0 GC_mark_some()
        8: ??:0 GC_stopped_mark()
        9: ??:0 GC_try_to_collect_inner()
        10: ??:0        GC_collect_or_expand()
        11: ??:0        GC_allocobj()
        12: ??:0        GC_generic_malloc_inner()
        13: ??:0        GC_register_finalizer_inner()
        14: ??:0        scm_closedir()
        15: ??:0        scm_i_new_smob()
        16: smob.h:91   scm_new_smob()
        17: ??:0        scm_vm_engine()
        18: ??:0        scm_call_1()
        19: ??:0        scm_vm_engine()
        20: ??:0        scm_call_3()
        21: ??:0        scm_vm_engine()
        22: ??:0        scm_call_1()
        23: ??:0        scm_vm_engine()
        24: ??:0        scm_call_3()
        25: ??:0        scm_vm_engine()
        26: ??:0        scm_call_4()
        27: SchemeEval.cc:606     opencog::SchemeEval::do_eval(std::string
const&)
        28: SchemeEval.cc:535     opencog::SchemeEval::c_wrap_eval(void*)
        29: ??:0        scm_at_abort()
        30: ??:0        scm_vm_engine()
        31: ??:0        scm_call_4()
        32: ??:0        scm_at_abort()
        33: ??:0        scm_c_with_continuation_barrier()
        34: ??:0        GC_call_with_gc_active()
        35: ??:0        scm_current_processor_count()
        36: ??:0        GC_call_with_stack_base()
        37: ??:0        scm_with_guile()
        38: SchemeEval.cc:502     opencog::SchemeEval::eval_expr(std::string
const&)
        39: basic_string.h:293    std::string::_M_data() const
        40: ??:0
std::this_thread::__sleep_for(std::chrono::duration<long, std::ratio<1l, 1l>
>, std::chrono::duration<long, std::ratio<1l, 1000000000l> >)
        41: ??:0        start_thread()
        42: ??:0        clone()


Garbage 9
---------
submit-one: The town has an area of 88 km², population (2003) is 158,000, and is
located 20 km north of downtown Hồ Chí Minh City, on the left bank of the Saigon River,
upstream from Hồ Chí Minh City.
duuude before nectcat open 27
opencog-en> Backtrace:
In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 195d220> ...]
In unknown file:
   ?: 5 [apply-smob/1 #<catch-closure 195d220>]
In ice-9/boot-9.scm:
 157: 4 [catch #t #<catch-closure 195d0e0> ...]
   ?: 5 [apply-smob/1 #<catch-closure 1959f00>]
In ice-9/boot-9.scm:
 157: 4 [catch #t #<catch-closure 1959dc0> ...]
In unknown file:
   ?: 3 [apply-smob/1 #<catch-closure 1959dc0>]
   ?: 2 [call-with-input-string "(observe-text \"Although this town is an
administratively separate town, it is considered as part of the H\u1ed3 Chí Minh City
Metropolitan Area since this town is bordered by H\u1ed3 Chí Minh City\u2019s urban
area.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure 1addf40 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error
...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure 1959d80> encoding-error ...]


/bin/bash ./ss-one.sh en beta-pages/T/Thủ Dầu Một lo


So -- looks like netcat is sending correct text, but by the time
observe-text gets it, it has been garbled. Why?
Well, nlp/scm/processing-utils.scm prints it ungarbled.

OK, so this fails:
(define sss (socket PF_INET SOCK_STREAM 0))
(connect sss AF_INET (inet-pton AF_INET "127.0.0.1") 7777)
(display "SmålandSmåland" sss)

prints garbage

(set-port-encoding! sss "utf-8")
does not fix it.
(use-modules (rnrs bytevectors))
(send sss (string->utf8 "Hòa Phú Tân Mỹ")) does not work


Ahhh ... (string->utf8 "Småland") prints
#vu8(83 109 63 108 97 110 100)
which is what netcat receives

å should be c3 a5  U+00E5

guile-git is broken
guile-2.0.9 shell works great.
cogserver linked to guile-2.0.9 is broken
... but is fixed by using (set-port-encoding! sss "utf-8")
Yayyy!


Boo ...
submit-one: Phú Mỹ.
(observe-text \"Phú M\u1ef9.\")

ditto for Phú Thọ.
Chánh Nghĩa.
Hiệp Thành
Thủ Dầu Một
Hồ Chí Minh

(relex-parse "Ćićolina\n")
(x-sock-io "Ćićolina\n")
(x-sock-io "Thủ Dầu Một\n")

(define (x-sock-io sent-txt)
   (let ((s (socket PF_INET SOCK_STREAM 0)))
      (connect s AF_INET (inet-pton AF_INET "10.70.70.2") 7777)
      (set-port-encoding! s "utf-8")

      (display sent-txt s)
      (system (string-join (list "echo \"Info: send to parser: " sent-txt "\"")))
      (close-port s)))

(setlocale LC_ALL "")

(set-port-encoding! (current-input-port) "utf-8")
(display "Ćićolina\n")
(define x "Ćićolina\n")   breaks  on cogserver for guile-2.0.9 but
(define x "Thủ Dầu Một\n")  as above
(define x "Småland\n") ... works!!
(define x "Hòa Phú Phú Tân\n")  ... works  !!!
(define x "x\u0106i\u0107olina\\n") works

Ć is U+0106  c4 86
ć is U+0107  c4 87

what is bizarre is that the above is exactly inverted for
the guile-2.1 shell:
sending the  "working" ones sends garbage, sending the failing ones goes through.
(ditto for just simply printing....)

both define/display seem to work fine in the guile-2.0.9 shell...
and define/display seem to work well for the cogserver-2.1 shell. WTF.
so is it due to a shared-library mis-match?  Caching of compiled files?

(display "SmålandSmåland\n")
(display "Ćićolina\n")
(display "Thủ Dầu Một\n")
(display "Hòa Phú Phú Tân\n")

Avoiding preload of all scheme does not fix it.
Clearling the guile compiler cache does not fix it.

[call-with-input-string "(define x \"\u0106i\u0107olina\\n\")\n" ...]

ERROR: In procedure call-with-input-string:
ERROR: Throw to key `encoding-error' with args `("scm_to_stringn" "cannot convert wide
string to output locale" 84 #f #f)'.

(define x "\u0106i\u0107olina\n")

%default-port-encoding is a fluid ....
make fluid with default ...
make dynamic state (with parent ....)

Solution! (setlocale LC_ALL "") every time a new thread is created.





Performance
-----------
Performance seems to suck: 
-- two parsers, each takes maybe 4% cpu time total. Load avg of about 0.03
-- each parser runs 4 async write threads pushing atoms to postgres. 
   each one complains about it taking too long to flush the write queues.
-- postmaster is running 10 threads, load-avg of about 2.00  so about
   2 cpu's at 100%
-- vmstat shows 500 blks per second written. This is low...
-- top shows maybe 0.2% wait state. So its not disk-bound.
-- what is taking so long?

So, take a tcpdump:
-- a typical tcpdump packet:
   UPDATE Atoms SET tv_type = 2, stv_mean = 0 , stv_confidence = 0, stv_count = 54036 WHERE uuid = 367785;
   its maybe 226 bytes long.
-- this gets one response from server, about 96 bytes long.
-- then one more req, one more repsonse, seems to be a 'were'done' mesg
   or something ...  which I guess is due to SQLFreeHandle(SQL_HANDLE_STMT ???
-- time delta in seconds, of tcpdump of traffic packets, between update, and 
   response from server:
   0.0006  0.0002 0.0002 0.0002 0.028 (yow!!) 0.001 0.0002

-- so it looks like about every 8-10 packets are replied to fairly quick,
   then there's one that takes 0.025 seconds to reply.... stair-steps in
   response time like this all the way through the capture.

Wild guess:
-- Hmm ... this seems to be related to the commit delay in postgresql.conf
   Change commit_delay to 1 second
   change wal_bufers to 32MB since its mostly update traffic.
   change checkpoint_segments to 32 (each one takes up 16MB of disk space.)

-- Making these changes has no obvious effect ... bummer.

I don't get it; performance sucks and I don't see why.  Or rather: postmaster
is chewing up vast amounts of cpu time for no apparent reason...


select * from pg_stat_user_tables;
select * from pg_stat_all_tables;
select * from pg_statio_user_tables;
select * from pg_database;

pg_stat_user_indexes
pg_stat_all_indexes

select * from pg_catalog.pg_stat_activity;
select * from pg_catalog.pg_locks;


-- WOW!!!   VACUUM ANALYZE; had a huge effect!!

-- vacuum tells em to do following:
   change max_fsm_pages to 600K
   chage max_fsm_relations to 10K

Anyway ... performance measured as of 27 Dec 2013:

Takes about 105 millisecs to clear 90 eval-links from the write-back
queues. This each eval-link is 5 atoms (eval, defind, list, word, word)
so this works out to 5*90 atoms /0.105 seconds = 4.3KAtoms/sec 
which is still pretty pathetic... 



Misc 
----
Change load-atoms to return a count?

gdb:
---
handle SIGPWR nostop noprint
handle SIGXCPU nostop noprint


How about using a reader-writer lock?
----------------------------------

boost::shared_lock  for reading,
unique_lock for writing ... 

upgrade_lock<shared_mutex> lock(workerAccess); 
        upgrade_to_unique_lock<shared_mutex> uniqueLock(lock);



shared_mutex 
write uses:  unique_lock<shared_mutex>
readers use shared_lock<shared_mutex>

writer does:

  // get upgradable access
  boost::upgrade_lock<boost::shared_mutex> lock(_access);

  // get exclusive access
  boost::upgrade_to_unique_lock<boost::shared_mutex> uniqueLock(lock);
  // now we have exclusive access
}

am using boost-1.49 on cray

------------------------------------------

Setup, July 2015
----------------
LXC container on gnucash.org

AtomSpace.cc line 303

LXC container on backlot
------------------------
nlp-base and nlp-server (currently used by rohit)
morf-server (currrently used by ainish)

LXC container on fanny
----------------------
ssh learn@10.0.3.182
cd src/learn
./run-all-servers.sh
tmux attach
psql en_pairs
