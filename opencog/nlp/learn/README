
                   Language Learning
                   -----------------
             Linas Vepstas December 2013
                 Updated May 2017

Current project, under construction.  See the [language learning wiki]
(http://wiki.opencog.org/w/Language_learning)
for an alternate overview.

Summary
-------
The goal of the project is to build a system that can learn parse
dictionaries for different languages, and possibly do some rudimentary
semantic extraction.  The primary design point is that the learning is
to be done in an unsupervised fashion.  A sketch of the theory that
enables this can be found in the paper "Language Learning", B. Goertzel
and L. Vepstas (2014) on [ArXiv abs/1401.3372](https://arxiv.org/abs/1401.3372)
A shorter sketch is given below.  Most of this README concerns the
practical details of configuring and operating the system, and some
diary-like notes about system configurtion and operation. A diary of
scientific notes and results is in the `learn-lang-diary` directory.

The basic algorithmic steps, as implemented so far, are as follows:
A) Ingest a lot of raw text, such as novels and narrative literature,
   and count the occurance of nearby word-pairs.
B) Compute the mutual information (mutual entropy) between the word-pairs.
C) Use a Minimum-Spanning-Tree algorithm to obtain provisional parses
   of sentences.  This requires ingesting a lot of raw text, again.
   (Independently of step A)
D) Extract linkage disjuncts from the parses, and count their frequency.
E) Use maximum-entropy principles to merge similar linkage disjuncts.
   This will also result in sets of similar words. Presumably, the
   sets will roughly correspond to nouns, verbs, adjectives, and so-on.

Currently, the software implements steps A, B, C and D. Step E is
a topic of current research; its not entirely clear what the best
merging algorithm might be, or how it will work.

Steps A-C are "well-known" in the academic literature, with results
reported by many resaeachers over the last two decades. The results
from Steps D & E are new, and have never been published before.
(Results from Step D can be found in the file, in this directory
`learn-lang-diary/drafts/connector-sets.lyx`, the PDF of which
was posted to the mailing lists)

All of the statistics gathering is done within the OpenCog AtomSpace,
where counts and other statisical quantites are associated with various
different hypergraphs.  The contents of the atomspace are saved to an
SQL (Postgres) server for storage.  The system is fed with raw text
using assorted ad-hoc scripts, which include both link-grammar and
RelEx as central components of the processing pipeline. Most of the
data analysis is performed with an assortment of scheme scripts.

Thus, operating the sytem requires three basic steps:
* Setting up the atomspace with the SQL backing store,
* Setting up the misc scripts to feed in raw text, and
* Processing the data after it has been collected.

Each of these is described in greater detail in separate sections below.

If you are lazy, you can just hop to the end, to the section titled
"Precomputed LXC containers", and grab one of those.  It will have
everything in it, up to the last step.  Of course, you won't know what
to do with it, if you don't read the below. Its not a magic "get smart"
pill. You've got a lot of RTFM in front of you.


Setting up the AtomSpace
------------------------
This section describes how to set up the atomspace to collect
statistics.  Most of this revolves around setting up postgres.

0.0) Optional. If you plan to run the pipeline on multiple
   different languages, it can be convenient, for various reasons,
   to run the processing in an LXC container. If you already know
   LXC, then do it. If not, or this is your first time, then don't
   bother.

0.1) Probably mandatory.  Chances are good that you'll work with large
   datasets; in this case, you also need to do the below. Skipping this
   step will lead to the error `Too many heap sections: Increase
   MAXHINCR or MAX_HEAP_SECTS`. So:
```
   git clone https://github.com/ivmai/bdwgc
   cd bdwgc
   git checkout release-7_6
   ./autogen.sh
   ./configure --enable-large-config
   make; sudo make install
```

0.2) The atomspace MUST be built with guile version 2.2.2.1 or newer,
   which can only be obtained from git: that is, by
```
   git clone git://git.sv.gnu.org/guile.git
   git checkout stable-2.2
```
   Earlier versions have problems of various sorts. Version 2.0.11
   will quickly crash with the error message: `guile: hashtab.c:137:
   vacuum_weak_hash_table: Assertion 'removed <= len' failed.`

   Also par-for-each hangs:
   https://debbugs.gnu.org/cgi/bugreport.cgi?bug=26616
   (in guile-2.2, it doesn't hang, but still behaves very badly)

1) Set up and configure postgres, as described in
   `opencog/persist/sql/README.md`

2) Create and initialize the database. Pick any name you want;
   here it is `learn-pairs`.
```
   createdb learn-pairs
   cat atom.sql | psql learn-pairs
```

3) Start the cogserver, verify that connections to the database work.
   (see steps 6,7,8 below, for more on how to do this.)

Obtaining word-pair counts requires digesting a lot of text, and
counting the word-pairs that occur in the text.  The easiest way of
doing this, at the moment, is to parse the text with link-grammar and
RelEx, using the "any" language.  This pseudo-language will link any
word to any other, and is simply a convenient way of extracting word
pairs from text.  Although this might seem to be a very convoluted way
of extracting word pairs, it actually "makes sense", for two reasons:
a) it already works; little or no new code required. b) later processing
steps will require passing text through the link-grammar parser anyway,
so we may as well start using it right away. (RelEx is used only to
convert link-grammar output to "atomese"; a stripped-down version of
RelEx could be created that does only this, or maybe a native
link-grammar server could be written.)

So, set up the text-ingestion pipeline:

4) Install `link-grammar-5.3.15` or newer.

5) Install the current version of RelEx. The current version has
   assorted bugfixes that are required to get a working system.

6) Create/edit the `~/.guile` file and add the following content:
```
   (use-modules (ice-9 readline))
   (activate-readline)
   (debug-enable 'backtrace)
   (read-enable 'positions)
   (add-to-load-path "/usr/local/share/opencog/scm")
   (add-to-load-path ".")
```

7) Select a language, and copy one of the `pair-count-??.scm` files
   from the `run` directory to your working directory. Review and edit
   the configuration as necessary. It contains the REPL port number,
   and a default prompt: These are set up by default to avoid conflicts
   and confusion, and to allow multiple languages to be processed at
   the same time.  It also contains the database credentials -- that's
   probably the only thing you need to change.

8) Start the various servers. Eventually, you can use the
   `run-all-servers.sh` file in the `run` directory to do this;
   it creates a `byobu` session with different servers in different
   terminals, where you can keep an eye on them. However, the
   first time through, it is better to do all this by hand. So:

8a) Start `run/relex-server-any.sh` in a terminal.

8b) Start the REPL server, in another terminal, as follows:
```
   guile -l pair-count-??.scm
```

9) Verify that the language processing pipeline works. In a third
   terminal, try this:
```
   rlwrap telnet localhost 17005  # or wherever your REPL server is configured.
   scheme@(en-pairs)> (observe-text "this is a test")
```

   Better yet:
```
   echo -e "(observe-text \"this is a another test\")" |nc localhost 17005
   echo -e "(observe-text \"Bernstein () (1876\")" |nc localhost 17005
   echo -e "(observe-text \"Lietuvos žydų kilmės žurnalistas\")" |nc localhost 17005
```

   This should result in activity on the RelEx server, on the cogserver,
   and on the database: the "observe text" scheme code sends the text to
   RelEx for parsing, counts the returned word-pairs, and stores them in
   the database.

9a) Verify that the above resulted in data sent to the SQL database.
   Log into the database, and check:
```
   psql learn-pairs
   learn-pairs=# SELECT * FROM atoms;
   learn-pairs=# SELECT COUNT(*) FROM atoms;
   learn-pairs=# SELECT * FROM valuations;
```
   The above shows that the database now contains word-counts for
   pair-wise linkages for the above sentences. If the above are empty,
   something is wrong. Go to step zero.

9b) Optionally reduce the amount of data that is collected. Currently,
  the `observe-text` function in 'link-pipeline.scm` collects counts
  on four different kinds of structures:

  * Word counts -- how often a word is seen.
  * Clique pairs, and pair-lengths -- This counts pairs using the "clique
                  pair" counting method.  The max length between words
                  can be specified. Optionally, the lengths of the pairs
                  can be recorded. Caution: enabling length recording
                  will result in 6x or 20x more data to be collected,
                  if you've set the length to 6 or 20.  That's because
                  any given word pair will be observed at almost any
                  length apart, each of these is an atom with a count.
                  Watch out!
  * Lg "ANY" word pairs -- how often a word-pair is observed.
  * Disjunct counts -- how often the random ANY disjuncts are used.
                You almost surely do not need this.  This is for my
                own personal curiosity.

9c) Optionally tune the forced garbage collection paramters. Currently
  garbage collection is forced whenever the guile heap exceeds 750
  MBytes; this helps keep RAM usage down on small-RAM achines.  However,
  it does cost CPU time.  Adjust the `max-size` parameter in
  `observe-text` in the `link-pipeline.scm` file to suit your whims in
  RAM usage and time spent in GC.

10) Get ready to feed it text. It would be best to get text that
  consists of narrative literature, adventure and young-adult novels,
  newspaper stories. These contain a good mix of common nouns and verbs,
  which is needed for conversational natural language.

  Turns out that Wikipedia is a poor choice for a dataset. That's
  because the "encyclopedic style" means it contains few pronouns,
  and few action-verbs (hit, jump, push, take, sing, love) because
  its mostly describing objects and events (is, has, was). It also
  contains large numbers of product names, model numbers, geographical
  place names, and foreign langauge words, which do little or nothing
  for learning grammar. Finally, it has large numbers of tables and
  lists of dates, awards, ceremonies, locations, sports-league names,
  battles, etc. that get mistaken for sentences, and leads to unusual
  deductions of grammar.  Thus, it turns out Wikipedia is a bad choice
  for learning text.

  There are various scripts in the `download` directory for downloading
  and pre-processing texts from Project Gutenberg, Wikipedia, and the
  "Archive of Our Own" fan-fiction website.

10a) The current pipeline for Chinese text requires word segmentation
  to be performed outside of OpenCog. This can be done using jieba
  https://github.com/fxsjy/jieba So: first, install it:
  `pip install jieba` and then segment text:
  `run/jieba-segment.py PATH-IN PATH-OUT`. This python utility is in
  the `run` directory.  It might be best to create modified versions
  of the `run/ss-one.sh` and `run/mst-one.sh` scripts to invoke jieba;
  I have not done this yet. Soon...

  Instead of using the `zh` or `yue` languages for sentence splitting,
  you will want to use `zh-pre` or `yue-pre` for the language; this
  disables the addition of spaces between hanzi characters.

11) After doing at least several days worth of parsing  (i.e. at least
   hundreds of articles, tens of thousands of sentences) the word-pair
   mutual information can be extracted. Do the stuff in section "Mutual
   Information" section below.

   If the parsing is interrupted, you can restart the various scripts;
   they will automatically pick up where they left off.

12) Using the extracted mutual information for word-pairs, the MST
   parsing step can be started. See below. Run this for several days.
   At the end of this, you will have a bunch of statistics on
   "pseudo-connector sets": pairs which associate a word to a
   disjunct of pseudo-connectors.

13) Perform clustering research.  You're on your own, now.

That's all for now, folks!


Bulk Text Parsing
-----------------
Set up distinct databases, one for each language:
```
    createdb fr_pairs lt_pairs pl_pairs simple_pairs
    cat opencog/persist/sql/multi-driver/atom.sql | psql foobar_pairs
```

Now, build some working directories (so that we don't mess up
alpha-pages after all that work)
```
    cp -pr alpha-pages beta-pages
```
* Review the file `opencog/nlp/learn/run/README`, and then copy the
  scripts into your working dir.
* Modify as needed for chosen language. None of these need modification,
  by default, unless you want to do something unusual.

  -- `run-all-servers.sh` runs (almost) all of the needed servers, for
     one langauge, all at once, in a multi-paneled `byobu` window. What
     it doesn't do is start the final batch script `./wiki-ss-<lang>.sh`,
     which needs to be started by hand (for the <lang> that you want to
     process). Use F4 to move to the 4th tab and start it there.

  -- `relex-server-any.sh` performs random-syntax parsing for languages
     that do not need morphological analysis.

  -- `wiki-ss-en.sh` performs batch processing of input text files.
     This calls `ss-one.sh` to process one text file.

  -- `ss-one.sh` calls `split-sentences.pl` to split the text file into
     sentences, and then calls `submit-one.pl` to send the sentences
     to relex for parsing. None of these should need any modification.

* Copy `pair-count-lang.scm` to your working directory.  Start the REPL
  server: `guile -l pair-count-<lang>.scm `
* Go back to the input text dir (the dir that contains `beta-pages`), and
  run `./wiki-ss-<lang>.sh`
* Wait a few days.
* This requires postgres 8.4 or newer, for multiple reasons. One reason
  is that older postgres don't automatically VACUUM (see "performance"
  below) The other is the list membership functions are needed.
* Be sure to perform the postgres tuning recommendations found in
  various online postgres peformance wikis, or in the
  `opencog/persist/sql/README.md` file.  See also 'Performance' section
  below.

Some handy SQL commands:
```
SELECT count(uuid) FROM Atoms;
select count(uuid) from atoms where type =123;
```

type 123 is `WordNode` for me; verify with
```
SELECT * FROM Typecodes;
```

The total count accumulated is
```
select sum(floatvalue[3]) from valuations where type=7;
```
where type 7 is `CountTruthValue`.


Mutual Information of Word Pairs
--------------------------------
After accumulating a few million word pairs, we're ready to compute
the mutual entropy between them.  This is done manually, by runnning
functions defined in `(opencog nlp learn)`.

```
	(use-modules (opencog) (opencog persist) (opencog persist-sql))
	(use-modules (opencog matrix))
	(use-modules (opencog nlp) (opencog nlp learn))
	(sql-open "postgres:///en_pairs?user=ubuntu&password=asdf")
	(define ala (make-any-link-api))
	(define asa (add-pair-stars ala))
	(batch-pairs asa)
	(print-matrix-summary-report asa)
```

Batch-counting might take hours or longer, depending on your dataset
size. The batching routine will print to stdout, giving a hint of
the rate of progress.

Example stats and performance:
current fr_pairs db has 16785 words and 177960 pairs.

This takes 17K + 2x 178K = 370K total atoms loaded.
These load up in 10-20 seconds-ish or so.

New fr_pairs has 225K words, 5M pairs (10.3M atoms):
Load 10.3M atoms, which takes about 10 minutes cpu time to load
20-30 minutes wall-clock time (500K atoms per minute, 9K/second
on an overloaded server).

RSS for cogserver: 436MB, holding approx 370K atoms
So this is about 1.2KB per atom, all included. Atoms are a bit fat...
... loading all pairs is very manageable even for modest-sized machines.

RSS for cogserver: 10GB, holding 10.3M atoms
So this is just under 1KB per atom.

(By comparison, direct measurement of atom size i.e. class Atom:
typical atom size: 4820384 / 35444 = 136 Bytes/atom
this is NOT counting indexes, etc.)

For dataset (fr_pairs) with 225K words, 5M pairs:
Current rate is 150 words/sec or 9K words/min.

After the single-word counts complete, and all-pair count is done.
This is fast, takes a couple of minutes.

Next: batch-logli takes 540 seconds for 225K words

Finally, an MI compute stage.
Current rate is 60 words/sec = 3.6K per minute.
This rate is per-word, not per word-pair.

Update Feb 2014: fr_pairs now contains 10.3M atoms
SELECT count(uuid) FROM Atoms;  gives  10324863 (10.3M atoms)
select count(uuid) from atoms where type = 77; gives  226030 (226K words)
select count(uuid) from atoms where type = 8;  gives 5050835 (5M pairs ListLink)
select count(uuid) from atoms where type = 27; gives 5050847 (5M pairs EvaluationLink)

The code for computing word-pair MI is in `batch-word-pair.scm`. It uses
the `(opencog matrix)` subsystem to perform the core work.


Minimum Spanning Tree parsing
-------------------------------
The MST parser discovers the minimum spanning tree that connects the
words together in a sentence.  The link-cost used is (minus) the mutual
information between word-pairs (so we are maximizing MI). Thus, MST
parsing cannot be started before the above steps to compute word-pair
MI have been accomplished.

Minimum spanning tree code is in `mst-parser.scm`. The current version
works well.

To perform MST parsing in bulk:
* Copy `run/mst-count-en.scm`, `run/mst-one.sh` and `run/wiki-mst-en.sh`
  to yor working directory.

* (Optional but suggested) Make a copy of your word-pair database, "just
  in case".  You can copy databases by saying:
````
  createdb -T existing_dbname  new_dbname
```

* Edit `mst-count-en.scm` and set the correct database login
  credentials. (Maybe to the new_dbname you crate above.)

* Edit `wiki-mst-en.sh` and change the directory `gamma-pages` to
  whatever directory holds your source text (or copy your text to
  the `gamma-pages` directory). Be aware that, during processing,
  text files are removed from this directory.

* Start the REPL server by saying
```
  guile -l mst-count-en.scm
```
  Wait 10 to 60+ minutes for the guile prompt to appear.  This script
  opens a connection to the database, and then loads all word-pairs
  into the atomspace.  This can take a long time, depending on the
  size of the database. The word-pairs are needed to get the pair-costs
  needed to perform the MST parse.

* Once the above has finished loading, the parse script can be
  started. Run the `wiki-mst-en.sh` in a terminal, and wait a few days
  for data to accumulate.  Once this is done, you can move to the next
  step, which is exploring the resulting connector-set (disjunct)
  database.

* If parsing is stopped for any reason, you can just re-run these
  scripts; they will pick up where they left off.


Exploring Connector-Sets
-------------------------
Once you have a database with some fair number of connector sets in it,
you can start exploring.

* (optional) Make a copy of the MST database created above.

* Do not run the connector-set code at the same time (on the same
  database) as the MST parser. For one, if you are actively updating
  the counts, then the connector-set counting will get confused.
  Also, if you have two servers writing to the same database
  at the same time, the issueing of UUID's will get confused, and
  one or both the servers will crash, and possible database corruption
  may occur.  It is possible to have multiple writers (and I've done
  this before, any years ago), but this takes additonal configuration,
  and a miscellany of coding changes and a couple of enhancements, to
  keep things in sync.

* So, assuming a clean start: just start `guile` by hand, and enter
  the following commands (by hand). They load the full disjunct/
  connector-set database into the atomspace; it needs to be loaded
  in order to get access to the cosine-distance tool.
```
  (use-modules (opencog) (opencog persist) (opencog persist-sql))
  (use-modules (opencog nlp) (opencog nlp learn))
  (use-modules (opencog matrix))
  (sql-open "postgres:///en_pairs_sim?user=linas")
  (fetch-all-words)
  (length (get-all-words))
  ; This reports 396262 for one my DB's has.

  (define pca (make-pseudo-cset-api))
  (define psa (add-pair-stars pca))
  (psa 'fetch-pairs)
  (define all-cset-words (get-all-cset-words))
  (length all-cset-words)
  ; This reports 37413 in for my `en_pairs_sim`.
  (define all-disjuncts (get-all-disjuncts))
  (length all-disjuncts)
  ; This reports 291637 in for my `en_pairs_sim`.

```
  You can now play games:
```
 (cset-vec-cosine (Word "this") (Word "that"))
 (cset-vec-cosine (Word "he") (Word "she"))

```
  Recall that subroutine documentation can be gotten by typing
  `,apropos` or `,a` for short at the guile command line.  Docs
  for individual routines can be read by saying `,describe subr-name`
  or `,d` for short.

  The `pseudo-csets.scm` file contains code for this stuff. Any routine
  that is `define-public` can be invoked at the guile prompt. Most are
  safe.  If its not `define-public`, you should not call it by hand.

  The `lang-learn-diary/disjunct-stats.scm` file contains ad-hoc code
  used to prepare the summary report.  To use it, just cut-n-paste to
  the guile prompt, to get it to do things.

  The marginal entropies and the mutual information between words and
  disjuncts can be computed in the same way that it's done for
  word-pairs:
```
  (define pca (make-pseudo-cset-api))
  (define psa (add-pair-stars pca))
  (batch-pairs psa)
```


Precomputed LXC containers
--------------------------
For your computing pleasure, there are some LXC containers that you
can download that have a fully-configured, functioning system on them.
The set of available containers will change over time.  The first one
actually has a bunch of bugs and other problems with it, but it's
enough to do some basic work. The steps are:

* Make sure you have some relatively modern Linux system handy.

* Install lxc on it. Like so:
```
  sudo apt-get install lxc
```

* Download a container from
  https://linas.org/lxc-nlp-containers/

* The following describe root-owned containers. You can also have
  user-owned containers, if you know how to do that.

* Go to the directory `/var/lib/lxc/lxc`.  You may need to create
  this directory.  Unpack the file you downloaded above, in this
  directory. Do this as root. Do NOT change the ownership of the
  files!  Avoid changing the datestamps on the files.

* Type in: `sudo lxc-ls -f` You should see a display similar to this:
```
NAME           STATE   AUTOSTART GROUPS IPV4      IPV6
simil-en       STOPPED 0         -      -         -
```

* Start the contianer by saying `lxc-start -n simil-en`. Give it
  5 or 10 or 30 seconds to boot up. Then `lxc-ls -f` again. You
  should see the below. It ay take a minute for the IPV4 address to
  show up. You will proably get a different IP than the one shown.
```
NAME           STATE   AUTOSTART GROUPS IPV4      IPV6
simil-en       RUNNING 0         -      10.0.3.89 -
```

* You can now `ssh` into this container, just as if it were some
  other machine on your network.  It more-or-less is another machine
  on your network.
```
  ssh ubuntu@10.0.3.89
```
  The password is `asdfasdf`.

* The run-scripts are in the `run` directory.  The opencog sources
  are in the `src` directory. You can `git pull; cmake..; make` these
  if you wish. Or not. Its all set up already.  Pull only if you need
  to get the latest.

  (For example, this README file, that you are reading, is in
  `/home/ubuntu/src/opencog/opencog/nlp/learn/README`. Right now.)

* You can now hop directly to the section "Exploring Connector-Sets"
  above, and just go for it.  Everything should be set and done.
  Well, you do need to start guile, of course, etc.



That's all for now!
-------------------
The next step is to cluster the pseudo-disjuncts into clusters, and
thereby extract grammar. There is no code for this, so far. You're
on your own.

Everything below this point consists of personal notes, of work-items
of stuff that still needs doing. You can ignore the rest of this file.
It will eventually get cleaned up.


TODO List
=========
* Count single-word probabilities.
* Resume work on morphology.
* Implement sql-delete.

DONE
----
* Make sure that link-parser fully randomizes linkage choices for
  long sentences. Done. See the rand_state and predictable_rand
  flags in version 4.8.3.
* Need a mutli-language sentence splitter; the maxent splitter only
  works for English sentences.  Probably something simple will do ...
  Done: copied one from moses-smt and put it into a relex subdirectory
* Raw psql seems not to be utf8.  Were the tables created wrong,
  or is this a client bug?
  Fixed: turned out to be a guile-related bug.
* Handle database write asynchronously: have a thread collect
  up the atoms, and write them. Maybe even multiple threads.
  This is OK due to the atomptr design.
  Done.  Not just one, but multiple writers.
* Can we start a guile thread for each incoming sentence?
  Threading will require more subtle sentence cleanup.
  No. The main guile bug, opened 5 years ago, is still not fixed.
* Table counts need to be more than 32-bit. Looks like the
  any-language goes hog-wild and creates huge counts for just
  one sentence ... is this desired?  Anyway, they'll overflow...
  Done. atoms.sql now uses bigint 64-bit uuids.
* Investigate crash: while writing truuth value, stv_confidence
  was pure virtual (circa line 920 of AtomStorage.cc) There's some
  kind of smart-pointer race. See "Crash" section below.
  Done. Ran for a month without issues.
* Fix crash at linkage_set_domain_names+0xe0  Done. Need LG 5.4.0
* Create OO system for generic pair statistics. Done.
* Reduce num samples for 999 to 16. Done.
* Fix (WordNode "…!ANY-PUNCT") Done.
* Rework pipeline to use literary English sources. DONE.
  New scripts in the `download` directory.
* Prune low observation counts. Well, no that is a bad idea.
  However, the (opencog matrix) code now includes data filters.
* Create an LXC container for public use. DONE.
  But not updated.... Well no one is interested in thsi so punt.


Some typical entropies for word-pairs
-------------------------------------
Three experiments:
1) Get H(de,*) H(*,de)  H(en,*) H(*,en) and compare to
   H(de+en,*) H(*, de+en)

2) H(vieux,*) H(* vieux) H(nouveaux, *) H(*, nouveaux)

3) H(vieille, *) etc + vieux

4) H(le, *) H(la,*)  vs. H(le+la)

5) H(le,*) H(famille,*) which should fail ...!?



Some typical entropies for word-pairs
-------------------------------------
The below is arithmeticaly correct, but theoretically garbage.

(WordNode "famille") entropy H=11.185

H(*, famille) = 11.195548
H(famille, *) = 11.174561

MI(et, famille) = -5.2777815
H(et, *) = 5.5696678
P(et, *) = 0.021055372363972875

thus:
H(et, famille) = -MI(et, famille) + H(famille, *) + H(et, *) = 22.0220103
P(et, famille) = 2.348087815164205e-7

MI(de, famille) = 2.1422486
H(de, *) = 4.3749881
P(de, *) = 0.04819448582223504

H(de, famille) = -2.1422486 + 4.3749881 + 11.195548 = 13.4282875
P(de, famille) = 9.071574511509601e-5

P(de+et. *) = 0.06924985818620791
H(de+et, *) = 3.8520450730427047

P(de+et, famille) = 9.095055389661243e-5
H(de+et, famille) = 13.424558050397735
MI(de+et, famille) = 1.6230350226449701

So  MI(et, famille) < MI(de+et, famille) < MI(de, famille)
       -5.2777815   <   1.6230350226     <   2.1422486

By contrast, the arithmetic average is:
(MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille)) /
    (P(de, famille) + P(et, famille))
  = 2.1230921666199825

Change in entropy:
MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille) =  0.0012169

MI(de+et, famille) * P(de+et, famille) = 1.476159343e-4

Oh, wait ...
H(de, famille) * P(de, famille) + H(et, famille) * P(et, famille) =  0.001223328

H(de+et, famille) * P(de+et, famille) = 0.00122097099

Change in entropy = 0.00122097099 - 0.001223328 = -2.35701e-6

-------
H(de) = 4.3808608
H(et) = 5.5862331

P(de) = 0.04799870191172842
P(et) = 0.02081499323761464
P(de+et) = 0.06881369514934306
H(de+et) = 3.8611604742976153  = -log_2 (P(de)+P(et))
By contrast, the weighted average is

(P(de)*H(de) + P(et)*H(et)) /(P(de) + P(et)) = 4.745465784790553

Combinations:
  P(de+et)*H(de+et) = 0.2657007
  P(de)*H(de) + P(et)*H(et) = 0.32655303

The change in entropy, from forming a union, is:
  P(de+et)*H(de+et) - P(de)*H(de) - P(et)*H(et) = -0.060852316

Recap: Delta(de+et) = -0.060852316
       Delta(de+et, famille) = -2.35701e-6

Entropy increases (strongly) if word-pair merged, words are separated,

-------

MI(d'une, famille) = 5.230504
H(d'une, *) = 9.792551

H(la) = 5.6536283
H(la, *) = 5.5858526

sa
est
de
H(d'une) = 9.7960119
H(un) = 7.1578913
H(et) = 5.5862331

-----
repeat, for vielle+nouveaux
H(nouveaux) = 14.28815
P(nouveaux) = 4.998483553100357e-5

H(vieille) = 16.16037
P(vieille) = 1.365349e-5

P(nouveaux+vieille) = 6.363833e-5
H(nouveaux+vieille) = 13.93974
P(nouveaux+vieille)*H(nouveaux+vieille) = 8.87102088-4

P(nouveaux)*H(nouveaux) + P(vieille)*H(vieille) = 9.3483638e-4

Change in entropy is diff of the two: -4.7734297e-5


-----
repeat, for vielle+nouveaux
H(*, famille) = 11.195548

H(nouveaux, *) = 13.974219
P(nouveaux, *) = 6.213565989765264e-5
MI(nouveaux, famille) = 5.2966957
H(nouveaux, famille) = 19.8730713
P(nouveaux, famille) = 1.0413804797188067e-6

H(vieille, *) = 15.998603
P(vieille, *) = 1.5273571710064995e-5
MI(vieille, famille) = 10.195547
H(vieille, famille) = 16.998604
P(vieille, famille) = 7.636780561617735e-6

P(vieille+nouveaux, famille) = 8.678161041336542e-6
H(vieille+nouveaux, famille) = 16.814179210712517

P(vieille+nouveaux, *) = 7.740923160771763e-5
H(vieille+nouveaux, *) = 13.657134846045357

MI(vieille+nouveaux, famille) = 8.038503635332841

so MI(nouveaux, famille) < MI(vieille+nouveaux, famille) < MI(vieille, famille)
         5.2966957       <         8.038503635332841     <       10.195547

Change in entropy:
P(nouveaux, famille)*H(nouveaux, famille)  + P(vieille, famille)*H(vieille, famille)
    = 1.505100371e-4

P(vieille+nouveaux, famille) * H(vieille+nouveaux, famille) = 1.459161549e-4

Change = 1.459161549e-4 - 1.505100371e-4 = -4.5938821e-6

To recap: Delta(vieille+nouveaux) = -4.7734297e-5
               reduces the entropy more than
          Delta(vieille+nouveaux, famille) = -4.5938821e-6

i.e. entropy increses if the word-pairs are merged, the words are separated.


======================================================================
Minimal morphology output


;; Lets say that there was one word in the sentence, it was 'foobar'
;; and the splitter split it into foo and bar
;; then the following should be generated:

;; for each sentence, create one of these, each with a distinct uuid:
(ParseLink (stv 1 1)
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
   (SentenceNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9")
)

;; For each pair of morphemes, cereate the below:
(EvaluationLink (stv 1.0 1.0)
   (LinkGrammarRelationshipNode "MOR")
   (ListLink
      (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
      (WordInstanceNode "bar@cb2443bb-fbec-472c-baee-36b822579861")
   )
)

;; For each "word" aka morpheme, create these two clauses:
;; note that the UUID's match up exactly with the above.
;; the below shows only "foo", another pair is needed for "bar".
(ReferenceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (WordNode "foo")
)
(WordInstanceLink (stv 1.0 1.0)
   (WordInstanceNode "foo@5e179119-3966-4bb9-8a38-ef2014b48f12")
   (ParseNode "sentence@fc98a97a-4753-45d9-be5b-1c752b5b21d9_parse_0")
)


;; finally, at the very end:
;; again, the UUID must match with what was given above.

(ListLink (stv 1 1)
   (AnchorNode "# New Parsed Sentence")
   (SentenceNode "sentence@68e51cae-98bc-4102-b19c-78649c5f6cfb")
)

======================================================================
Tagalog status:

31 july 2014
4519631 = 4.5M morpehem pairs
204K morpehemes

======================================================================

Setup, July 2015
----------------
LXC container on gnucash.org

AtomSpace.cc line 303

LXC container on backlot
------------------------
nlp-base and nlp-server (currently used by rohit)
morf-server (currrently used by ainish)

LXC container on fanny
----------------------
ssh learn@10.0.3.182
cd src/learn
./run-all-servers.sh
tmux attach
psql en_pairs
