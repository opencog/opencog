
                   Wikipedia cleaning utilities
                   ----------------------------

The wiki-scrub.pl utility will take a Wikipedia XML database dump,
marked up in the standard Wikipedia XML format, and split it into
multiple articles, one per file. In the process, it will remove all
markup, leaving only plain text. The goal is to have clean text that
can be input into Relex.  Runtime is roughly 5 minutes per 100K
articles, or one hour per million articles.

The wiki-clean-en.sh shell script will remove certain unwanted wikipedia
pages (templates, images, categories, etc.) for the English-language
wikipedia. These files contain almost no parsable text.

* `wiki-clean-fr.sh` As above for frwiki (French)
* `wiki-clean-gu.sh` As above for guwiki (Gujarati)
* `wiki-clean-hi.sh` As above for hiwiki (Hindi)
* `wiki-clean-lt.sh` As above for ltwiki (Lithuantian)
* `wiki-clean-pl.sh` As above for plwiki (Polish)
* `wiki-clean-tl.sh` As above for tlwiki (Tagalog)
* `wiki-clean-zh.sh` As above for zhwiki (Chinese)

The `wiki-alpha.sh` shell script will move (Latin-font) wikipedia
articles to alphabetical directories.  This can simplify and speed up
article processing.

* `wiki-alpha.sh` - Same, but for Chinese
