
                       OpenCog Chatbot
                       ---------------

A (partial) demo the OpenCog NLP pipeline, connected to IRC or Telegram
chat.

At this time, the bot is a demo of a portion of the OpenCog
natural-language processing (NLP) pipeline; missing feautres include
reference resolution, reasoning, dialog planning and emotional control.

The chatbot can currently answer certain simple English-language
wh-questions.  It does this by using "fuzzy-pattern-matching". This
works by performing a fuzzy search, comparing the question to sentences
in the atomspace: if a similar sentence is found, the fuzzy matcher
will offer that up as an answer.  See below for additional details.

A more sophisticated chatbot can be found in the `chatbot-eva`
directory.  That chatbot can be used to control a Hanson Robotics
animated head (via a blender animation rig), and pose self-awareness
questions to it ('what are you doing?', 'where are you looking?' etc.)
It builds on top of and extends the concepts described in this file.

Running the chatbot requires three steps:

1) Running the parse server.
2) Running the opencog server.
3) Running the chat-to-opencog bridge.

Run the parse server:
---------------------
The chatbot requires the Link Grammar + RelEx English-language parse
serving daemon to be running.  The server is provided in the RelEx
package, and is started by running `./opencog-server.sh` from the main
RelEx directory. (You must first download, compile and install Link
Grammar first, and then download and compile RelEx.)

The parse server listens on port 4444: it will read plain-text input
and return parsed text in OpenCog format.

There is a simple "sniff-test" that can be used to verify that the
server is running: try "telnet localhost 4444" to connect to the server,
then type in an English sentence, then hit return. It should respond
with a verbose parse of the sentence.


Run the OpenCog server:
-----------------------
The chatbot requires an opencog server running on port 17004. This
server needs to be appropriately configured and loaded with assorted
databases, code, etc.

This is most easily done by starting the cog server using the
`run-chatbot.scm` script.  It loads the various needed modules,
the relex2logic rules, and starts the cogserver. The
`opencog-chatbot.conf` file sets the port number to 17004 and
silences the opencog prompt, so that it doesn't echo to IRC.

From the build directory:
```
   guile -l ../opencog/nlp/chatbot/run-chatbot.scm
```

A simple "sniff-test" to verify that this is working is to telnet to
port 17004 and manually specify a sentence.  Note that the normal
cogserver prompt is blanked, to prevent it from showing on the IRC
channel.  Thus, after the telnet, no prompt will be displayed; that
is normal.
```
   rlwrap telnet localhost 17004
   (process-query "luser" "Are you a bot?")
```
This should return either a reasonable answer, or something like
"I can't process truth query for now"


Configure and run the chat bridge:
----------------------------------
The chatbot itself is just a simple daemon that bridges text from the
chat system to the OpenCog server, and returns the responses that
OpenCog generates. The bridge is tailored for IRC chat; other chat
systems could be supported in a straightforward manner.  See the file
`../irc/README` for details.

Start the bot directly from the build directory:
```
   opencog/nlp/irc/cogita
```

It should then appear on the freenode.net #opencog IRC channel. You
can then talk to it by addressing it by name, i.e.  prefacing comments
with cog:, cogita: or cogita-bot: or by opening a private dialog
window to it, in which case, you don't need to address it directly.



Architecture:
-------------
The current architecture makes use of several servers interconnected by
TCP/IP sockets to perform NLP processing.

The pipeline is as follows:

1) IRC I/O, via a simple irc daemon, acts as an intermediary between
   IRC and OpenCog.  It listens for input on an IRC channel, and
   forwards the resulting plain-text to OpenCog. This is done by
   issuing one simple scheme expression to OpenCog, via the OpenCog
   command-line interface/scheme shell on port 17004. The expression
   is ''(process-query user text)'', where the ''user'' is
   the user's IRC nick, and ''text'' is what the user entered.  The
   return value from this command is sent back to the IRC channel.
   Communications is stateless and blocking: the irc bridge closes the
   socket to indicate end-of-messsage, and expects that the cogserver
   will do the same. Only after the cog-server closes its socket does
   the bridge reply on the IRC channel.

   Note that if the cog-server is busy, then "whirr" can block for an
   indefinitely long time. The person who is chatting will start to
   wonder about the lack of response. This is currently hacked around
   by having the chat processing periodically return to the bridge,
   and then resume again. A proper architecture remains unimplemented.

2) Parsing of English text by RelEx. The very first thing that the
   ''process-query'' routine does is to send the input text to RelEx
   for parsing.  This is done by sending the plain-text via a socket
   to a listening RelEx server; the server responds with the parsed
   text, in the form of a hypergraph of atoms expressed in scheme.
   The hypergraph is then loaded into the OpenCog atomspace, and is
   ready for processing.

   I/O to the parser is stateless: RelEx will close the socket after
   it has completed its parse and returned its results.

   The newly parsed sentence will be sent to a scheme function called
   ''check_query_type''. This function will identify the speech act
   type of the sentence, and then call the next appropriate processing
   functions. For instance, ''(wh_query_process query)'' will
   be invoked if the speech act tag is `InterrogativeSpeechAct`.

3) The RelEx dependency parse is converted to a "logical" form,
   suitable for reasoning, by a set of Relex2Logic (R2L) rules,
   applied by the rule engine (URE).  The current fuzzy-pattern
   matcher uses the R2L output. Also PLN will use R2L output for
   reasoning.

4) Question-Answering. The bot currently recognizes the speech act
   types `InterrogativeSpeechAct`, `TruthQuerySpeechAct` and
   `DeclarativeSpeechAct`.

   The bot answers some simple wh-questions (`InterrogativeSpeechAct`)
   by using the fuzzy-pattern-matcher.  The atomspace must first have
   been primed with factual sentences pertaining to the question.

5) Word-sense disambiguation (WSD) (not hooked up).  This is just the
   most elementary step of determining the word senses of the words
   used in the input text. See the wiki page for details.
   http://wiki.opencog.org/w/Word_sense_disambiguation

6) Reasoning (not hooked up). PLN, the core reasoning engine will be
   used. This stage interacts strongly with WSD.

7) Anaphora resolution (not hooked up) will be used to resolve pronouns
   and other references.  This interacts strongly with WSD.

8) OpenPsi (not hooked up yet) will be used to make the chatbot to
   generate a speech-act goal plus context for utterances. Here,
   different speech act schema will be triggered based on the goal
   and context.

9) The result of the above processing is a collection of natural
   language dependency relations.  These are converted to one or
   more syntactically correct output sentences, using the micro-
   planning+sureal (surface realization) subsystem.  The resuklting
   sentences are passed back to the IRC bridge.

Examples:
---------
Currently, the following dialog is working.


 <Mr. x> hello
 <cogita-bot> Hello Mr. x, you said: "hello
 <cogita-bot> Sorry, I don't know the type

 <Mr. x> Tom eats apples.
 <cogita-bot> Hello Mr. x, you said: "Tom eats apples.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> What does Tom eat?
 <cogita-bot> Hello Mr. x, you said: "What does Tom eat?
 <cogita-bot> You made an Interrogative SpeechAct Tom eats apples .

 <Mr. x> John threw a green ball.
 <cogita-bot> Hello Mr. x, you said: "John threw a green ball.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> who threw a ball?
 <cogita-bot> Hello Mr. x, you said: "who threw a ball?
 <cogita-bot> You made an Interrogative SpeechAct John threw a green ball .

 <Mr. x> Did Fred throw a ball?
 <cogita-bot> Hello Mr. x, you said: "Did Fred throw a ball?
 <cogita-bot> You ask a Truth Query I can't process truth query for now

 <Mr. x> The color of the book is red.
 <cogita-bot> Hello Mr. x, you said: "The color of the book is red.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> What is the color of the book?
 <cogita-bot> Hello Mr. x, you said: "What is the color of the book?
 <cogita-bot> You made an Interrogative SpeechAct the color of the book is red .

 <Mr. x> John Kennedy died.
 <cogita-bot> Hello Mr. x, you said: "John Kennedy died.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> who died?
 <cogita-bot> Hello Mr. x, you said: "who died?
 <cogita-bot> You made an Interrogative SpeechAct John Kennedy die

  <Mr. x> The red ball is under the couch.
 <cogita-bot> Hello Mr. x, you said: "The red ball is under the couch.
 <cogita-bot> You made a Declarative SpeechAct
 <Mr. x> Where is the red ball?
 <cogita-bot> Hello Mr. x, you said: "Where is the red ball?
 <cogita-bot> You made an Interrogative SpeechAct the red ball is under the couch .


ToDo:
-----
* Implement the truth-query processing after the issue on the backward
  chaining is fixed.

* Write different  speech act schema's as it is described on the
  "dialog system" paper.

* Create an object holding the name of the user, the chat connection,
  and the 'pseudo-continuation' of the chat status: Create a dialog node
  for each dialog and an utterance node for each utterance.

* Integrate the components described in the architecture part.

References
==========
[Katz05] Boris Katz, Gary Borchardt and Sue Felshin
    "Syntactic and Semantic Decomposition Strategies for Question
     Answering from Multiple Resources"
     Proceedings of the AAAI 2005 Workshop on Inference 2005 - aaai.org


http://en.wikipedia.org/wiki/MultiNet

“Initial OpenCog Based Dialogue System Design Document”
an ongoing thesis by Ruiting Lian
