
Combined language and animation
-------------------------------
Proposal - April 2016 - Linas Vepstas

This document sketches a proposed architecture for combining language
with sensory perception, memory, reasoning and action. Yes, such a
combination is a "holy grail" of AGI.  Yes, there must be 1000 other
proposals for how to do this.  This proposal is very low brow: it
attempts to describe something that is doable by a small number of
programmers, in a short period of time, making use of the existing
OpenCog code base, and, at the same time, result in a system that
is not a train-wreck of hack, but something that could maybe survive
and be improved upon in an incremental fashion.

As such, it will touch base with existing source code within OpenCog,
even if that source code is imperfect or inadequate.  It is up to the
reader to understand what portions of the code need to be improved, and
to do that work.

This document recapitulates many things the Ben talks about, but also
tries to draw attention to those areas where we have deficiencies.
During prototyping, I have discovered large, *major* holes, missing
pieces in the OpenCog infrastructure.  I will attempt to describe these
here. These are areas where "we" (i.e. "Ben") have not invested enough
time to formulate and clarify and design.  Because no one associated
with OpenCog has spent much time thinking about these missing parts,
any answers I might propose are necessarily provisional, flawed and
subject to re-appraisal and redesign.


General overview
----------------
If you are bored by generalities, then jump immediately to the section
titled "The existing code base (March/April 2016)" below.  That's where
the meat is.

The general strategy is this:
 0. Raw sensory input.
 1. Identify what is being asked (said, perceived)
 2. Assemble a set of candidate answers (responses)
 3. Seek evidence for or against each answer (each action to be taken)
 4. Formulate a reply (perform that action)
 5. Raw motor and animation.

Step 0. is the external environmental: video frames, audio. It includes
all the software needed to digest this input and extract some
preliminary, low-level features from it: the words that were spoken,
the intensity of the sound, the faces that we can see.  The rest of this
document will say relatively little about this, other than that we need
to improve it.

Step 1. is the integrated sensory system, sometimes called the
"perception synthesizer". Currently, it is extremely limited: the
RelEx and R2L subsystem, and a face-detector (face tracker).  The
sensory system needs to be greatly expanded. See Appendix A for some
relatively simple/easy things we could do to improve the situation.

Note that R2L provides varying levels of text analysis: the raw words
in the sentence, part-of-speech markup, syntactic analysis,
logic-analysis, identification of a handful of speech acts. It would be
nice to have sound-level and timing information to go with this (see
Appendix A for more).

The point of step 1. is to pull together data from disparate sensory
systems, pre-digest it to pull out some basic abstracted elements
(words, syntax, visible faces) and the timing info for events.

This document will be mostly about steps 2 and 3. Technologies like
PLN and OpenPsi kick in at this location.

Step 4. is what we've loosely been terming "the action orchestrater"
(AO): a place where a small number of responses are converted into
actions: for example: raise hand, smile, and say hello. Notice that
all three of these actions are still "integrated", and have to play
out on the same time line in a coordinated fashion.  Step 4 is where
these get dispatched to be performed.

Step 5. is the actual performance of movements, animations, speech.
It includes the text-to-speech subsystem (TTS) and the blender
animation system.  We have these subsystems currently, they sort-of
work, they could be improved.  This document will mostly ignore
the stuff in step 5.

The goal
--------
The goal of this document is to sketch how steps 2 and 3 might work,
and how they might interact with steps 1 and 4.  It will be a mish-mash
description of the prototype code that has been implemented, together
with aspirations about what should be or could be done.

The goal is also to delineate the modules and subsystems that comprise
steps 2 and 3 in such a way that they can be (a) understood sufficiently
clearly to be converted into actual working code, (b) designed in such
a way that future expansion minimizes how much redesign is required,
(c) propose a system that can be prototyped in weeks or months,
(actually, a prototype exists already; it needs expansion) (d) be
kept in continuous-operation, continuous-demo mode, avoiding major
down-times for system overhaul (e) provide an indicator or path to
integrating learning and increasing automation.

Note that step (e) is absolutely vital, and very easy to loose track of
and sideline.  A "temporary", "short-term" goal or substitute for (e) is
(f) allow artists, directors and creatives to alter and script
behaviors.


The existing code base (March/April 2016)
-----------------------------------------
The current code base uses a multi-step process to manage verbal
interactions with the robot.  The stages are given below. Some of the
code violates design-philosophy point A) (below) of not being expressed
as rules.  Where there are rules, they all have been written as
BindLinks, and not ImplicationLinks.  This means that they are fragile
and not easily chained or searched-for or selected.  That is, they
all violate point G) below: too much of the implementation details
have entered into the rules.  That's bad.

Upshot: the existing code is not actually written in the fashion
recommended by the "philosophy" secion, below. That's because the
"philosophy" wasn't clear at the outset.  Prototyping helps clarify
such issues.

The steps are:

 I) Obtain input text, identify and dispatch processing based
    on speech type -- `process-query` / `grounded-talk` in `bot-api.scm`
    The dispatch-on-speech-act-type should be ImplicationLinks,
    and not scheme code, as written.

 II) Dispatch imperatives to imperative-process, in `imperative.scm`
    The rest of this section deals with imperatives only.

 II.a) Converted LG-parsed sentences into an "intermediate form".
    The intermediate form uses a new custom link type, an `ActionLink`
    to hold a simplified action-name (i.e. abstracted "verb"), and a
    and a simplified object of that action (i.e. something that,
    in the English sentence, as a direct object or maybe a prepositional
    object, or possibly some adverb).  The `ActionLink` was invented
    only to simplify processing; it should probably be eliminated??

    This conversion stage includes the recognition of synonyms and
    synonymous phrases. This is currently done in an ad-hoc basis.

    The intermediate form is NOT R2L!! This is an important point!!
    The R2L representation, as currently implemented, seems
    insufficient to represent simple conceptual relations.  Part of
    the problem is that R2L is doing its thing *before* the
    synonymous-phrase analysis happens. I need a form that captures
    the intent, *after* synonymous-phrase processing.

    Synonymous-phrase processing trips over philosophy points A)-E)
    above.  So its extremely ad-hoc right now.

    For example: the current synonymous phrase rules are `look-rule-1`,
    `look-rule-2`, `single-word-express-rule`, `single-word-gesture-rule`,
    `show-rule-1`, `show-rule-2` in the file `imperative-rules.scm`.

    These rules search for specific LG link types.  This is an important
    point!!  I found that the RelEx relations were too garbled and
    confused to be reliably useful for the extraction of the needed
    info!  I had gone into this thinking that I would ONLY use R2L,
    and maybe sometimes fall back to RelEx; instead, I had to fall back
    all the way to LG to get something that was not crazy.

    This is important!! The core problem was synonymy!! Different
    sentences, that mean almost the same thing, were being converted
    into wildly-different forms by RelEx and R2L. It was easier to
    manage synonymy with LG, than with the RelEx dependency relations!


 II.b) Perform "semantic analysis" on the intermediate form. This
    attempts to match the contents of the intermediate form to one
    or more bits of "grounded knowledge".

    The current set of atoms representing "grounded knowledge" can be
    found in `knowledge.scm`.  Knowledge comes in two forms: "grounded
    meanings" for single words, (single WordNodes) and "grounded
    meanings" for graphs (Links).

    An example of a "grounded meaning for a word" would be "leftwards"
    which is attached to an 3D robot-body-relative x,y,z coordinate.
    This particular "meaning" for "leftwards" is ALSO built into an
    ontology: "leftwards" is-A "direction".  The ontology is needed
    because "look left" needs to be disambiguated from "look happy".

    An example of a "grounded meaning for a graph" is "turn-model".
    The "turn-model" is an EvaluationLink with two parts. The first
    part has to belong to the the class of turning-movement-actions
    (abstracted "verbs" that expect a "direction" as an argument).
    The second part is a "direction" i.e. belongs to the class of
    "directions" in the ontology.

    So, for example, the word "look" is-A turning-movement-action.
    If the intermediate form combines this with a word that is-A
    "direction", then the "turn-model" pattern becomes a "grounded
    meaning for the graph".  Thus, the semantic analysis of the
    intermediate form results in the discovery of the "turn-model".
    This discovery grounds the intermediate form "look left" to
    specific 3D spatial x,y,z coordinates, and also a specific
    motor-control (animation) predicate (DefinedPredicate "Look
    command") which, if executed, would cause the robot to turn.
    (as well as other things, such as update the self model. More
    on execution later.)

    A different example of a "grounded meaning for a graph" is the
    "express-action".  Its also a two-part EvaluationLink, except
    that the first slot must belong to the class of verbs having to
    do with facial expressions or gestures, and the second part is
    a word naming an expression or gesture.  Note that the word
    "look" is-A facial-expression-action.

    Thus, the word "look" has two different grounded meanings; one
    meaning is "turning-movement", and the other is "showing-
    expression".  The correct meaning cannot be found without looking
    to see if the intermediate-form-object of the intermediate-form-
    verb is a direction, or an emotion-gesture name.

    The rules that perform the above semantic analysis are in the
    file `semantics.scm`.  The rules here depend on the knowledge
    representation being wired up just right, in `knowledge.scm`.

    Observe that there is a *second stage* of synonym handling that
    happens here, as well: in step II.a)

 II.c) Convert grounded knowledge into a set of candidate actions
    (a list of suggestions for actions that could be performed).
    `action-rule-ao` in `imperative.scm`. Currently a hacky stub.

    Basically, if semantic analysis resulted in a non-empty set
    of things that are understood, then each of these is a candidate
    for performing an action.

    For a single, given understanding, there are several actions to
    be performed:
    *) Play a blender animation (which will move the robot)
    *) Update the self-model (so that the robot knows what its doing,
       and is able to remember and talk about what it did).
    *) Say something.

 II.d) Decide what to do. That is, narrow down the list of candidates
    from the step above.  Currently a stub: all proposed actions are
    performed. The line of code is a for-loop in `imperative.scm`:
    `(for-each cog-evaluate! action-list)`

 II.e) Action orchestration and self-modelling.  The update of the
    self-model must happen only *after* step II.d) above.  That is,
    we should NOT memorize what we did, until *after* we decided to
    do it, and *after* action orchestration.   That is, even if stage
    II.d) decides to do something, the AO may block or over-ride it,
    due to some conflict or interlock.  Conflicts and over-rides
    might occur if commands are issues too rapidly in time (smile and
    frown at same time) or if multiple systems are trying to control:
    e.g. if the robot is being puppeteered, but in a semi-autonomous
    puppeteered mode.  Or maybe two different subsystems are each
    making conflicting suggestions.  The use-cases and issues here are
    unclear; what is clear is that updating the self-model too early
    results in bad self-awareness.

    The code for handling this is mostly in the github repo for
    opencog/ros-behavior-scripting.  It needs to be refactored.

 III) Question-answering.  A prototype for question-answering was working
    earlier, but is currently broken, due to the self-model redesign.
    Question-answering is conceptually similar to the above: very similar,
    it seems:

 III.a) Just as in II.a, we need a simplified intermediate form to
    represent what we think was asked.

 III.b) Just like II.b, we need to see if the intermediate form can
    be matched up to our current knowledge of the world (or of
    ourselves).  The is, we need a world model roughly of the form
    `EvaluationLink "visibility-item" "face 42"` that can be matched
    to the intermediate form of the question "what do you see?"
    I think that the same general semantic processing and KR that
    II.b uses would work fine, here as well.

 III.c) Just like II.c. For example, the response to "what do you see?"
    might generate a long list of things.

 III.d) Just like II.d, of all the things we could say in response
    to a question, we have to say only one or two.


Lessons learned: Subsystem we don't have, but we need
-----------------------------------------------------
The above prototype makes clear that there are many important
subsystems that are missing in OpenCog.

II.a) make clear: we don't have any go way of dealing with synonymous
words or phrases.  Note also that step II.b) ALSO does synonym
processing, which is different than the synonymous-phrase processing
that is done in step II.a).  Synonym handling doesn't fit into a small
little box: its spread out across multiple components.

II.a) makes clear: we need some reasonable "intermediate form" that
is not R2L.

II.b) makes clear: we don't have any sort of agreed-upon system for
representing ontological knowledge (for example, that "leftwards"
is-a "direction") or for grounded knowledge (for example, that
"leftwards" corresponds to (x,y,z)=(1, 0.5, 0) in robot-body coords).
Knowledge representation also has to provide meaning for the abstracted
"intermediate forms", such as "direction-during-verb-followed-by-
direction-name", or "facial-expression-imperative-verb-followed-by-
name-of-emotion".

What I've constructed there seems simple enough, and adequate for now.
It will be interesting to see how we can automatically learn new
knowledge.

II.a) plus II.b) plus II.e) makes clear that we cannot dis-associate
knowledge, and the way in which we represent knowledge, from the
subsystems that act on that knowledge, that query that knowledge.
So, step II.e) is updating the self-model, but the self-model has to
be in a form such that the question-answering system III.b) can
access that knowledge.  Thse are all interlocked designs.

III.b) makes it clear that we have an inadequate way of storing sensory
information.  If someone asks "what do you see?", we need to match the
intermediate-form of that question to the world-model.  We don't have
a KR design for the world-model that is queryable.

II.d) we don't have a way of deciding what to do. I thought that maybe
OpenPsi was this system, but based on what I've seen of OpenPsi, its
not clear how to use it to make decisions.  Specifically: make decisions
based on the current real-world situation, and the mood that the robot
is in.  For example, if someone is being mean to the robot, the robot
may choose to ignore all imperatives (i.e. discard the list of actions
that the NLP subsystem II.a-II.c came up with, and do something else).

For example: if the imperative "look to the left" was heard, and
correctly processed, but the vision system reports that a new face is
visible on the right.  Should the robot look left, or right?

We don't have any sort of decision-making subsystem spec'ed or designed.
Its hard to imagine how to spec or design such a system, until we have
better control over the world-model and the sensory input that drives
it.  For that, we need more prototyping, just to see what the issues
are.


Overall design philosophy
-------------------------
The above prototype made clear that there are several design meta-issues
that need to be addressed.  The code, as written, is functional. That
is, it works, and it can be expanded and enhanced and broadened without
too much trouble. However, it has some serious meta-design issues that
should be corrected. These meta-issues did not become clear until after
the prototype was created.  The issues, and a way of resolving them,
is presented below.  Its presented as a "design philosophy" that should
be adhered to, in moving forward with the implementation.

This philosophy has several parts to it:

A) Attempt to describe all possible actions as "rules".  These rules
   are probably best expressed as ImplicationLinks, with an antecedent
   that describes the situation in which the rule applies, and a
   consequent describing what should be done.

B) The set of rules to be applied in any given situation is done by
   performing a "fuzzy" search, to see how well a given antecedent
   matches the current state of the system.  The DualLink, and the
   as-yet-partly-implemented FuzzyLink can be used for this.  The
   existing fuzzy matcher could be used, but the existing fuzzy matcher
   has some serious design flaws that make it under-powered for anything
   more than rough prototyping.

C) There should be a graceful fall-back, so that if there are no rules
   that seem to apply to the given situation, then perhaps something
   more general can be performed/applied.  For example, if some sentence
   is not completely understood, but some of the words or phrases in
   it can be understood, then processing continues with those.

D) Point C) suggests that there should almost always be more than one
   rule that applies to the given situation.  Multiple rules provide
   choices and alternatives, but also are a challenge in terms of
   ranking priorities.

E) Point D) raises questions of how to rank rules.  Should fuzzy-overlap
   be used? Should the PLN TV rules be used? Should total mutual
   information be maximized?  Should some Bayesian or possibly Markovian
   (e.g. HMM) model be used?  Some neural-net approach? Something
   involving attention allocation? Some combination of the above?
   The claim here is that such ranking and choosing is an open research
   topic, and thus committing to an architecture that forces an answer,
   or restricts possibilities, is premature.

F) Because there are multiple stages of processing, this implies that
   the rules obtained via points B) thru E) have to be chained. The
   current chainer does not obviously provide the needed generality.
   Thus, it seems that chaining needs to be ad-hoc, for now.

G) Point C) suggests that the rules should be as general as possible,
   and should avoid focusing on implementation details.  For example,
   a rule meant to understand a sentence should NOT include a hook
   to grab the current sentence: instead, the rule should trust that
   the processing pipeline is feeding it the correct sentence at the
   correct time.

H) Point G) suggests that the rules should be ImplicationLinks and not
   BindLinks or GetLinks. Its the job of the rule-driver to convert
   these "soft, abstract" ImplicationLinks into explicit BindLinks
   to perform some action.  The rule-driver deals with the specifics
   of pushing the data through the rules.

More about points C), D), E): For example: maybe some parts of a single
rule need to be applied strictly, while other parts are very fuzzy. We
don't currently have a way of defining "strict" and "loose" portions of
a fuzzy match.  (Note that, in genetics, there are both highly-conserved
gene sequences, and highly-variable sequences.  Thus, the idea of
different amounts of fuzziness seems "natural").

Perhaps its better to have multiple classes of rules, where some rules
demand a very strict or exact match, while others can be very loose and
fuzzy.  Its not clear how to represent strictness/looseness, or how
to combine them.  Should strict/loose be treated as a probability? A
log-probability? Via some PLN-like formulas? Something else?

For the above reasons, I have been shying away from using either
OpenPsi, or the rule-engine, or anything else besides ad-hoc coding:
its not yet clear how to best create rules, and how to best combine
them.

Folded into the above concerns is how to auto-generate or learn new
rules.



Appendix A - Expanding sensory input
------------------------------------
The sensory inputs to the system need to be expanded.  Here is a list
of some relatively easy and useful things that could be done.

 * Sound intensity: is the room noisy and loud, or is it quiet?
 * Sudden changes: Was there a loud bang/crash just now?
 * Voices. Are many people talking all at once?
 * Nature of background noise: is it crowd noise? is it applause?
   Is it laughter?  Whistling, jeering? Is the audience/crowd
   whispering?  Did a previously noisy room suddenly become still?
   Time stamps for all such events.
 * Conversational pauses:  did the speaker/speech come to an end
   or a pause?  Is speaker saying "umm", "uhh", or other
   conversational-pause device?

Speech-to-text system needs:
 * Provide timestamps for words, i.e. to indicate: is speaker talking
   quickly, slowly?
 * Is speaker talking quietly or loud?  Shouting?
 * General affect perception: angry speech? gentle speech? Falling or
   rising tones?
 * Sex perception: is speaker male or female?

A rough cut for all of the above is surely not that hard: we don't need
a hyper-engineered system right now: we need more than zero, for any of
the above.

Visual system needs:
 * Identify faces as known or unknown faces
 * Identify sex of speaker.
 * Identify children (perhaps from height).
 * Identify direction that person/people are facing: are they facing
   us (the camera)? Are they at 3/4ths face? at 1/2 face (looking
   sideways, turned 90 degrees to us)? turned completely away?
 * Are they making eye contact?
 * Are their lips moving? i.e. visually, might they be talking?
 * Are there gestures of surprise (lips parting open, suddenly)?
 * Are the still, or animated? Bouncing with excitement or bored?
   Even a simple measure of visual flow will do: a general rate of
   movement, amplitude of movement.
 * Are we looking at a large crowd?  Is the crowd agitated or still?
 * Are they moving towards or away from us? Are they turning sideways?
 * Are they sitting or standing?

Some of these things could be easy: just like we listen for audio noise,
and measure the noise intensity, we could measure "video noise" and
simply report if there is a lot of movement (lots of pixels changing),
or only a little, and report when there are big changes in movement.
Even something as simple as this is more than what we currently have.

The End
-------
